https://www.knowledgehut.com/tutorials/aws/amazon-route-53

https://tutorialsdojo.com/amazon-route-53/Creating an AWS account
https://docs.aws.amazon.com/accounts/latest/reference/root-user.html

https://docs.aws.amazon.com/network-firewall/latest/developerguide/what-is-aws-network-firewall.html

https://aws.amazon.com/network-firewall/
https://k21academy.com/amazon-web-services/aws-certified-security-specialty-amazon-web-services/aws-waf/


https://aws.amazon.com/waf/
https://www.stormit.cloud/blog/what-is-aws-shield-how-does-it-work/
https://www.techtarget.com/searchcloudcomputing/tip/Which-should-I-choose-AWS-Shield-vs-WAF-vs-Firewall-Manager



--------------------------------------------------------------------------------------------------------
o	AWS Service Core: Introduction to Amazon Web Services (AWS) & the Cloud:
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/whitepapers/latest/aws-overview/introduction.html
Amazon Web Services 
	offers a broad set of global cloud-based products including 
		compute, 
		storage, 
		databases, 
		analytics, 
		networking, 
		mobile, 
		developer tools, 
		management tools, 
		IoT, 
		security, and 
		enterprise applications: 
			on-demand, 
			available in seconds, 
			with pay-as-you-go pricing. 
			
		From 
			data warehousing to deployment tools, 
			directories to content delivery, 
			over 200 AWS services are available. 
			
Today, 
	AWS provides 
		a highly reliable, 
		scalable, 
		low-cost infrastructure platform 
			in the cloud that 
		powers hundreds of thousands of businesses in 190 c ountries around the world.

In 2006, 
	Amazon Web Services (AWS) 
		began offering IT infrastructure services to businesses 
		as web services—
			now commonly known as cloud computing. 
--------------------------------------------------------------------------------------------------------
o	What Is Cloud Computing
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/whitepapers/latest/aws-overview/what-is-cloud-computing.html
Cloud computing 
	on-demand delivery of 
		compute power, 
		database, 
		storage, 
		applications, and 
		other IT resources 
	through a 
		cloud services platform 
	via 
		the internet 
	with 
		pay-as-you-go pricing. 
	

	cloud services platform 
		provides rapid access 
			to flexible and low-cost IT resources. 
	don’t make large upfront investments 
		in hardware and 
		spend a lot of time on the heavy lifting of managing that hardware. 
	provision exactly the right type and size of computing resources 
		you need to power your newest bright idea 
		or operate your IT department. 
	access as many resources as you need, 
		almost instantly
		pay for what you use.


--------------------------------------------------------------------------------------------------------
o	Advantages of Cloud Computing
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/whitepapers/latest/aws-overview/six-advantages-of-cloud-computing.html

fixed expense Vs variable expense – 
	invest heavily in data centers and servers 
	Vs
	pay only when you consume computing resources
		and pay only for how much you consume.

Benefit from massive economies of scale – 
	lower variable cost 
		usage from hundreds of thousands of customers is aggregated in the cloud, 
	economies of scale
		lower pay as-you-go prices.

Stop guessing capacity – 
	Don't guess your infrastructure capacity needs. 
	

Increase speed and agility – 
	quickly provision infra.
	
Stop spending money running and maintaining data centers – 

Go global in minutes – 

--------------------------------------------------------------------------------------------------------
o	Cloud Service Models
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/whitepapers/latest/aws-overview/types-of-cloud-computing.html

IaaS, PaaS and SaaS

............................
N.B:............................
Reference: https://www.eginnovations.com/blog/saas-vs-paas-vs-iaas-examples-differences-how-to-choose/
............................

	Infrastructure as a Service (IaaS)
	----------------------------------
Infrastructure as a Service (IaaS) 
	basic building blocks for cloud IT and 
	access to (virtual)
		networking features, 
		computers 
			(virtual or on dedicated hardware), and 
		data storage space. 
	
	IaaS provides 
		highest level of flexibility 
		management control on IT resources 
		most similar to existing IT resources 
			
e.g. 
	AWS EC2.
	Rackspace.
	Google Compute Engine (GCE).
	Digital Ocean.
	Microsoft Azure.
	Magento 1 Enterprise Edition*.


	Platform as a Service (PaaS)
	----------------------------
Platform as a Service (PaaS) 
	manage the underlying infrastructure 
		(usually hardware and operating systems) and 
		we can focus on 
			deployment and 
			management 
				of your applications. 
			
		we don’t need to worry about 
			resource procurement, 
			capacity planning, 
			software maintenance, 
			patching, 
		or 
			any of the other undifferentiated heavy lifting involved in running your application.
e.g.
	AWS Elastic Beanstalk.
	Heroku.
	Windows Azure (mainly used as PaaS).
	Force.com.
	Google App Engine.
	OpenShift.
	Apache Stratos.
	Adobe Magento Commerce Cloud.

	Software as a Service (SaaS)
	----------------------------
	Software as a Service (SaaS) 
		provides you with a completed product 
			run and 
			managed 
				by the service provider. 
	mostly end-user applications. 
	e.g.
		web-based email 
			send and receive email 
			
			
	More information
https://www.bigcommerce.com/articles/ecommerce/saas-vs-paas-vs-iaas/	
--------------------------------------------------------------------------------------------------------
o	Public, Private and Hybrid cloud
--------------------------------------------------------------------------------------------------------
https://www.geeksforgeeks.org/cloud-deployment-models/

Deployment Models 
The cloud deployment model 
	identifies the specific type of cloud environment 
	based on 
		ownership, 
		scale, and 
		access, as 
		cloud’s nature and purpose. 
		location of the servers 
		who controls them 
			defined by a cloud deployment model. It specifies how your cloud infrastructure will look, what you can change, and whether you will be given services or will have to create everything yourself. Relationships between the infrastructure and your users are also defined by cloud deployment types. 

Different types of cloud computing deployment models are:
	Public cloud 
	Private cloud
	Hybrid cloud
	Community cloud
	Multi-cloud 




Public Cloud 
	anybody can access services. 
	[arguably] may be less secure 
		as it is open to everyone. 
	cloud infrastructure services provided over the internet 
		to general people or 
		major industry groups. 
	
	infrastructure 
		owned by the cloud provider
		not by the consumer. 
		allows customers and users 
			to easily access systems and services. 
		
		storage backup and retrieval services are 
			given for free
			as a subscription, 
			or on a per-user basis. 
			Example: Google App Engine etc.

	Advantages of Public Cloud Model:

		Minimal Investment: 
		No setup cost: 
		No Infrastructure Management 
		No maintenance: 
		Dynamic Scalability:  

	Disadvantages of Public Cloud Model:
		Arguably Less secure: 
			Public cloud is less secure as resources are public so there is no guarantee of high-level security.
		Low customization: 
			It is accessed by many public so it can’t be customized according to personal requirements. 


Private Cloud 

	exact opposite of the public cloud deployment model. 
	one-on-one environment 
		for a single user (customer). 
	don't share your hardware 
	access systems and services 
		within a given border or organization. 
	The cloud platform 
		implemented in a cloud-based secure environment 
		protected by powerful firewalls and 
		under the supervision of an organization’s IT department. 
		greater flexibility of control over cloud resources.

	Advantages of Private Cloud Model:
		Better Control: 
			sole owner of the property. 
			complete command over 
				service integration, 
				IT operations, 
				policies, and 
				user behavior. 
		Data Security and Privacy: 
			suitable for storing critical corporate information 
				only authorized staff have access. 
			segment resources 
				improved access and 
				security can be achieved.
		Supports Legacy Systems: 
			designed to work with legacy systems 
				that are unable to access the public cloud. 
		Customization: 
			Unlike a public cloud deployment, 
			private cloud allows a company to tailor its solution to meet its specific needs.
	Disadvantages of Private Cloud Model:
		Less scalable: 
			scaled in a range as 
				less number of clients.
		Costly: 
			costly 
			as you provide personalized facilities.
 
Hybrid Cloud 
	bridge 
		public and private cloud 
	gives the best of both worlds. 
	host the app in a safe environment 
		save cost using public cloud’s . 
	move data and applications 
		between different clouds 
			using a combination of 
				two or more cloud deployment methods.
	Advantages of  Hybrid Cloud Model:
		Flexibility and control: 
			Businesses with more flexibility 
			can design personalized solutions 
				that meet their particular needs.
		Cost: 
			public clouds provide scalability
		Security: 
			data is properly separated
			chances of data theft 
				reduced. 
	Disadvantages of Hybrid Cloud Model:
		Difficult to manage: 
			combination of both 
			So, it is complex.
		Slow data transmission: 
			Data transmission through the public cloud 
			so latency occurs.
		Cost: 	
			pay for extra capacity 
			and continous data transfer

Community Cloud
	allows systems and services 
		to be accessible by a 
			group of organizations. 
	distributed system 
		created by integrating the services of 
			different clouds 
				to address the specific needs of a 
					community, 
					industry, or 
					business. 
	infrastructure of the community 
		could be shared between 
			organization which has shared concerns or tasks. 
	generally managed by 
		third party 
	or 
		combination of multiple organizations in the community. 

	Advantages of Community Cloud Model:
		Cost Effective: 
			shared by multiple organizations or communities.
		Security: 
			Community cloud provides better security.
		Shared resources: 
			share 
				resources, 
				infrastructure, 
				etc. with 
					multiple organizations.
		Collaboration and data sharing: 
			suitable for both collaboration and data sharing.
	Disadvantages of Community Cloud Model:
		Limited Scalability: 
			relatively less scalable 
				as many organizations share the same resources 
					according to their collaborative interests. 
		Rigid in customization: 
			everybody should agree
			
Multi-cloud 
	Use multiple cloud providers 
		at the same time 
	similar to hybrid cloud 
	public cloud providers 
		provide numerous tools 
			improve the reliability of their 
				services, 
				mishaps still occur. 
	Highly unlikely 
		two distinct clouds 
		would have an incident at the same moment. 
		improves the high availability of your services even more. 

	Advantages of a Multi-Cloud Model:
		mix and match 
			the best features  
		Reduced Latency: 
			To reduce latency 
				improve user experience
				choose cloud regions and zones 
					that are close to your clients. 
		High availability of service: 
			two distinct clouds 
				would not have an incident 
					at the same moment. 
	Disadvantages of Multi-Cloud Model:
		Complex: 
			
		Security issue: 
			complex structure
				there may be loopholes 
			hacker may take advantage 

--------------------------------------------------------------------------------------------------------
o	Amazon Web Services (AWS) and its Benefits
--------------------------------------------------------------------------------------------------------
https://aws.amazon.com/what-is-aws/
	Cloud computing with AWS
	Amazon Web Services (AWS) 
		world’s most 
			comprehensive and 
			broadly adopted cloud platform
		over 200 fully featured services 
			from data centers globally. 
		Millions of customers—including 
			fastest-growing startups, 
			largest enterprises, and 
			leading government agencies
			by 
				lower costs, 
				become more agile, and 
				innovate faster.



https://aws.amazon.com/application-hosting/benefits/
	
	Most functionality
		significantly 
			more services
			more features 
				than 
					any other cloud provider–from 
		makes it 
			faster, 
			easier, and 
			more cost effective 
				to move your existing applications to the 
					cloud.
		AWS also has the deepest functionality within those services. 
		For e.g.
			widest variety of databases 
				purpose-built for different types of applications 
					so you can choose the right tool 
					for the job to get the best cost and performance.
	Largest community of customers and partners
		largest and most dynamic community
		millions of active customers 
		tens of thousands of partners globally. 
		Customers across every 
			industry and 
			every size
				including 
					startups, 
					enterprises, and 
					public sector organizations, 
						running every imaginable use case on AWS. 
		The AWS Partner Network (APN) 
			includes thousands of systems integrators who 
			specialize in AWS services and 
			tens of thousands of independent software vendors (ISVs) 
				who adapt their technology to work on AWS. 
	Most secure
		architected to be the 
			most flexible and 
			secure cloud computing environment 
			
		Core infrastructure 
			used in 
				military, 
				global banks, and 
				other high-sensitivity organizations. 
		Specific deployment for US defence
		backed by a deep set of cloud security tools
			over 300 
				security, 
				compliance, and 
				governance 
					services and features. 
		supports 98 
			security standards and 
			compliance certifications
		all 117 AWS services that store customer data 
			offer the ability to encrypt that data.
	Fastest pace of innovation
		latest technologies to 
			experiment and 
			innovate 
				more quickly. 
		continually accelerating our pace of innovation 
			to invent entirely new technologies 
		For e.g.
			in 2014, 
				AWS pioneered the serverless computing 
					launch of AWS Lambda
			AWS built Amazon SageMaker
				a fully managed machine learning service 
		Lot of data already present with aws
		11.6 seconds deployment 
	Most proven operational expertise
		unmatched 
			experience, 
			maturity, 
			reliability, 
			security, and 
			performance 
				
		over 16 years
			delivering cloud services 
				to millions of customers 
					around the world 
					running a wide variety of use cases. 
		most operational experience
			at greater scale
				of any cloud provider.
--------------------------------------------------------------------------------------------------------
o	AWS Global Infrastructure
--------------------------------------------------------------------------------------------------------
https://aws.amazon.com/about-aws/global-infrastructure/
--------------------------------------------------------------------------------------------------------
o	Different Amazon Web Services
--------------------------------------------------------------------------------------------------------
https://mindmajix.com/top-aws-services
list of Top 30 AWS Services List:

	virtualization
	networking
	storage
	compute power
	databases
	analytics
	security
	developer tools
	etc.


Service #1. Amazon EC2 [Elastic Compute Cloud]
	AWS EC2 AWS Service Amazon EC2 
		virtual servers 
		computing infrastructure 
			with the best suitable 
				processors, 
				networking facilities, and 
				storage systems. 
		highly secure, 
		reliable, 
		performing computing infrastructure 
			meeting business demands. 
		access resources quickly and 
		dynamically scale capacities as per demands.

Service #2. Amazon S3
	Amazon S3 
		object storage AWS service
		highly scalable. 
		access any quantity of data from anywhere. 
		data is stored in ‘storage classes’ 
			to reduce costs 
			no extra investment and 
			manage it comfortably. 
		data is highly secured and 
		supports meeting audit and compliance requirements. 
		handle any volume of data 
			with Amazon S3’s 
			robust access controls, 
			replication tools, and 
			higher visibility. 
		maintain data version controls and 
		preventing accidental deletion.
	
	
Service #3. AWS Aurora
	AWS RDS AWS Service  
	MySQL and PostgreSQL compatible relational database 
		high performance. 
	five times faster than standard MySQL databases. 
	can automate crucial tasks 
		e.g. 
			hardware provisioning, 
			database setup and 
			backups, and 
			patching. 
	Amazon Aurora 
		distributed, 
		fault-tolerant, 
		self-healing 
			storage system 
	can scale automatically as per needs. 
	reduce costs significantly 
	enhance databases' 
		security, 
		availability, and 
		reliability.	
	
Service #4. Amazon DynamoDB
	DynamoDB AWS Service DynamoDB 
		fully managed and 
		serverless NoSQL database AWS service. 
		fast and flexible database system 
		low cost. 
		single-digit millisecond performance 
		with unlimited throughput and storage. 
		in-built tools to generate 
			actionable insights, 
			useful analytics, and 
			monitor traffic trends in applications.	
	
	
Service #5. Amazon RDS
	AWS RDS 
	managed Relational Database 
		for 
			MySQL, 
			PostgreSQL, 
			Oracle, 
			SQL Server, and 
			MariaDB. 
		allows the 
			setup, 
			operation, and 
			scale of a 
				relational database in the cloud quickly. 
		high performance 
			by automating the tasks such as 
				hardware provisioning, 
				database setup, 
				patching, and 
				backups. 
	don’t need to 
		install and 
		maintain 
			the database software. 
	can optimize costs 
	high availability, 
	security, and 
	compatibility for your resources.	
	
	
Service #6. Amazon Lambda
	AWS Lambda 
	serverless and event-driven computing AWS service. 
	run codes automatically 
	without worrying about servers and clusters. 
	codes can be uploaded directly 
		don't need to provision or manage infrastructure. 
	automatically accepts 'code execution requests' 
	pay the price only for the computed time
	effective cost-control.	
	

Service #7. Amazon VPC
	AWS VPC 
		Virtual Private Cloud
		
		controls the virtual networking environment
			resource placement, 
			connectivity, and 
			security. 
	build and manage compatible VPC networks 
		across cloud AWS resources and on-premise resources. 
	improves security 
		inbound and 
		outbound connections. 
	monitors VPC flow logs 
		delivered to Amazon S3 and Amazon Cloudwatch 
		gain visibility over network dependencies and traffic patterns. 
	Amazon VPC 
		detects anomalies in the 
			patterns, 
			prevents data leakage, and 
			troubleshoots network connectivity and 
			configuration issues.


Service #8. Amazon CloudFront
	Amazon CloudFront 
		delivers content globally, 
		high performance and security. 
		delivers data with high speed and low latency. 
		content is delivered to destinations successfully 
			with automated network mapping and 
			intelligent routing mechanisms. 
		Improved security of data 
			with traffic encryption 
			access controls. 
	data can be transferred within milliseconds 
		with its in-built 
			data compression, 
			edge computing capabilities, and 
			field-level encryption. 
	stream high-quality video using AWS media services 
		to any device 
			quickly and consistently using Amazon CloudFront.

Service #9. AWS Elastic Beanstalk
	Supports running and managing web applications. 
		on an aws service
	we deploy code
		web app infrastructure managed by aws
	allows easy deployment of applications from 
		capacity provisioning, 
		load balancing
		auto-scaling 
		application health monitoring. 
		
	auto-scaling properties
	manage peaks in workloads and traffic 
		with minimum costs. 
	developer-friendly tool 
		manages 
			servers, 
			load balancers, 
			firewalls, and 
			networks simply. 
			

Service #10. Amazon EC2 Auto-scaling
	scales computing capacity 
		add or remove EC2 instances automatically. 
	two types of scaling 
		dynamic scaling and 
			responds to the presently changing demands
		predictive scaling. 
			responds based on predictions. 
			e.g. increase based on time etc.
	Through Amazon EC2 Auto-scaling
		identify the unhealthy EC2 instances, 
		terminate them
		replace them with new instances.


Service #11. Amazon ElastiCache
	 fully-managed, flexible, in-memory caching AWS service. 
	 supports increasing the performance of your applications and database. 
	 reduce the load in a database 
		by caching data in memory. 
	in-memory with 
	high speed, 
	microsecond latency
	high throughput. 
	reduce costs and 
	eliminate the operational overhead of your business.

Service #12. Amazon S3 Glacier
	archive storage in the cloud 
		at a low cost. 
	built with three storage classes 
		S3 Glacier instant retrieval, 
			supports immediate access 
		flexible retrieval, and 
		deep archive. 
	to data, and the flexible class allows flexible access within minutes to hours with no cost. The third one, deep archive, helps archive compliance data and digital media. Overall, they support you to access data from archives faster.

Service #13. Amazon Lightsail
	Amazon Lightsail AWS Service  Amazon Lightsail is the website and applications building AWS service. This service offers Virtual Private Server instances, containers, databases, and storage. It allows a serverless computing service with AWS Lambda. With Amazon Lightsail, you can create websites using pre-configured applications such as WordPress, Magento, Prestashop, and Joomla in a few clicks and at a low cost. In addition to this, it is the best tool for testing, so you can create, test, and delete sandboxes with your new ideas.

Service #14. Amazon Sagemaker
	amazon sagemaker aws service  Amazon Sagemaker is the AWS service that allows building, training, and deploying Machine Learning (ML) models at a large capacity. It is an analytical tool that functions based on Machine Learning power to analyze data more efficiently. With its single tool-set, you can build high-quality ML models quickly. Amazon Sagemaker not only generates reports but provides the purpose for generating predictions too. In addition, Amazon Ground Truth Plus creates datasets without labeling applications.

Service #15. Amazon SNS
	AWS SNS AWS Service  It is the Amazon Simple Notification Service (SNS). It is a messaging service between Application to Application (A2P) and Application to Person (A2Person). Here, A2P helps many-to-many messaging between distributed systems, microservices, and event-driven serverless applications. And, A2P supports applications to send messages to many users via mail, SMS, etc. For instance, you can send up to ten messages in a single API request. With effective filtering systems, subscribers will receive messages that they are interested in. Besides, Amazon SNS works alongside Amazon SQS to deliver messages accurately and consistently.

Service #16. Amazon EBS
	AWS EBS AWS Service Amazon Elastic Block Store (EBS) is the block storage service. It supports scaling high-performance workloads such as SAP, Oracle, and Microsoft products. And it provides better protection against failures up to 99.999%. It helps to resize clusters for big data analytics engines such as Hadoop and Spark. Also, you can build storage volumes, optimize storage performance, and reduce costs. Amazon EBS’s lifecycle management creates policies that help create and manage backups effectively.

Service #17. Amazon Kinesis
	AWS Kinesis AWS Service  It is the AWS service that analyses the video as well as data streams. Amazon Kinesis collects, processes, and analyzes all types of streaming data. Here, the data may be audio, video, application logs, website clickstreams, and IoT telemetry. Then, it generates real-time insights within seconds once the data has arrived. With the help of Amazon Kinesis, you could stream and process a large quantity of real-time data with low latencies, very simply.

Service #18. Amazon Elastic File System (EFS) 
	Amazon Elastic File System AWS Service  Amazon EFS is the fully managed file system for Amazon EC2. And it is a simple and serverless elastic file system. You can create and configure file systems without provisioning, deploying, patching, and maintenance using Amazon EFS. Here, files can be added and deleted as per the scaling needs. Especially, you can pay only for the used space, hence this service helps to reduce costs.

Service #19. AWS IAM
	AWS IAM AWS Service  It is the Identity and Access Management (IAM) service offered by AWS to securely access the applications and resources. It regulates access to various resources based on roles and access policies; as a result, you can achieve a fine-grained access control on your resources. The AWS IAM access analyzer helps streamline permission management through setting, verifying, and refining. In addition, AWS IAM attribute-based access control helps create fine-grained permissions based on user attributes such as department, job role, team name, etc.

Service #20. Amazon SQS
	AWS SQS AWS Service  Amazon SQS is a fully-managed message queuing service. There are two types of message queuing services: SQS Standard and SQS FIFO. Here, the SQS standard offers the features such as maximum throughput, best-effort ordering, and quick delivery. And SQS FIFO processes messages only once in the same order by which they have been sent. Also, Amazon SQS allows decoupling or scaling microservices, distributed systems, and serverless applications. It helps you send, receive and manage messages in a large volume. Moreover, there is no need to install and maintain other messaging software, reducing costs significantly. Besides, scaling is carried out quickly and automatically in this service.

Service #21. Amazon RedShift
	AWS Redshift AWS Service Amazon Redshift is a quick, simple, and cost-effective data warehousing service. You can gain insights about cloud data warehousing in an easy, faster, and more secure way. It allows analysis of all the data in operational databases, data lakes, data warehouses, and third-party data. And Amazon Redshift helps analyze a large volume of data and run complex analytical queries. With its automation capabilities, this service increases query speed and provides the best price performance.

Service #22. Amazon Cloudwatch
	Amazon Cloudwatch AWS Service  This AWS service monitors the cloud resources and applications keenly. It is a single platform that helps to monitor all AWS resources and applications; it increases visibility to respond to issues quickly. Mainly, Amazon Cloudwatch provides actionable insights to optimize monitoring applications, systemwide performance changes, and resource utilization. And you can get a complete view of the health of AWS resources, applications, and services running on AWS and on-premises. In addition, Amazon Cloudwatch helps to detect anomalies in the behavior of the cloud environment, set alarms, visualize logs and metrics, make automated actions, troubleshoot issues, and discover insights.

Service #23. Amazon Chime
	Amazon Chime AWS Service Amazon Chime is a communication service. It is a single solution that offers audio calling, video calling, and screen sharing capabilities. With the help of this service, you can make quality meetings, chat, and video calls both inside and outside of your organization. And more features can be added to this service as per your business needs. Mainly, you can set calls for a pre-defined time to automatically make calls on time. Amazon Chime helps you not to miss a meeting amidst your hectic schedule at work. Besides, you can pay as per the usage of resources by which you can reduce the costs significantly.

Service #24. Amazon Cognito
	amazon cognito aws service It is the identity management AWS service. Amazon Cognito manages identities for accessing your applications and resources. Mainly, this service helps add sign-in, sign-up, and access control the web and mobile apps quickly. It can support millions of users to sign in with familiar applications such as Apple, Facebook, Google, and Amazon. In Amazon Cognito, the feature ‘Cognito user pools’ can be set up quickly without any infrastructure, and the pool members will have a directory profile. It supports multi-factor authentication and encryption of data-at-rest and data-in-transit.

Service #25. Amazon Inspector 
	Amazon Inspector AWS Service Amazon Inspector is an automated vulnerability management service. This service offers continuous and automated vulnerability management for Amazon EC2 and Amazon ECR. It allows scanning AWS workloads for software vulnerabilities and unwanted network exposure. Amazon Inspector quickly identifies vulnerabilities, which helps to take immediate actions to resolve them before it worsens the applications. Moreover, it supports meeting compliance requirements and reduces meantime-to-remediate vulnerabilities. And it provides you with accurate risk scores and streamlined workflow.

Service #26. AWS Firewall Manager
	AWS Firewall Manager AWS Service It is the central management service of firewall rules. The firewall manager supports managing firewall rules across all the applications and accounts. The common security rules help to manage new applications included over time. It is the one-time solution for consistently creating firewall rules and security policies and implementing them across the infrastructure. AWS firewall manager helps you audit VPC security groups for compliance requirements and control network traffic effectively.

Service #27. Amazon Appflow
	Amazon Appflow AWS Service Amazon Appflow is a no-code service that allows the integration of SaaS applications and AWS services effortlessly. To be more precise, it securely automates dataflows integrating third-party applications and AWS services without using codes. You can transfer data between SaaS applications such as Salesforce, SAP, Zendesk, etc. since Amazon Appflow can be integrated with other applications in a few clicks. Especially, a large volume of data can be moved without breaking it up into batches using this service.

Service #28. Amazon Route 53
	Amazon Route53 AWS Service It is a scalable cloud Domain Name System (DNS) service. It allows end-users to connect with Amazon EC2, Elastic load balancers, Amazon S3 buckets, and even outside AWS. In this service, the feature ‘Route 53 application recovery controllers’ configure DNS health checks and helps to monitor the ability of systems to recover from failures. And ‘Route 53 traffic flow’ helps manage traffic across the globe using routing methods such as latency-based routing, Geo DNS, Geoproximity, and weighted round-robin.

Service #29. AWS Cloud Formation
	Amazon Inspector AWS ServiceThis AWS service creates and manages resources with templates. It is a single platform that can handle all AWS accounts across the globe. It automates resource management with AWS service integration and offers turnkey application distribution and governance controls. Also, AWS Cloud Formation can automate, test, and deploy infrastructure with continuous integration and delivery. And you can run applications right from AWS EC2 to complex multi-region applications using this service.

Service #30. AWS Key Management Service (KMS)
	AWS KMS AWS Service AWS KMS manages the creation and control of encryption keys. It means that AWS KMS creates cryptographic keys and controls their uses across various applications. You can achieve a secure and resilient service using hardware resilient modules to protect keys. This service can be integrated with AWS Cloudtrail to provide logs of all key usage to precisely fulfil compliance and regulatory requirements.

 
--------------------------------------------------------------------------------------------------------
o	Lab: Quick walk through of the UI
--------------------------------------------------------------------------------------------------------
	

	Search for Route 53
		look at features
		select it 
			it is global and doesn't belong to a region/geography
	region specific services 
		will not be listed in another region.
		
	Search for "AWS regional services list"
		will show regions where a service is available
		all services are not available in all regions.

--------------------------------------------------------------------------------------------------------
o	Understanding How AWS is Physically Set Up
--------------------------------------------------------------------------------------------------------
https://www.youtube.com/watch?v=MUMyZ3FFfdg

https://aws.amazon.com/about-aws/global-infrastructure/

https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html

Refer image in above link
Amazon cloud computing resources 
	hosted in multiple locations world-wide. 
	locations 
		AWS Regions,
			separate geographic area
		Availability Zones, 
			Each AWS Region has 
				multiple, isolated locations 
					known as Availability Zones
		Local Zones (like edge locations). 
			can place resources 
				e.g.
					compute and 
					storage, 
						in multiple locations 
							closer to your users. 
		Amazon RDS 
				enables you to place resources like 
					DB instances, and 
					data 
						in multiple locations. 
		N.B: Resources aren't replicated across AWS Regions unless you do so specifically.
			
			aws ec2 describe-regions	# Regions that are enabled for your account.
			aws ec2 describe-regions --all-regions

--------------------------
aws ec2 run-instances  --image-id ami-0cc87e5027adcdca8 --count 1  --instance-type t2.micro  --key-name vilas  --security-group-ids sg-07570e17ab8331f13 \
    --subnet-id subnet-00b5ede5e160caa59 \
    --block-device-mappings "[{\"DeviceName\":\"/dev/sdf\",\"Ebs\":{\"VolumeSize\":30,\"DeleteOnTermination\":false}}]" \
    --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=demo-server}]' 'ResourceType=volume,Tags=[{Key=Name,Value=demo-server-disk}]'	
--------------------------

		Find name of the us-east-2 Region.
			aws lightsail get-regions --query "regions[?name=='us-east-2'].displayName" --output text
				ans: ohio
				
				

		(https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html)
		AWS Outposts 
			brings native AWS 
				services, 
				infrastructure, and 
				operating models 
					to virtually any data center, 
						co-location space, 
					or 
						on-premises facility.

		Wavelength Zones 
			allow developers to build applications that 
				deliver ultra-low latencies to 5G devices and end users. 
			Wavelength deploys 
				standard AWS compute and storage services 
					to the edge of telecommunication carriers' 5G networks.



	Amazon operates 
		state-of-the-art, highly-available data centers. 
	Rare but failures can occur 
		affect availability of [DB] instances 
			that are in the same location. 
	If you host all your DB instances in one location 
		none of your DB instances will be available.



	AWS Region 
		completely independent. 

		For e.g. any Amazon RDS activity you initiate 
			(for example, creating database instances or listing available database instances) 
			runs only in your current default AWS Region. 
			default AWS Region 
				can be changed in the console
			or 
				set the AWS_DEFAULT_REGION environment variable. 
			Or 
				override by using the 
					--region parameter 
					with the AWS Command Line Interface (AWS CLI). 
			
			More info: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html
		
	
	Availability Zones
		aws ec2 describe-availability-zones --region region-name
		aws ec2 describe-availability-zones --region us-east-1
			6 zones
		aws ec2 describe-availability-zones --region us-east-2
			3 zones
			
	https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html
		Availability Zones are multiple, isolated locations within each Region.
		
		
		

--------------------------------------------------------------------------------------------------------
•	Amazon Authentication - IAM (Identity and Access Management)	
--------------------------------------------------------------------------------------------------------
AWS Identity and Access Management (IAM) 
	web service 
	securely control access to AWS resources. 
	centrally manage permissions 
	control which AWS resources 
	IAM control 
		who is authenticated (signed in) 
	and 
		authorized (has permissions) to use resources.

create an AWS account
	get one sign-in identity 
	has complete access to all AWS services and resources 
		called AWS account root user 
	accessed by signing in with the 
		email address and password 
	Safeguard your root user credentials 
	
	

IAM features
------------
	1. Shared access to your AWS account
		can grant 
			other people permission to 
				administer and 
				use resources in your AWS account 
					without sharing 
						password or 
						access key.

	2. Granular permissions
	
		grant different permissions 
			to different people 
				for different resources. 
		For e.g.
			full access to 
				EC2
				S3
				DynamoDB
				Amazon Redshift
			allow 
				read-only access to 
					S3 buckets
			administer just some EC2 instances
			access your billing information but nothing else.

	3. Secure access to AWS resources 
		for applications that run on Amazon EC2
		
		securely provide credentials for 
			applications that run on EC2 instances. 
		provide permissions 
			for your application 
			to access other AWS resources. 
		Examples 
			access 
				S3 buckets and DynamoDB tables
				from ec2 applications.

	4. Multi-factor authentication (MFA)
		two-factor authentication to 
			your account and 
			to individual users 
				for extra security. 
		provide 
			password or	access key 
		and 
			code from a specially configured device. 
			
	5. Identity federation
		allow users 
			who already have accounts elsewhere—
			for example
				in your corporate network or with an internet identity provider—
				to get temporary access to your AWS account.

	6. Identity information for assurance
		use AWS CloudTrail, 
			you receive log records 
				include information like 
					who made requests for resources 
						based on IAM identities.

	7. PCI DSS Compliance
		IAM supports 
			processing, 
			storage, and 
			transmission of credit card data 
				by a merchant or service provider
		validated as being compliant with Payment Card Industry (PCI) Data Security Standard (DSS). 

	8. Integrated with many AWS services
		https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html
		

	9. Eventually Consistent
		IAM
			is eventually consistent. 
		achieves high availability 
			replicate data across multiple servers 
			in Amazon's data centers around the world. 
		change some data 
			change is committed and safely stored. 
		change must be replicated 
			across IAM
			can take some time. 
		Such changes include 
			creating or updating 
				users, 
				groups, 
				roles, or 
				policies. 
		do not include such IAM changes in 
			critical, 
			high-availability 
				code paths of your application. 
		make IAM changes 
			in a separate initialization or 
			setup routine that you run less frequently. 
		verify 
			changes have propagated 
				before production workflows depend on them. 
		
	10. Free to use
		IAM and AWS Security Token Service (AWS STS) are 
			features of your AWS account 
			offered at no additional charge. 
		charged only when you access other AWS services using 
			your IAM users or AWS STS temporary security credentials. 

	11. Accessing IAM
		can work with IAM in following ways.

		a. AWS Management Console
			browser-based interface 
				manage IAM and AWS resources. 
				
		b. AWS Command Line Tools
			issue commands at your system's command line 
			command line 
				can be faster and 
				more convenient 
					than the console. 
			can automate

		c. AWS provides two sets of command line tools: 
			AWS Command Line Interface (AWS CLI) 
		and 
			AWS Tools for Windows PowerShell. 
			
		d. AWS SDKs
			AWS provides SDKs (software development kits) 
			consist of 
				libraries and 
				sample code 
					for various programming languages and 
				platforms 
					(Java, Python, Ruby, .NET, iOS, Android, etc.). 
				convenient way to create programmatic access to 
					IAM and AWS. 
				
		16. IAM HTTPS API
			You can access IAM and AWS programmatically 
				by using the IAM HTTPS API, 
				issue HTTPS requests 
					directly to the service. 
			While using the HTTPS API
				include code to digitally sign requests 
					using your credentials. 
					
	https://catalog.workshops.aws/general-immersionday/en-US/basic-modules/30-iam
--------------------------------------------------------------------------------------------------------
	o	Understand the security measures AWS provides and 
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
			key concepts of AWS Identity and Access Management (IAM)
--------------------------------------------------------------------------------------------------------


AWS (flow chart)
	Refer diagram in https://shunliz.gitbooks.io/aws-certificate-notes/content/iam.html

	deny all by default.
	if a policy/role is not present 
		user can't do anything.
	get all policies and roles
		identify policies and roles associated with the current resource
		ignore all others
		consider all applicable
	
		if a deny policy exist
			yes
				ignore allow policies and deny
			no
				if there is a allow policy
					allow
		no policy applicable
			deny
	
	

--------------------------------------------------------------------------------------------------------
o	IAM Introduction
--------------------------------------------------------------------------------------------------------
https://www.youtube.com/watch?v=_ZCTvmaPgao
--------------------------------------------------------------------------------------------------------
	Users 
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users.html

An AWS Identity and Access Management (IAM) user 
	entity that you create in AWS. 
	represents 
		human user 
	or 
		workload who uses the IAM user 
			to interact with AWS. 
	consists of a 
		name and 
		credentials.
Two types
	Root user 
	IAM user


How AWS identifies an IAM user
	A "friendly name"
	An Amazon Resource Name (ARN)
		arn:aws:iam::account-ID-without-hyphens:user/Richard
	unique identifier for the IAM user	

You can access AWS in different ways depending on the IAM user credentials:
	Console password: 
	Access keys
	SSH keys for use with CodeCommit
	Server certificates
	
Following options to administer passwords
	Manage passwords for your IAM users. 
	Manage access keys for your IAM users.
	Enable multi-factor authentication (MFA) for the IAM user. 
		Find unused passwords and access keys. 
		security best practice 
			remove passwords and access keys 
				when users no longer need them.

--------------------------------------------------------------------------------------------------------

IAM Users
	represents the 
		person or 
		service 
			who uses the access to interact with AWS.
	IAM Best Practice 
		Create Individual Users
	User credentials can consist of the following
		Password
			access through console
		Access Key/Secret Access Key 
			access AWS services through 
				API, 
				CLI or 
				SDK
	IAM user 
		starts with no permissions 
		should be granted permissions 
			as per the job function requirement
	IAM Best Practice – 
		Grant least Privilege
	Each IAM user 
		associated with one and only one AWS account.
	IAM User 
		cannot 	be renamed from console
		be done from CLI or SDK tools.
	IAM handles the renaming of user w.r.t 
		unique id, 
		groups, 
		policies 
			where the user was mentioned as a principal. 
		However, you need to handle the renaming in the policies where the user was mentioned as a resource	

IAM users and permissions
	By default, 
		new IAM user 
			has no permissions. 
				except to login
				
	can assign 
		permissions through 
			roles
			policies
				
	can also add a permissions boundary to your IAM users. 
		advanced feature 
		allows you to use 
			AWS managed policies 
			limit the maximum permissions 
				policy can grant to an IAM user or role. 


IAM users and accounts
	Each IAM user 
		is associated with one AWS account only. 
		defined within your AWS account
		don't need to have a payment method 
		AWS activity performed by IAM users 
			billed to root account.

	The number and size of 
		IAM resources in an AWS account 
			are limited. 
	
IAM users as service accounts
	An IAM user 
		resource in IAM 
		has associated 
			credentials and 
			permissions. 
	An IAM user 
		can represent 
			a person or 
			an application 
				referred as a service account. 
	do not embed access keys 
		directly into your application code. 
	put access keys in 
		known locations 
	
	Most common features
		Users
		Groups
		Policies
		Roles
		https://www.youtube.com/watch?v=jP-1qPe6P4s&list=PLv2a_5pNAko0Mijc6mnv04xeOut443Wnk&index=9
	https://267092042432.signin.aws.amazon.com/console
	

--------------------------------------------------------------------------------------------------------
	Groups 

IAM Groups
	IAM group 
		collection of IAM users
	IAM groups 
		specify permissions 
			for a collection of users 
				
	IAM Best Practice – 
		Use groups to assign permissions to IAM Users
	A group 
		not truly an identity 
			because it cannot be identified as a Principal 
			only a way to attach policies to multiple users at one time
	A group to users
		many-to-many
		user can belong to multiple groups (10 max)
	Groups cannot be nested 
		can only have users within it
	no default group 
		if one is required 
			it should be created with all users assigned to it.
	Deletion of the groups requires you to detach users and managed policies and delete any inline policies before deleting the group. With AWS management console, the deletion and detachment is taken care of.

--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups.html


IAM user group 
	collection of IAM users. 
	specify permissions 
		for multiple users
	easier to manage the permissions for those users. 
	For e.g.
		give admin permission to a group.
		add new user 
		move a user out
		
	attach an identity-based policy 
		to a user group 
			all of the users in the user group 
				receive the policy's permissions. 
	cannot identify a user group as a Principal 
		in a policy 
			(such as a resource-based policy) 
			because groups relate to permissions
				not authentication
				and principals are authenticated IAM entities. 
	
important characteristics of user groups:
	can contain many users
	user can belong to multiple user groups.
	User groups can't be nested; 
		can contain only users
		not other user groups.
	No default user group 
		automatically includes all users in the AWS account. 
	number and size of IAM resources 
		like 
			number of groups
			number of groups that a user can be a member of, are limited. 
	refer below		
		https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_iam-quotas.html


--------------------------------------------------------------------------------------------------------
	Policies
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html

Helps to define	
	group of permissions
	boundaries for them
	
	create policies 
		attach them to IAM identities 
			users
			groups 
			roles
			or AWS resources. 
	object in AWS 
		associated with 
			identity or 
			resource, 
				defines their permissions. 
	AWS evaluates these policies 
		when an IAM principal (user or role) 
			makes a request. 
	
	Most policies 
		stored in AWS as JSON documents. 
	AWS supports six types of policies: 
		identity-based policies, 
		resource-based policies, 
		permissions boundaries, 
		Organizations SCPs, 
		ACLs, and 
		session policies.

For example
	if a policy allows 
		GetUser 
			user 
				can get user information from 
					AWS Management Console, 
					AWS CLI
					AWS API. 
	
	
	
	Policy types
	------------
The following 
	policy types
		listed in 
			most frequently used to 
			less frequently used, 
			are available for use in AWS. 
			
(1) 
Identity-based policies – 
	Attach managed and inline policies to IAM identities 
	
	grant permissions to an identity.
		AWS users, 
		groups, and 
		roles. 
	
	Give access to 
		EC2 instances
		S3 buckets
		RDS databases
			etc.

		two types of identity-based policies 
			AWS-managed policies and 
			customer-managed policies.

			AWS-managed policies 
				predefined policies 
					created by AWS 
						we use to manage access to AWS resources. 
						
			Customer-managed policies
				create and manage us. 
				
				can be attached to 
					IAM users, 
					groups, or 
					roles
					can be modified as needed to 
						grant or 
						restrict access to AWS resources.
(2)
Resource-based policies – 
	Attach inline policies to resources. 
	e.g. 
		Amazon S3 bucket policies and 
		IAM role trust policies. 
		
		grant permissions to  
			principal specified in the policy. 
			Principals can be 
				same account 
			or 
				other accounts.
---------------------------------------------------------------------
Hands on: 
a. Create a new user vilas1
b. login as vilas1
c. vilas1 cannot create a vpc
d. As admin create the following policy 
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:AcceptVpcPeeringConnection",
                "ec2:AcceptVpcEndpointConnections",
                "ec2:AllocateAddress",
                "ec2:AssignIpv6Addresses",
                "ec2:AssignPrivateIpAddresses",
                "ec2:AssociateAddress",
                "ec2:AssociateDhcpOptions",
                "ec2:AssociateRouteTable",
                "ec2:AssociateSubnetCidrBlock",
                "ec2:AssociateVpcCidrBlock",
                "ec2:AttachClassicLinkVpc",
                "ec2:AttachInternetGateway",
                "ec2:AttachNetworkInterface",
                "ec2:AttachVpnGateway",
                "ec2:AuthorizeSecurityGroupEgress",
                "ec2:AuthorizeSecurityGroupIngress",
                "ec2:CreateCarrierGateway",
                "ec2:CreateCustomerGateway",
                "ec2:CreateDefaultSubnet",
                "ec2:CreateDefaultVpc",
                "ec2:CreateDhcpOptions",
                "ec2:CreateEgressOnlyInternetGateway",
                "ec2:CreateFlowLogs",
                "ec2:CreateInternetGateway",
                "ec2:CreateLocalGatewayRouteTableVpcAssociation",
                "ec2:CreateNatGateway",
                "ec2:CreateNetworkAcl",
                "ec2:CreateNetworkAclEntry",
                "ec2:CreateNetworkInterface",
                "ec2:CreateNetworkInterfacePermission",
                "ec2:CreateRoute",
                "ec2:CreateRouteTable",
                "ec2:CreateSecurityGroup",
                "ec2:CreateSubnet",
                "ec2:CreateTags",
                "ec2:CreateVpc",
                "ec2:CreateVpcEndpoint",
                "ec2:CreateVpcEndpointConnectionNotification",
                "ec2:CreateVpcEndpointServiceConfiguration",
                "ec2:CreateVpcPeeringConnection",
                "ec2:CreateVpnConnection",
                "ec2:CreateVpnConnectionRoute",
                "ec2:CreateVpnGateway",
                "ec2:DeleteCarrierGateway",
                "ec2:DeleteCustomerGateway",
                "ec2:DeleteDhcpOptions",
                "ec2:DeleteEgressOnlyInternetGateway",
                "ec2:DeleteFlowLogs",
                "ec2:DeleteInternetGateway",
                "ec2:DeleteLocalGatewayRouteTableVpcAssociation",
                "ec2:DeleteNatGateway",
                "ec2:DeleteNetworkAcl",
                "ec2:DeleteNetworkAclEntry",
                "ec2:DeleteNetworkInterface",
                "ec2:DeleteNetworkInterfacePermission",
                "ec2:DeleteRoute",
                "ec2:DeleteRouteTable",
                "ec2:DeleteSecurityGroup",
                "ec2:DeleteSubnet",
                "ec2:DeleteTags",
                "ec2:DeleteVpc",
                "ec2:DeleteVpcEndpoints",
                "ec2:DeleteVpcEndpointConnectionNotifications",
                "ec2:DeleteVpcEndpointServiceConfigurations",
                "ec2:DeleteVpcPeeringConnection",
                "ec2:DeleteVpnConnection",
                "ec2:DeleteVpnConnectionRoute",
                "ec2:DeleteVpnGateway",
                "ec2:DescribeAccountAttributes",
                "ec2:DescribeAddresses",
                "ec2:DescribeAvailabilityZones",
                "ec2:DescribeCarrierGateways",
                "ec2:DescribeClassicLinkInstances",
                "ec2:DescribeCustomerGateways",
                "ec2:DescribeDhcpOptions",
                "ec2:DescribeEgressOnlyInternetGateways",
                "ec2:DescribeFlowLogs",
                "ec2:DescribeInstances",
                "ec2:DescribeInternetGateways",
                "ec2:DescribeIpv6Pools",
                "ec2:DescribeLocalGatewayRouteTables",
                "ec2:DescribeLocalGatewayRouteTableVpcAssociations",
                "ec2:DescribeKeyPairs",
                "ec2:DescribeMovingAddresses",
                "ec2:DescribeNatGateways",
                "ec2:DescribeNetworkAcls",
                "ec2:DescribeNetworkInterfaceAttribute",
                "ec2:DescribeNetworkInterfacePermissions",
                "ec2:DescribeNetworkInterfaces",
                "ec2:DescribePrefixLists",
                "ec2:DescribeRouteTables",
                "ec2:DescribeSecurityGroupReferences",
                "ec2:DescribeSecurityGroupRules",
                "ec2:DescribeSecurityGroups",
                "ec2:DescribeStaleSecurityGroups",
                "ec2:DescribeSubnets",
                "ec2:DescribeTags",
                "ec2:DescribeVpcAttribute",
                "ec2:DescribeVpcClassicLink",
                "ec2:DescribeVpcClassicLinkDnsSupport",
                "ec2:DescribeVpcEndpointConnectionNotifications",
                "ec2:DescribeVpcEndpointConnections",
                "ec2:DescribeVpcEndpoints",
                "ec2:DescribeVpcEndpointServiceConfigurations",
                "ec2:DescribeVpcEndpointServicePermissions",
                "ec2:DescribeVpcEndpointServices",
                "ec2:DescribeVpcPeeringConnections",
                "ec2:DescribeVpcs",
                "ec2:DescribeVpnConnections",
                "ec2:DescribeVpnGateways",
                "ec2:DetachClassicLinkVpc",
                "ec2:DetachInternetGateway",
                "ec2:DetachNetworkInterface",
                "ec2:DetachVpnGateway",
                "ec2:DisableVgwRoutePropagation",
                "ec2:DisableVpcClassicLink",
                "ec2:DisableVpcClassicLinkDnsSupport",
                "ec2:DisassociateAddress",
                "ec2:DisassociateRouteTable",
                "ec2:DisassociateSubnetCidrBlock",
                "ec2:DisassociateVpcCidrBlock",
                "ec2:EnableVgwRoutePropagation",
                "ec2:EnableVpcClassicLink",
                "ec2:EnableVpcClassicLinkDnsSupport",
                "ec2:ModifyNetworkInterfaceAttribute",
                "ec2:ModifySecurityGroupRules",
                "ec2:ModifySubnetAttribute",
                "ec2:ModifyVpcAttribute",
                "ec2:ModifyVpcEndpoint",
                "ec2:ModifyVpcEndpointConnectionNotification",
                "ec2:ModifyVpcEndpointServiceConfiguration",
                "ec2:ModifyVpcEndpointServicePermissions",
                "ec2:ModifyVpcPeeringConnectionOptions",
                "ec2:ModifyVpcTenancy",
                "ec2:MoveAddressToVpc",
                "ec2:RejectVpcEndpointConnections",
                "ec2:RejectVpcPeeringConnection",
                "ec2:ReleaseAddress",
                "ec2:ReplaceNetworkAclAssociation",
                "ec2:ReplaceNetworkAclEntry",
                "ec2:ReplaceRoute",
                "ec2:ReplaceRouteTableAssociation",
                "ec2:ResetNetworkInterfaceAttribute",
                "ec2:RestoreAddressToClassic",
                "ec2:RevokeSecurityGroupEgress",
                "ec2:RevokeSecurityGroupIngress",
                "ec2:UnassignIpv6Addresses",
                "ec2:UnassignPrivateIpAddresses",
                "ec2:UpdateSecurityGroupRuleDescriptionsEgress",
                "ec2:UpdateSecurityGroupRuleDescriptionsIngress"
            ],
            "Resource": "*"
        }
    ]
}

e. 	attach it to vilas1 (from Policy usage)
---------------------------------------------------------------------
(c)
Permissions boundaries – 
	Use a managed policy 
		as the permissions boundary 
			for an IAM entity 
				(user or role). 
		defines maximum permissions 
			that the identity-based policies 
				can grant to an entity
				but does not grant permissions. 
		Permissions boundaries 
			do not define 
				maximum permissions that a resource-based policy 
					can grant to an entity.

(d)
Organizations SCPs (service control policy) – 
	
	maximum permissions 
		for account members 
			of an organization or organizational unit (OU). 
			
(e)
Access control lists (ACLs) – 
	control 
		which principals 
			in other accounts 
				can access the resource 
					to which the ACL is attached. 
		similar to resource-based policies
		only policy type that does not use the JSON policy document structure. 
		cross-account permissions policies 
		cannot grant permissions to entities within the same account.
(f)
Session policies – 
	Pass advanced session policies 
		when you use 
			AWS CLI or 
			AWS API 
				to assume a role or a federated user. 
	Session policies limit the permissions that the role or user's identity-based policies grant to the session. 
	Session policies limit permissions for a created session, but do not grant permissions. For more information, see Session Policies.



--------------------------------------------------------------------------------------------------------
	Roles
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html



https://www.knowledgehut.com/tutorials/aws/iam-roles
	How to create an IAM role using the AWS Management Console?
	
	
Role for an user from different account	
	An IAM role 
		can be created from	
			AWS Management Console, 
			AWS CLI, 
			Tools for Windows PowerShell or 
			IAM API.

	Sign in to the AWS Management Console.
	Open the IAM console.
	Creating IAM roles
	In the navigation pane of the console
		click on the ‘Roles’ 
		choose ‘Create role’ option.
		Creating IAM roles
	
	Click on ‘Another AWS account’ role type.
		For the ‘Account ID’
			type the AWS Account ID 
				to which permissions need to be granted.
	The administrator of the account 
		has the ability to grant permission 
			to give this role to any IAM user of that account.

AWS IAM Role
	similar to user
	an identity with permission policies 
		determine what the identity 
			can and 
			cannot do 
		identiy 
			external account
			resources
				
	not used to 
		associate 
			with a particular 
				user, 
				group.
	does not have any static credentials 
		password or access keys
	whoever assumes the role 
		provided with dynamic temporary credentials.
		can access delegation 
			to grant permissions to someone 
			that allows access to resources that you control.
	help to prevent accidental access 
		to or modification of sensitive resources.
	Modification of a Role 
		can be done anytime 
		changes propogate immediately.
	Advantages of roles
		Services like EC2 instances 
			needs to access other 
				AWS services
				e.g. s3.
		Cross-Account access – 
			users from different AWS accounts 
				have access to AWS resources 
					in a different account
			no need to create a user.
		Identity Providers & Federation
			Company uses a Corporate Authentication mechanism 
				User doesn't need to authenticate twice 
				or create duplicate users in AWS
			(Federation)Applications allowing login 
				through external authentication mechanisms 
					e.g. Amazon, Facebook, Google, etc
		Role can be assumed by
			IAM user within the same AWS account
			IAM user from a different AWS account
			AWS services such as 
				EC2, 
				EMR 
					to interact with other services
		An external user authenticated 
			by an external identity provider (IdP) service 
				that is compatible with 
					SAML 2.0 or 
					OpenID Connect (OIDC), or 
					a custom-built identity broker.
		Role involves defining two policies
	Trust policy
		defines –
			who can assume the role
		involves 
			setting up a trust 
				between the account 
					that owns the resource 
						(trusting account) and 
						account that owns the user 
							that needs access to the resources (trusted account).
	Permissions policy
		defines – 
			what they can access
		determines 
			authorization
				grants the user of the role 
					with the needed permissions 
						to carry out the desired tasks on the resource
		Federation is creating a trust relationship 
			between an external Identity Provider (IdP) and AWS.
		Users can 
			sign in to an enterprise identity system 
				that is compatible with SAML

	Users can sign in to a 
		web identity provider, 
		such as Login with 
			Amazon, 
			Facebook, 
			Google, or any 
			IdP 
				that is compatible with 
					OpenID connect (OIDC).
	When using 
		OIDC and SAML 2.0 
			to configure a trust relationship between these 
				external identity providers and AWS
					user is assigned an IAM role 
					receives temporary credentials 
					enable the user to access AWS resources.



	
	IAM 
		consists of a list of 
			AWS managed policies and 
			customer managed policies 
		A policy can be chosen from this or click on ‘Create Policy’ to open a new browser tab and create a new policy.
	
	Once the policy has been created
		user needs to return to the original tab.
	
	Click on the check box which is present next to the permission policies
		thereby indicating that the specific user has the permission to take up the role.
	
	policies can be attached to a role 
		
lab
https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-access-s3-bucket/

https://www.youtube.com/watch?v=NHAuCWIHevk
	Create role with s3 full access
	click	
		Create role
		"Services" - ec2
		add permission  
			Search for S3FullAccess
			Give a name
			Create a role
		Verify the permission
		
	Launch instance with role
		While creating a new instance
		select "<role you created>" in IAM Role
	Access s3 bucket with cli commands
		aws s3 mb s3://<bucketname>
		aws s3 ls
		aws s3 rb s3://<bucketname>
--------------------------------------------------------------------------------------------------------
o	LAB- IAM – 
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	IAM UI walkthrough
--------------------------------------------------------------------------------------------------------
Login 
Different services quick guide
Region concept
https://www.youtube.com/watch?v=A43m4TDFCUM&list=PLv2a_5pNAko0Mijc6mnv04xeOut443Wnk&index=5

https://aws.amazon.com/about-aws/global-infrastructure/
	click "AWS Regional services"
	
	list of services supported in each region.
	
		AWS Services
		AWS Documentation
		Support
		Region selection
		Account Setting (will follow)
	


--------------------------------------------------------------------------------------------------------
	Creating and Managing (aws cli)
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	•	Roles (aws cli)
--------------------------------------------------------------------------------------------------------
	
D:\PraiseTheLord\HSBGInfotech\Others\vilas\aws\awscli\roles\instructions
		
		
		
	https://stackoverflow.com/questions/58725179/how-do-i-create-a-role-with-aws-managed-policy-using-aws-cli
	
--------------------------------------------------------------------------------------------------------
	•	Permissions (aws cli)
--------------------------------------------------------------------------------------------------------
	above assignment completes it
--------------------------------------------------------------------------------------------------------
	•	Providing access to a user to (aws cli)
--------------------------------------------------------------------------------------------------------

aws iam attach-user-policy --policy-arn <valid policy arn> --user-name <valid user>
	valid policy arn: get it from the console
	
aws iam attach-user-policy --policy-arn arn:aws:iam::267092042432:policy/createvpc-subnet --user-name testuser1
	go to the console and verify 
--------------------------------------------------------------------------------------------------------
	o	Understanding API modes of operation
--------------------------------------------------------------------------------------------------------
	https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-api-modes-of-operation.html
	
The API operations 
	work with AWS account's attributes 
	work in one of two modes of operation:
		Standalone context
		Organizations context
		
		Standalone context 
			user or role 
				accesses or changes 
					an account attribute 
						in the same account. 
			automatically used 
				don't include the AccountId parameter 
					call one of the Account Management 
						AWS CLI or 
						AWS SDK operations.

		Organizations context – 
			user or role 
				in an organization accesses or changes an account attribute 
					in a different member account 
						in the same organization. 
			automatically used 
				do include the AccountId parameter 
					when you call one of the Account Management 
						AWS CLI or 
						AWS SDK operation. 
			can call the operations in this mode 
				from only the management account of the organization
				or the 
					delegated admin account for Account Management.

	The AWS CLI and AWS SDK 
		can work in either standalone or organizations context.

Amazon Resource Names (ARN)

	if run an operation in standalone mode
		must be permitted to run the operation 
			with an IAM policy 
				includes a Resource element of either "*" to allow all resources, or an ARN 
					uses the syntax for a standalone account.

	if run an operation in organizations mode
		must be permitted 
			with an IAM policy 
				includes a Resource element of either "*" to allow all resources, or an ARN 
					uses the syntax for a member account in an organization.

	Granting permissions to update account attributes
		with most AWS operations
			grant permissions to 
				add, 
				update, or 
				delete account attributes for 
					AWS accounts by using 
						IAM permission policies. 
		
	The following are some Account Management specific considerations for creating a permissions policy.

	Amazon Resource Name format for AWS accounts
	The Amazon Resource Name (ARN) for an AWS account that you can include in the resource element of a policy statement is constructed differently based on whether the account you want to reference is a standalone account or an account that is in an organization. See the previous section on Understanding API modes of operation.

An account ARN for a standalone account:


arn:aws:account::{AccountId}:account
You must use this format when you run an account attributes operation in standalone mode by not including the AccountID parameter.

An account ARN for a member account in an organization:


arn:aws:account::{ManagementAccountId}:account/o-{OrganizationId}/{AccountId}
You must use this format when you run an account attributes operation in organizations mode by including the AccountID parameter.

Context keys for IAM policies
The Account Management service also provides several Account Management service-specific condition keys that provide fine-grained control over the permissions you grant.

account:AccountResourceOrgPaths
The context key account:AccountResourceOrgPaths lets you specify a path through your organization's hierarchy to a specific organizational unit (OU). Only member accounts that are contained by that OU match the condition. The following example snippet restricts the policy to apply to only accounts that are in either of two specified OUs.

Because account:AccountResourceOrgPaths is a multi-valued string type, you must use the ForAnyValue or ForAllValues multi-value string operators. Also, note that the prefix on the condition key is account, even though you're referencing paths to OUs in an organization.


"Condition": {
    "ForAnyValue:StringLike": {
        "account:AccountResourceOrgPaths": [
            "o-aa111bb222/r-a1b2/ou-a1b2-f6g7h111/*", 
            "o-aa111bb222/r-a1b2/ou-a1b2-f6g7h222/*"
        ]
    }
}
account:AccountResourceOrgTags
The context key account:AccountResourceOrgTags lets you reference the tags that can be attached to an account in an organization. A tag is a key/value string pair that you can use to categorize and label the resources in your account. For more information about tagging, see Tag Editor in the AWS Resource Groups User Guide. For information about using tags as part of an attribute-based access control strategy, see What is ABAC for AWS in the IAM User Guide. The following example snippet restricts the policy to apply to only accounts in an organization that have the tag with the key project and a value of either blue or red.

Because account:AccountResourceOrgTags is a multi-valued string type, you must use the ForAnyValue or ForAllValues multi-value string operators. Also, note that the prefix on the condition key is account, even though you're referencing the tags on an organization's member account.


"Condition": {
    "ForAnyValue:StringLike": {
        "account:AccountResourceOrgTags/project": [
            "blue", 
            "red"
        ]
    }
}




--------------------------------------------------------------------------------------------------------
	Multi-factor authentication
--------------------------------------------------------------------------------------------------------
https://aws.amazon.com/iam/features/mfa/

For increased security and to help protect the AWS resources, Multi-Factor authentication can be configured
IAM Best Practice – Enable MFA on Root accounts and privilege users
Multi-Factor Authentication can be configured using
Security token-based
AWS Root user or IAM user can be assigned a hardware/virtual MFA device
Device generates a six digit numeric code based upon a time-synchronized one-time password algorithm which needs to be provided during authentication
SMS text message-based (Preview Mode)
IAM user can be configured with the phone number of the user’s SMS-compatible mobile device which would receive a 6 digit code from AWS
SMS-based MFA is available only for IAM users and does not work for AWS root account
MFA needs to enabled on the Root user and IAM user separately as they are distinct entities. Enabling MFA on Root does not enable it for all other users
MFA device can be associated with only one AWS account or IAM user and vice versa
If the MFA device stops working or is lost, you won’t be able to login into the AWS console and would need to reach out to AWS support to deactivate MFA
MFA protection can be enabled for service api’s calls using “Condition”: {“Bool”: {“aws:MultiFactorAuthPresent”: “true”}} and is available only if the service supports temporary security credentials.
--------------------------------------------------------------------------------------------------------
o	IAM Best Practices for new accounts
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html
--------------------------------------------------------------------------------------------------------
o	Single sign-on
--------------------------------------------------------------------------------------------------------

https://aws.amazon.com/iam/identity-center/
	Can help you to integrate with third parties
		e.g. azure ad
		https://www.youtube.com/watch?v=vxvgpDZOuYw&list=PLR5Btp19bh0-V25GejKMxhA9FAGwHq7pw&index=1
		
	With AWS IAM Identity Center (successor to AWS Single Sign-On), you can manage sign-in security for your workforce identities, also known as workforce users. IAM Identity Center provides one place where you can create or connect workforce users and centrally manage their access across all their AWS accounts and applications. You can use multi-account permissions to assign your workforce users access to AWS accounts. You can use application assignments to assign your users access to IAM Identity Center enabled applications, cloud applications, and customer Security Assertion Markup Language (SAML 2.0) applications.



IAM Identity Center features
IAM Identity Center includes the following core features:

Workforce identities
Human users who are members of your organization are also known as workforce identities or workforce users. You can create workforce users and groups in IAM Identity Center, or connect and synchronize to an existing set of users and groups in your own identity source for use across all your AWS accounts and applications. Supported identity sources include Microsoft Active Directory Domain Services, and external identity providers such as Okta Universal Directory or Microsoft Azure AD.

Application assignments for SAML applications
With application assignments, you can grant your workforce users in IAM Identity Center single sign-on access to SAML 2.0 applications, such as Salesforce and Microsoft 365. Your users can access these applications in a single place, without the need for you to set up separate federation.

Identity Center enabled applications
AWS applications and services, such as Amazon Managed Grafana, Amazon Monitron, and Amazon SageMaker Studio Notebooks, discover and connect to IAM Identity Center automatically to receive sign-in and user directory services. This provides users with a consistent single sign-on experience to these applications with no additional configuration of the applications. Because the applications share a common view of users, groups, and group membership, users also have a consistent experience when sharing application resources with others.

Multi-account permissions
With multi-account permissions you can plan for and centrally implement IAM permissions across multiple AWS accounts at one time without needing to configure each of your accounts manually. You can create fine-grained permissions based on common job functions or define custom permissions that meet your security needs. You can then assign those permissions to workforce users to control their access over specific accounts.

AWS access portal
The AWS access portal provides your workforce users with one-click access to all their assigned AWS accounts and cloud applications through a simple web portal.
	
		
https://www.miniorange.com/amazon-web-services-(aws)-single-sign-on-(sso)
--------------------------------------------------------------------------------------------------------
o	Manage users, roles etc through AWS Cognito 
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html


Amazon Cognito Cheat Sheet
	user management and authentication service 
	can be integrated to 
		our web or mobile applications. 
	supports federation
		can authenticate users through 
			external identity provider 
			provides temporary security credentials 
				to access your app’s backend resources in AWS 
			or 
				any service behind Amazon API Gateway. 
		Amazon Cognito 
			works with external identity providers 
			that support 
				SAML or OpenID Connect, 
				social identity providers 
					(Facebook, Twitter, Amazon, Google, Apple) 
			An Amazon Cognito ID token is 
				represented as a JSON Web Token (JWT). 
			Amazon Cognito uses JSON Web Tokens for token authentication.

User Pools
	User pools 
		user directories 
		provide 
			sign-up and 
			sign-in options 
				for your app users.
	Users can 
		sign in to our 
			web or mobile app 
			through Amazon Cognito, 
		or 
			federate through a 
				third-party identity provider (IdP).
	You can use 
		aliasing feature 
			enable your users to 
				sign up or 
				sign in 
					with 
						email address and a password 
					or a 
						phone number and a password.
	User pools 
		A User Pool is like a directory of users.
		created in one AWS Region
		they store the user profile data only in that region. 
		You can also send user data to a different AWS Region.
	Tokens provided through user pools:
		Access tokens 
			contain scopes and groups 
			used to grant access to authorized resources. 
			can configured to expire 
				min. five minutes 
			or 
				as long as 24 hours.
		Refresh tokens 
			contain information necessary to obtain a new ID or access token. 
			can be configured to expire in as little as one hour or as long as ten years.
		
	Manage Users
		After you create a user pool
			can 
				create, 
				confirm, and 
				manage users accounts. 
		Amazon Cognito User Pools groups 
			lets you manage users and 
			their access to resources by 
				mapping IAM roles to groups.
		User accounts are added to your user pool in one of the following ways:
			The user signs up in your user pool’s client app
				can be 
					mobile or 
					web app.
			You can import the user’s account into your user pool.
			You can create the user’s account in your user pool and 
				invite the user to sign in.
			Sign up authflow 

Identity Pools 
	Use to federate users to your AWS services.
	enable to grant your users temporary AWS credentials to 
		access AWS services, such as 	
			Amazon S3 and 
			DynamoDB.
	support anonymous guest users, as well as the following identity providers:
		Amazon Cognito user pools
		Social sign-in with Facebook, Google, and Login with Amazon
		OpenID Connect (OIDC) providers
		SAML identity providers
		Developer authenticated identities
	To save user profile information
		identity pool needs to be integrated with a user pool.
	Amazon Cognito Identity Pools 
		can support unauthenticated identities 
			by providing a unique identifier and 
			AWS credentials for users who do not authenticate with an identity provider.
	The permissions for each authenticated and non-authenticated 
		user are controlled through IAM roles that you create.
	Once you have an OpenID Connect token, you can then trade this for temporary AWS credentials via the AssumeRoleWithWebIdentity API call in AWS Security Token Service (STS). This call is no different than if you were using Facebook, Google+, or Login with Amazon directly, except that you are passing an Amazon Cognito token instead of a token from one of the other public providers.


--------------------------------------------------------------------------------------------------------
o	Overview of AWS Artifact
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/artifact/latest/ug/what-is-aws-artifact.html
self-service central repository of 
	AWS’ security and compliance reports 
An audit artifact 
	piece of evidence that demonstrates 
		that an organization is following a 
			documented process or 
			meeting a specific requirement (business compliant). 


AWS Artifact Reports include the following:
	ISO,
	Service Organization Control (SOC) reports, 
	Payment Card Industry (PCI) reports, 
	and certifications that 
		validate the implementation and operating effectiveness of AWS security controls.
		
--------------------------------------------------------------------------------------------------------
o	Lab:
	Creating New Users on AWS Console
--------------------------------------------------------------------------------------------------------
	Follow the documented procedure
--------------------------------------------------------------------------------------------------------
	Install and Setup for AWS CLI 
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Aws config
--------------------------------------------------------------------------------------------------------

https://www.youtube.com/watch?v=X_fznJtSyV8

--------------------------------------------------------------------------------------------------------
	AWS SDK overview
--------------------------------------------------------------------------------------------------------
https://aws.amazon.com/developer/tools/
--------------------------------------------------------------------------------------------------------
	IAM Security tools
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Credential report (account level)

--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html
--------------------------------------------------------------------------------------------------------
•	Access advisor (user-level)

Goto the users dashboard.
	Select a user
	Click on "Access Advisor"
	

Access Advisor 
	shows the services that current user 
		can access and 
		when those services were last accessed. 
		Review this data to remove unused permissions. 

IAM console 
	gives you information on policies that were accessed by a user. This information is very useful to implement the least privilege principle for assigning permissions to your resources. Through access advisor, you can find out what permissions are used infrequently or permissions that are never used, you can then revoke these permissions to improve the security of your AWS account. The following figure shows a few policies in the access advisor that were last accessed 144 days back; there is no reason these policies should be attached to this user:


--------------------------------------------------------------------------------------------------------
https://aws.amazon.com/about-aws/whats-new/2019/06/now-use-iam-access-advisor-with-aws-organizations-to-set-permission-guardrails-confidently/
--------------------------------------------------------------------------------------------------------
o	IAM best practices
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html
(already covered)
https://jayendrapatil.com/aws-iam-best-practices/


AWS IAM Best Practices
AWS recommends the following AWS Identity and Access Management service – IAM Best Practices to secure AWS resources

	Root Account – 
		Don’t use & Lock away access keys
	Do not use the AWS Root account 
		which has full access to all the AWS resources and services including the Billing information.
	Permissions associated with the AWS Root account cannot be restricted.
	Do not generate the access keys, if not required
	If already generated and not needed, delete the access keys.
	If access keys are needed, rotate (change) the access key regularly
	Never share the Root account credentials or access keys, instead create IAM users or Roles to grant granular access
	Enable AWS multifactor authentication (MFA) on the AWS account
	User – Create individual IAM users
	Don’t use the AWS root account credentials to access AWS, and don’t share the credentials with anyone else.
	Start by creating an IAM User with an Administrator role that has access to all resources as the Root except the account’s security credentials.
	Create individual users for anyone who needs access to your AWS account and gives each user unique credentials and grant different permissions.
	Groups – Use groups to assign permissions to IAM users
	Instead of defining permissions for individual IAM users, create groups and define the relevant permissions for each group as per the job function, and then associate IAM users to those groups.
	Users in an IAM group inherit the permissions assigned to the group and a User can belong to multiple groups
	It is much easier to add new users, remove users and modify the permissions of a group of users.
	Permission – Grant least privilege
	IAM user, by default, is created with no permissions
	Users should be granted LEAST PRIVILEGE as required to perform a task.
	Starting with minimal permissions and adding to the permissions as required to perform the job function is far better than granting all access and trying to then tighten it down.
	Passwords – Enforce strong password policy for users
	Enforce users to create strong passwords and enforce them to rotate their passwords periodically.
	Enable a strong password policy to define password requirements forcing users to create passwords with requirements like at least one capital letter, one number, and how frequently it should be rotated.
	MFA – Enable MFA for privileged users
	For extra security, Enable MultiFactor Authentication (MFA) for privileged IAM users, who are allowed access to sensitive resources or APIs.
	Role – Use temporary credentials with IAM roles
	Use roles for workloads instead of creating IAM user and hardcoding the credentials which can compromise the access and are also hard to rotate.
	Roles have specific permissions and do not have a permanent set of credentials.
	Roles provide a way to access AWS by relying on dynamically generated & automatically rotated temporary security credentials.
	Roles  associated with it but dynamically provide temporary credentials that are automatically rotated
	Sharing – Delegate using roles
	Allow users from same AWS account, another AWS account, or externally authenticated users (either through any corporate authentication service or through Google, Facebook etc) to use IAM roles to specify the permissions which can then be assumed by them

	A role can be defined that specifies what permissions the IAM users in the other account are allowed, and from which AWS accounts the IAM users are allowed to assume the role
	Rotation – Rotate credentials regularly
	Change your own passwords and access keys regularly and enforce it through a strong password policy. So even if a password or access key is compromised without your knowledge, you limit how long the credentials can be used to access your resources
	Access keys allows creation of 2 active keys at the same time for an user. These can be used to rotate the keys.
	Track & Review – Remove unnecessary credentials
	Remove IAM user and credentials (that is, passwords and access keys) that are not needed.
	Use the IAM Credential report that lists all IAM users in the account and the status of their various credentials, including passwords, access keys, and MFA devices and usage patterns to figure out what can be removed
	Passwords and access keys that have not been used recently might be good candidates for removal.
	Conditions – Use policy conditions for extra security
	Define conditions under which IAM policies allow access to a resource.
	Conditions would help provide finer access control to the AWS services and resources for e.g. access limited to a specific IP range or allowing only encrypted requests for uploads to S3 buckets etc.
	Auditing – Monitor activity in the AWS account
	Enable logging features provided through CloudTrail, S3, CloudFront in AWS to determine the actions users have taken in the account and the resources that were used.
	Log files show the time and date of actions, the source IP for an action, which actions failed due to inadequate permissions, and more.
	Use IAM Access Analyzer
	IAM Access Analyzer analyzes the services and actions that the IAM roles use, and then generates a least-privilege policy that you can use.

	Access Analyzer helps preview and analyze public and cross-account access for supported resource types by reviewing the generated findings.
	IAM Access Analyzer helps to validate the policies created to ensure that they adhere to the IAM policy language (JSON) and IAM best practices.
	Use Permissions Boundaries
	Use IAM Permissions Boundaries to delegate permissions management within an account
	IAM permissions boundaries help set the maximum permissions that you delegate and that an identity-based policy can grant to an IAM role.
	A permissions boundary does not grant permissions on its own.



--------------------------------------------------------------------------------------------------------
Overview of 
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Active Directory Federation Role
--------------------------------------------------------------------------------------------------------

(Not tried)
https://aws.amazon.com/blogs/security/aws-federated-authentication-with-active-directory-federation-services-ad-fs/

https://medium.com/reverberations-1/create-an-active-directory-federation-services-instance-on-amazon-aws-bae04c0c767d
https://www.youtube.com/watch?v=JLsF0auZlZY
--------------------------------------------------------------------------------------------------------
o	Web Identity Federation Role
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
		
--------------------------------------------------------------------------------------------------------
o	Understanding 
	EC2 
	What is ec2 instance?
	What is virtualization?
	
	-------------------------------------------------------------
	Private Vs Public Vs Elastic IP
	IPv4: 192.168.1.1
		most common 
		3.7 Billion 
		running out.
	IPv6:  2001:0db8:85a3:0000:0000:8a2e:0370:7334.
	
	An IP address 
		uniquely identify a network resource 
			to establish connection.

	
	Private IP: 
		like your nickname. 
		recognized in your private circles 
		identify network resources within a private network. 
		can’t identify it over internet. 
		some IPs dedicated for use as a private IP.
		defined in RFC 1918 addresses. 
		use Private IP from these addresses 
			not a strict requirement. 
		In AWS, Private IP 
			most used
			required during creation of 
				VPC, 
				Subnet 
				ec2 instance etc. 
		by default assigned to every instance on creation.
		hostname formed from this in aws

	Public IP: 
		like your mobile number. 
		People all over the world can identify and reach out to you using it. 
		temporary 
		no guarantee to have same number for ever. 
		
		Same mobile number can be allocated to other person once surrendered and you can also choose to have another mobile number at any time.

		
		routable address over internet. 
		AWS EC2 instances 
			can be assigned public ip 
				if you choose 
				but it is dynamic. 
			Stop and Start
				gets a different Public IP. 
		restart your instance 
			keep the same Public IP. 
		
		
	Elastic IP: 
		Like Aadhaar number or Social Security Number (SSN). 
		unique and permanently identify 
			for lifetime 
			does not change. 
		belongs to an account 
			can be attached to resources like 
				ec2 instance
				vpc 
				subnet etc
		doesn't change if you stop and start.
		default
			only 5 allowed in an account
		try avoid using it
		
	Hands on of ip's 
		stop/start
		reboot
		
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Essential Characteristics of Cloud
--------------------------------------------------------------------------------------------------------
https://www.techtarget.com/searchcloudcomputing/feature/7-key-characteristics-of-cloud-computing
1. On-demand self-service
2. Resource pooling
	multiple customers 
		share physical resources 
			using a multi-tenant model
3. Scalability and rapid elasticity
4. Pay-per-use pricing
5. Measured service
	metering capability optimizes resource utilization
6. Resiliency, Geogrpahical distribution and availability
	Resiliency: recover quickly from any disruption
7. Security
	e.g. 
		bastion host
		firewalls 
		easy to configure and cheaper security
8. Broad network access
	access cloud services 
		over the network and 
		on portable devices 
 			like 
				mobile phones, 
				tablets, 
				laptops, and 
				desktop computers. 
	A public cloud uses the internet; 
	a private cloud uses a local area network. 
	Latency and bandwidth 
		critical in cloud computing and broad network access 
9. Flexibility
	Scale latter
	Multiple options
10. Remote access

--------------------------------------------------------------------------------------------------------
o	Cloud Computing Deployment Models
--------------------------------------------------------------------------------------------------------

(Already covered)

https://www.geeksforgeeks.org/cloud-deployment-models/
Public cloud 
	Advantages of Public Cloud Model:
		Minimal Investment: 
			pay-per-use service
		No setup cost: 
		Infrastructure Management is not required: 
		No maintenance: 
		Dynamic Scalability: 
	Disadvantages of Public Cloud Model:
		Arguably Less secure: 
		Low customization: 
			limited customization according to personal requirements.
Private cloud
	exact opposite of the public cloud deployment model
	access systems and services 
		within a given border or organization.
	cloud platform 
		implemented in a cloud-based secure environment 
		protected by powerful firewalls and 
		under the supervision of an organization’s IT department. 
	more flexibility of control over cloud resources.
	Advantages of Private Cloud Model:
		Better Control: 
			sole owner 
			complete command over 
				service integration, 
				IT operations, 
				policies, and 
				user behavior. 
		Data Security and Privacy: 
			suitable for storing corporate information 
			authorized staff only have access. 
			segmenting resources in infrastructure, 
				improved security access  
		Supports Legacy Systems: 
			designed to work with legacy systems 
				which cannot access the public cloud. 
		Customization: 
			can tailor its solution 
				to meet its specific needs.
	Disadvantages of Private Cloud Model:
		Less scalable: 
			scalability is limited
				as there is less number of clients.
		Costly: 
			Private clouds are more costly as they provide personalized facilities.
Hybrid cloud
	bridge
		public and private clouds
		best of both worlds. 
		e.g. 
			host the app in a safe environment 
			take advantage of the public cloud’s cost savings. 
			move data and applications between different clouds 
	Advantages of  Hybrid Cloud Model:
		Flexibility and control: 
			flexibility 
			design personalized solutions 
				meet their particular needs.
		Cost: 
			Use scalability from public clouds provide
				pay as you use.
		Security: 
			Seperate data 
				data theft attack probablity reduced. 
	Disadvantages of Hybrid Cloud Model:
		Difficult to manage: 
			difficult to manage 
			Need to understand proprietary setup..
	Slow data transmission: 
		More data transfer 

Community cloud
	allows systems and services 
		to be accessible by a 
			group of organizations. 
	distributed system 
		created by integrating the services of 
			different clouds to 
				address the specific needs of a 
					community, 
					industry, or 
					business. 
	The infrastructure of the community 
		could be shared between 
			organization with 
				shared concerns or tasks. 
	generally managed by  
		third party 
		or 
			combination of one or more organizations in the community. 

	Advantages of Community Cloud Model:
		Cost Effective: 
			
		Security: 
		Shared resources: 
			allows you to share 
				resources, 
				infrastructure, 
				etc. with multiple organizations.
		Collaboration and data sharing: 
			suitable for both collaboration and data sharing.
	Disadvantages of Community Cloud Model:

		Limited Scalability: 
			relatively less scalable 
				many organizations share 
		Rigid in customization: 
			data and resources 
				shared among different organizations 
					on mutual interests 
				changes according to collaborative interests. 

	e.g. 
		Our government organization in India 
			may share computing infrastructure in the cloud to manage data.


Multi-cloud 
	use multiple cloud providers 
		at the same time 
	similar to the hybrid cloud deployment approach
		combines public and private cloud resources. 
	many public clouds. 
	rare that two distinct clouds 
		would have a catastrophic incident 
	improves the 
		high availability 
		
Advantages of a Multi-Cloud Model:
		can mix and match the best features/services
		Reduced Latency: 
			choose cloud regions and zones 	
				close to your clients. 
		High availability of service: 
			two clouds incident probablity is very less.
	Disadvantages of Multi-Cloud Model:
		Complex: 
		Security issue: 
			can have loopholes
--------------------------------------------------------------------------------------------------------
	o	Security boundary
--------------------------------------------------------------------------------------------------------
https://www.tutorialspoint.com/cloud_computing/cloud_computing_security.htm
Security in cloud computing 
	major concern. 
	Data in cloud 
		store in encrypted form. 
	restrict client from accessing data directly
		use 
			proxy and 
			brokerage 
				services should be employed.
	Security Boundaries
		A service model 
			defines 
				boundary between responsibilities of 
					service provider and 
					customer. 
	Cloud Security Alliance (CSA) stack model 
		defines the boundaries between each service model 
		shows how different functional units relate to each other. 
		
		Refer dia https://www.tutorialspoint.com/cloud_computing/cloud_computing_security.htm
		
	Key Points to CSA Model
	-----------------------
	1. IaaS 
		most basic level of service 
		provides infrastructure
	PaaS 
		next level 
		provides platform development environment
	SaaS.
		upper most
		provides operating environment
		
	Moving upwards
		each service 
			inherits capabilities and 
			security concerns 
				of the model beneath.

	2. IaaS 
		least level of integrated functionalities and 
		integrated security 
	while 
		SaaS has the most.

	3. CSA Model 
		describes the security boundaries 
			where 
				cloud service provider's responsibilities end and 
				customer's responsibilities begin.

	Any security mechanism below the security boundary 
		must be built into the system and 
		should be maintained by the customer.

	Each service model has security mechanism
		security needs 
			depend upon where these services are located
				in 
					private, 
					public, 
					hybrid or 
					community cloud.

Understanding Data Security
	Data 
		transferred through Internet
		data security 
			major concern in the cloud. 
	key mechanisms for protecting data.
		Access Control
		Auditing
		Authentication
		Authorization
	All service models 
		should incorporate 
			necessary security mechanism 
			
Isolated Access to Data
	data stored in cloud 
		can be accessed from anywhere
		must have a way to 
			isolate data and 
			protect it from client’s direct access.

	Brokered Cloud Storage Access 
		approach for isolating storage in the cloud. 
	In this approach, 
		two services are created:
			A broker 
				full access to storage 
				but no access to client.
			A proxy 
				no access to storage 
				access to both client and broker.
				
Working Of Brokered Cloud Storage Access System
	client request to access data:
		The client data request 
			goes proxy.
				forwards request to 
					broker.
						requests data from cloud storage system.
		The cloud storage system returns the data to the broker.
		The broker returns the data to proxy.
		Finally the proxy sends the data to the client.
		
		
Cloud Computing Brokered Cloud Storage Access
	Encryption
		protect data from compromise. 
		It protects 
			data in transit as 
			data in store 
		protect data from any unauthorized access
		doessnot prevent data loss.

--------------------------------------------------------------------------------------------------------
	o	View account identifiers
--------------------------------------------------------------------------------------------------------
	https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-identifiers.html
	AWS 
		provides below unique identifiers to each AWS account:
			AWS account ID
				A 12-digit number
				e.g. 123456789012
				uniquely identifies an AWS account. 
				Many AWS resources 
					include the account ID 
						in their Amazon Resource Names (ARNs). 
				IAM user
					can sign in to the AWS Management Console 
						using 
							account ID 
						or 
							account alias.

			Canonical user ID
				An alpha-numeric identifier
				e.g. 79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be, 
				obfuscated form of the AWS account ID. 
				can use this ID 
					to identify an AWS account 
					when granting cross-account access to buckets and objects using Amazon Simple Storage Service (Amazon S3). You can retrieve the canonical user ID for your AWS account as either the root user or an IAM user.


--------------------------------------------------------------------------------------------------------
	o	Modifying the account name, email address, or password for the AWS account root user
--------------------------------------------------------------------------------------------------------
Login as root user
	right top 
		click "Account" 
		click "Edit"
		
	Look at this entire page to understand lot more details.	

	to update a user's details  (not root)
		https://www.youtube.com/watch?v=a3Eka2_upWo
		
	
--------------------------------------------------------------------------------------------------------
	o	Understanding API modes of operation
--------------------------------------------------------------------------------------------------------
	Introduction to API mode of operations
		Different options
			cli
			boto3
		
	Outside aws: terraform

	1. install aws
	2. configure aws
	3. aws cli 
		aws iam list-users
		
		
	cheatsheet
		https://www.bluematador.com/learn/aws-cli-cheatsheet
		https://gist.github.com/apolloclark/b3f60c1f68aa972d324b
--------------------------------------------------------------------------------------------------------
	o	Updating contact information
--------------------------------------------------------------------------------------------------------
Login as root user
	right top 
		click "Account" 
--------------------------------------------------------------------------------------------------------
	o	Setting or changing security challenge questions
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	o	Specifying which AWS Regions your account can use
--------------------------------------------------------------------------------------------------------
Login as root user
	right top 
		click "Account" 
--------------------------------------------------------------------------------------------------------
	o	Setting or changing your AWS account alias
--------------------------------------------------------------------------------------------------------
Login as root user
	right top 
		click "Account" 
--------------------------------------------------------------------------------------------------------
	o	Closing your AWS account
--------------------------------------------------------------------------------------------------------
Login as root user
	right top 
		click "Account" 
		scroll to end
--------------------------------------------------------------------------------------------------------
	o	Using the AWS account root user
--------------------------------------------------------------------------------------------------------
Login as root user
	right top 
		click "Account" 
--------------------------------------------------------------------------------------------------------
			Activate MFA on the AWS account root user
--------------------------------------------------------------------------------------------------------
Login as root user
	right top 
		click "Security Credentials" 
		
		
		https://www.youtube.com/watch?v=OVaYHdYfaH4
		
		install google authenticator
--------------------------------------------------------------------------------------------------------
			Changing the password for the root user
--------------------------------------------------------------------------------------------------------
Login as root user
	right top 
		click "Account" 
		"Edit"
		
		
		Only iam users can change their own password
		Root users are not allowed to change password through cli.
		
		aws iam change-password --generate-cli-skeleton > change-password.json
		aws iam change-password --cli-input-json file://change-password.json
--------------------------------------------------------------------------------------------------------
			Creating and deleting access keys for the AWS account root user
--------------------------------------------------------------------------------------------------------
Login as root user
	right top 
		click "Security Credentials" 
--------------------------------------------------------------------------------------------------------
			Comparing AWS account root user credentials and IAM user credentials
--------------------------------------------------------------------------------------------------------

https://docs.aws.amazon.com/accounts/latest/reference/root-user-vs-iam.html

Root User
----------
Root Account Credentials 
	email address and 
	password 
		used during sign-in into the AWS account

	full unrestricted access to AWS account 
		including the account security credentials which include sensitive information
IAM Best Practice – 
	Do not use or share the Root account once the AWS account is created, instead create a separate user with admin privilege
An Administrator account can be created for all the activities which too has full access to the AWS account except the accounts security credentials, billing information and ability to change password

Root user credentials
---------------------
	credentials of the account owner 
		allow full access to all resources in the account. 
		can't use IAM policies 
			to explicitly deny the root user access to resources. 
		can use an AWS Organizations service control policy (SCP) 
			to limit the permissions of the root user of a member account. 
		So recommend 
			create an administrative user in IAM Identity Center 
				to use for everyday AWS tasks. 
			safeguard root user credentials 
			use them when there are no other options. 

IAM credentials
---------------
	An IAM user 
		entity you create in AWS 
		represents the person or service 
			uses the IAM user to 
				interact with AWS resources. 
		identities within your AWS account 
			have specific custom permissions. 
		have long-term credentials 
			can access AWS 
				using 
					AWS Management Console
				or 
					programmatically 
						using 
							the AWS CLI or 
							AWS APIs. 
			
--------------------------------------------------------------------------------------------------------
			Tasks that require root user credentials
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/accounts/latest/reference/root-user-tasks.html

--------------------------------------------------------------------------------------------------------
	o	AWS organization account management
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html


Account 
An AWS account is a container of AWS resources. Using multiple AWS accounts is a best practice for scaling environments, as it provides a natural billing boundary for costs, isolates resources for security, gives flexibility for individuals and teams, in addition to being adaptable for new business processes.

Organization 
An AWS Organization is a collection of AWS accounts that can be organized into a hierarchy and managed centrally. Organizations help to programmatically create new accounts and allocate resources, and simplify billing by setting up a single payment method for all accounts. In addition, AWS Organizations is integrated with other AWS services so admins can define central configurations, security mechanisms, and resource sharing across accounts.


User
An AWS user is an AWS identity created directly in the AWS IAM or AWS IAM Identity Center admin console that consists of a name and credentials.

Federated User
A federated user is a user identity that is created in and centrally managed and authenticated by an external identity provider. Federated users assume a role when accessing AWS accounts. 

Group 
A group is a collection of users. Groups let admins specify permissions for multiple users, which can make it easier to manage the permissions. Any user in that group automatically has the permissions that are assigned to the group. Any user removed from the group will lose those permissions. For instance, if Bob places a new employee into the Engineering group, which has access to the Lambda and DynamoDB production account, then the new employee will also be granted access to the resources in that account. 

Role
A role is similar to a user in that it is an AWS identity with permissions and policies that determine what the identity can and cannot do in an AWS account. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. A role does not have standard long-term credentials such as a password or access keys associated with it. Instead, when a user assumes a role, it provides them with a set of temporary security credentials for that session. Admins can use roles to delegate access to users, applications, or services that don’t normally have access to those AWS resources. 


AWS IAM Identity Center Permission Set
A permission set defines the level of access a user has to AWS resources within an AWS account. For Bob, once he provides access to the necessary accounts in AWS IAM Identity Center, he can use predefined or custom permission sets to control the level of access.




Recommend"
	configure an administrative user 
		in AWS IAM Identity Center 
	(successor to AWS Single Sign-On) 
		to perform daily tasks and access AWS resources. 
		
	Tasks listed below 
		can be done only by root user.

Tasks
	Change your account settings. 
		account name, 
		email address, 
		root user password
		root user access keys. 
		
	Following DOESTN'T require root user credentials	
		Other account settings, 
			contact information, 
			payment currency preference
			AWS Regions, .

	Restore IAM user permissions. 
		IAM administrator revokes their own permissions, 
		only root user can edit policies and 
			restore those permissions.

	Activate IAM access to the Billing and Cost Management console.

	View certain tax invoices. 
		An IAM user with the "aws-portal:ViewBilling" permission 
			can view and download VAT invoices from AWS Europe, 
			but not AWS Inc. or Amazon Internet Services Private Limited (AISPL).

	Close your AWS account.

	Register as a seller 
		in the Reserved Instance Marketplace.

	Configure an Amazon S3 bucket to enable MFA 
		(MFA: multi-factor authentication).

	Edit or delete an Amazon Simple Storage Service (Amazon S3) bucket policy 
		that includes an invalid virtual private cloud (VPC) ID or VPC endpoint ID.

	Sign up for AWS GovCloud (US).
	Request AWS GovCloud (US) account root user access keys from AWS Support.
	
	AWS organization
		centrally manage and govern aws accounts
		
		create an organization 
			connect multiple accounts
				create a new account 
			or 
				connect an existing account
	management account
		account that created the organization
		get consolidated bill for all accounts
		
	group accounts in an hierarchy 
		assign policies
			SCP: service control policies
			
	SCP
		like IAM permissions
			but DON'T grant permissions
		specify the max. permissions for an 
			OU (Organizational unit)
			accounts
		two types of scp strategies
			Deny list strategy (default)
			Allow list strategies
	
	
	Lab:
		Login as root user
		right top 
			click "Organization" 
			you have to enable 
			that send's a mail to your account
			Verify email in the inbox.
			
	list accounts
		aws organizations list-accounts | jq '.Accounts | length'
	default - should allow 10
		but there are bugs 
		raise customer support request.
		
		https://stackoverflow.com/questions/73356254/aws-organizations-you-have-exceeded-the-allowed-number-of-aws-accounts
		
		Create an ou for security
			create a child ou for
				production
				non production
				
			create a child ou of above
				move it 
				
				
https://www.youtube.com/watch?v=jtKqYM9GjNM				

	
--------------------------------------------------------------------------------------------------------
			Enabling trusted access for AWS Account Management
--------------------------------------------------------------------------------------------------------
	https://docs.aws.amazon.com/accounts/latest/reference/using-orgs-trusted-access.html
	To enable 
		management account in your organization 
			to call the AWS Account Management API operations 
				for other member accounts in the organization, use the following procedure.

Minimum permissions
	To perform these tasks, you must meet the following requirements:
	You can perform this only from the organization's management account.
	Your organization must have all features enabled.

--------------------------------------------------------------------------------------------------------
			Enabling a delegated admin account for AWS Account Management
https://docs.aws.amazon.com/accounts/latest/reference/using-orgs-delegated-admin.html

A delegated admin account 
	can call AWS Account Management API operations 
		for other member accounts in the organization. 
	To designate a member account in your organization as a delegated admin account, use the following procedure.

Minimum permissions
	To perform these tasks, you must meet the following requirements:
		You can perform this only from the organization's management account.
		Your organization must have all features enabled.
		You must have enabled trusted access for Account Management in your organization.

This task isn't supported in the AWS Account Management management console. You can perform this task only by using the AWS CLI or an API operation from one of the AWS SDKs.



https://www.youtube.com/watch?v=HgKkFoc7eRY

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "DelegatingNecessaryDescribeListActions",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::<<account id of a diff. root user>>:root"
      },
      "Action": [
        "organizations:DescribeOrganization",
        "organizations:DescribeOrganizationalUnit",
        "organizations:DescribeAccount",
        "organizations:DescribePolicy",
        "organizations:DescribeEffectivePolicy",
        "organizations:ListRoots",
        "organizations:ListOrganizationalUnitsForParent",
        "organizations:ListParents",
        "organizations:ListChildren",
        "organizations:ListAccounts",
        "organizations:ListAccountsForParent",
        "organizations:ListPolicies",
        "organizations:ListPoliciesForTarget",
        "organizations:ListTargetsForPolicy",
        "organizations:ListTagsForResource"
      ],
      "Resource": "*"
    }
  ]
}


 aws sts get-caller-identity --query Account --output text

aws organizations register-delegated-administrator --account-id <<account id of a diff. root user>>     --service-principal account.amazonaws.com




--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
			Restricting access with AWS Organizations service control policies

https://www.youtube.com/watch?v=_50P0o14Ul0
		Service control policies (SCPs)
		-------------------------------
		define max. an org. unit (ou) has 
		applicable to hierarchically all children's also
		not a replacement for IAM
			need to still give IAM for user
			
			
	Service control policies (SCPs) 
		type of organization policy 
		use to manage permissions in your organization. 
		central control over 
			maximum available permissions for all accounts in your organization. 
		help you to ensure 
			accounts stay within your organization’s access control guidelines. 
		available only in an organization 
			that has all features enabled. 
		aren't available 
			if your organization has enabled 
				only the consolidated billing features. 
		

	SCPs 
		alone are not sufficient to granting permissions 
			to the accounts in your organization. 
		No permissions are granted by SCP. 
		defines a max sets limits
		administrator must still attach 
			identity-based or resource-based policies 
				to IAM users or roles
					to grant permissions. 
		The effective permissions 
			logical intersection between 
				what is 
					allowed by 
						ALL SCP (at diff. hierarchical level) and 
						IAM and resource-based policies.		
						
	Restrictions defined may be applicable to the root user as well.


	Refer "Getting started" in organization 

how to do it?
https://aws.amazon.com/blogs/security/how-to-use-service-control-policies-to-set-permission-guardrails-across-accounts-in-your-aws-organization/	
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples.html

https://www.youtube.com/watch?v=_50P0o14Ul0
	from 15:37
	
	
Use policy
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "DenyAllOutsideEU",
      "Effect": "Deny",
      "NotAction": [
        "a4b:*",
        "acm:*",
        "aws-marketplace-management:*",
        "aws-marketplace:*",
        "aws-portal:*",
        "budgets:*",
        "ce:*",
        "chime:*",
        "cloudfront:*",
        "config:*",
        "cur:*",
        "directconnect:*",
        "ec2:DescribeRegions",
        "ec2:DescribeTransitGateways",
        "ec2:DescribeVpnGateways",
        "fms:*",
        "globalaccelerator:*",
        "health:*",
        "iam:*",
        "importexport:*",
        "kms:*",
        "mobileanalytics:*",
        "networkmanager:*",
        "organizations:*",
        "pricing:*",
        "route53:*",
        "route53domains:*",
        "s3:GetAccountPublic*",
        "s3:ListAllMyBuckets",
        "s3:PutAccountPublic*",
        "shield:*",
        "sts:*",
        "support:*",
        "trustedadvisor:*",
        "waf-regional:*",
        "waf:*",
        "wafv2:*",
        "wellarchitected:*"
      ],
      "Resource": "*",
      "Condition": {
        "StringNotEquals": {
          "aws:RequestedRegion": [
            "eu-central-1",
            "eu-west-1",
            "us-east-1",
            "us-east-2",
            "us-west-1",
            "us-west-2"
          ]
        },
        "ArnNotLike": {
          "aws:PrincipalARN": [
            "arn:aws:iam::*:role/Role1AllowedToBypassThisSCP",
            "arn:aws:iam::*:role/Role2AllowedToBypassThisSCP"
          ]
        }
      }
    }
  ]
}	


--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	o	Best practices
--------------------------------------------------------------------------------------------------------
https://summitroute.com/blog/2020/03/25/aws_scp_best_practices/

Go to IAM to get recommendations
	Create a new root account 
		Delete root account access keys
		Activate MFA on your root account
		Create individual iam 
			users
			groups
		Assign permissions
		Apply an IAM password policy
	

--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
Day 2
•	AWS EC2 Instances (AWS Elastic Compute Cloud - AWS EC2)
--------------------------------------------------------------------------------------------------------

aws ec2 run-instances  --image-id ami-0cc87e5027adcdca8 --count 1  --instance-type t2.micro  --key-name vilas 


aws ec2 run-instances  --image-id ami-0cc87e5027adcdca8 --count 1  --instance-type t2.micro  --key-name vilas  --security-group-ids sg-07570e17ab8331f13 \
    --subnet-id subnet-00b5ede5e160caa59 \
    --block-device-mappings "[{\"DeviceName\":\"/dev/sdf\",\"Ebs\":{\"VolumeSize\":30,\"DeleteOnTermination\":false}}]" \
    --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=demo-server}]' 'ResourceType=volume,Tags=[{Key=Name,Value=demo-server-disk}]'	
--------------------------------------------------------------------------------------------------------
•	Understanding EC2 Instance Types
--------------------------------------------------------------------------------------------------------

https://mindmajix.com/aws-ec2-instance-types
What are the AWS EC2 Instance Types?
Here are different types of EC2 Instances:

	General Purpose Instances
	Compute Optimized Instances
	Memory-Optimized Instances
	Accelerated Computing Instances
	Storage Optimized Instances


Type #1 - General Purpose Instances
	most popular and widely used EC2 instance types
	good choice if you are new to cloud computing or AWS in general. 
	wide balance of 
		computing power, 
		memory, and 
		storage, it is suited for a majority of AWS workloads.

EC2 General-purpose instance Types
Here are the details of the various general-purpose instances that you can choose from:

i. A1 instance
	A1 instances are the ARM-based offering of EC2 instance types – as compared to others that use either Intel or AMD processing. This type of instance is more suited for web servers and containerized microservices – along with applications that are run on open source tools like Java or Python.

ii. M5 instance

This is the latest generation of general-purpose instances that is powered by the Intel Xeon Platinum 8175 3.1Ghz processor. With their cloud computing power, M5 instances provide a balance of computing, memory, and networking power. This instance type is suited for small-to-midsize databases, data processing tasks, and as a backend server for enterprise applications like SAP or SharePoint.

iii. T3/ T3a instance

Available with burstable instances, T3 and T3a are the  
general-purpose instance types powered with 
	Intel and AMD processors. 
	cheaper and less powerful option 
		than the M5 fixed instance. They are commonly used for long-lasting application instances such as websites, web applications, and code repositories.

Also Read: Learn here AWS Certifications List and their Path
 

Type #2 - Compute Optimized Instances
	compute-optimized instances 
		use for compute-intensive 
		e.g. 
			web servers and 
			scientific modeling.

EC2 Compute Optimized instances Types
Here are the details of the two types of Compute-optimized instances that you can choose from:

i. C5/ C5n instance

The C5 series of instances 
	online gaming, 
	scientific modeling, 
	media transcoding – 
		which require raw computing power. 
	Intel Xeon Platinum processor 
		have recorded a 25% improvement in speed as 
			compared to the previous C4 instance generation. 
		With the C5d instance type, you can physically connect the NVME-based SSD device to the host server to provide block-level storage for the entire instance lifetime.

ii. C6/ C6g instance

	AWS Graviton2 series of processors 
	suited for highly intensive and 
	advanced applications such as high-performance computing, 
		video encoding, 
		ad serving, and 
		distributed analytics. 
	40% improvement in price-performance as 
		compared to the C5 instance family.


Type #3 - Memory-Optimized Instances
	used for memory-intensive workloads 
	process large datasets at a fast speed. 
	Examples 
		Big Data analytics or those 
			running on 
				Hadoop or 
				Apache Spark.

	EC2 Memory Optimized instances Types
	Here are the details of the different types of memory-optimized instances that you can choose from:

i. R5/ R5a/ R5n instance

	high memory consumption 
	high-performance databases
	real-time Big data analytics, 
	large in-memory cache applications. 
	
	R5 Intel
	R5a AMD processors – 
	AWS Nitro system that provides easy access to the computing and memory resources of the server. 

As compared to the previous R4 type, R5 instances offer 
	5% more memory for each vCPU along with a 
	10% price improvement per GiB over R4.

ii. R6g/ R6gd instance

	AWS Graviton2 processor, 
	R6 instances are suited for high memory 
		open-source databases 
			(example, MySQL) and 
			in-memory caching (example, KeyDB). 
	custom-built AWS Graviton2 processor 
	equipped with a 64-bit ARM Neoverse core. 
	In the R6gd instance, the local NVME-based SSD drive is physically connected to the host server, thus enabling block-level storage. 

iii. X1/ X1e instance

	Powered by the Intel Xeon processor, the X1 family of memory-optimized instances are designed to provide high computational memory for memory-intensive applications like SAP HANA, Apache Spark, and for high-performance computing. Among all EC2 instances, the X1e instance type provides the highest memory-to-compute ratio at the lowest price calculated for each GiB of RAM.

iv. High memory instance

	high memory instances 
	provide the highest capacity of RAM – 
	ranging from 6TB to 24TB in a single instance. 
	High memory instances are used to run high in-memory databases – 
		including the deployment of the 
			SAP HANA database to the cloud platform.

High memory instances are only available on dedicated hosts – where you need to commit to running instances for a 3-year period.

 

Type #4 - Accelerated Computing Instances
	use additional hardware accelerators 
		like Graphics Processing Units (or GPUs) and 
		Field Programmable Gate Arrays (or FPGAs) 
		enable higher throughput in compute-intensive applications 
			with more parallelism. 
			For example, with 
				GPU-powered instances
				applications can access NVIDIA GPUs that have thousands of computing cores. 

Similarly, FPGA-powered instances provide applications with access to large FPGAs with millions of parallel logic cells.

This instance type is suitable for applications that require parallel processing. This includes graphic processing, floating-point calculators, and data pattern matching.

EC2 Accelerated Computing instances Types
Here are the details of the different types of accelerated computing instances that you can choose from:

i. P3 instance

The latest generation of GPU-based instances uses up to 8 NVIDIA Tesla GPUs and is powered by a high-frequency Intel Xeon processor. This instance type supports NVLink for peer-based GPU communication and provides up to 100Gbps of network bandwidth. 

ii. P2 instance

Designed for general-purpose GPU compute applications, P2 instances are powered by the Intel Xeon processor and feature high-performance NVIDIA K80 GPUs. P2 instances provide capabilities like high-performance networking and double-precision floating-point calculations – thus making it ideal for deep learning applications and high-performance databases.

iii. Inf1 instance

Featuring 16 AWS Inferential machine learning chips that enable low latency and cost-effective performances, Inf1 instances are powered by Intel Xeon 2nd generation processors. API developers can also work with AWS Deep Learning APIs that are bundled with AWS Inferential.

iv. G3 instance

Designed with the NVIDIA Tesla M60 GPU, G3 instances provide high-performance and cost-effective solutions for graphic-intensive applications using DirectX or OpenGL. Some of the graphics-related tasks include graphic rendering and streaming. G3 instances are powered by high-frequency Intel Xeon processors and can enable the NVIDIA Grid Virtual Workstation functionality.

v. G4 instance

Powered by a 2nd generation Intel Xeon Scalable processor, G4 instances are suited for accelerating machine learning inference and graphics-intensive workloads. Designed with the NVIDIA T4 Tensor Core GPU, this instance type also supports the NVIDIA Grid Virtual Workstation feature.

vi. F1 instance

F1 instances offer hardware acceleration using FPGAs. Powered with the high-frequency Intel Xeon processor, F1 instances feature NVMe SSD storage and support for enhanced networking. You can also use customized FPGA AMIs for quicker development and deployment of applications.

Also Read: Learn AWS vs Azure
 

Type #5 - Storage Optimized Instances
	used for applications that 
		have high storage requirements, 
		particularly with 
			sequential read and write applications 
				like log processing. 
	Storage optimized instances are designed to deliver a high number of low latency and random I/O operations each second (or IOPS).

Storage optimized instances are also suitable for cloud-running applications that run the high transaction and low latency workloads in use cases such as in-memory databases, data warehousing, and data analytics.

EC2 Storage Optimized instances Types
Here are the details of the different types of storage optimized instances that you can choose from:

i. D2 instance

Featuring 48TB of HDD storage, D2 instances is a storage-optimized instance with high-density storage with high sequential read/write for large datasets – including large Hadoop distributed environments. Along with high disk throughput, D2 instances are available for the lowest cost per disk throughput on Amazon EC2. These instances are designed for applications like MPP data warehousing, MapReduce and Hadoop computing, and log processing.

ii. H1 instance

As compared to D2 instances, H1 instances offer lesser density storage with a maximum of 16TB HDD. Powered by an Intel Xeon processor, H1 instances offer high disk throughput and enhanced networking of up to 25Gbps. These instances are most suitable for data-intensive applications like MapReduce, and for applications that require high throughput and sequential access to large data volumes.

iii. l3/ l3en instance

The l3 instance family offers SSD storage that has lower latency as compared to HDD-based instances. The l3 instance features an NVMe SSD storage that provides high IOPS at low costs thanks to its offering of low latency and high random I/O performance. This instance family is most suitable for high-frequency OLTP systems, relational databases, and caching for in-memory databases like Redis.
--------------------------------------------------------------------------------------------------------
	https://aws.amazon.com/ec2/instance-types/
--------------------------------------------------------------------------------------------------------
	Understanding the AMI
--------------------------------------------------------------------------------------------------------
How to use Amazon Machine Image (AMI)?
AMI (Amazon Machine Image) provisions information required to launch an instance. An AMI needs to be specified when an instance is launched. But multiple instances can be launched with the help of a single AMI when multiple instances need the same configuration to run. Different AMIs are used to launch multiple instances when the configurations are different. 

An AMI consist of the following attributes: 

One or more EBS snapshots or a template for root volume of an instance (meant for the instance-store-backed AMIs). 
Launch permissions which help in controlling the AWS accounts that can use AMI to launch the instances.  
A block device mapping that is used to specify volumes that need to be attached to the instance during its launch.  
Using an AMI
An AMI lifecycle has been shown in the below snip:

Using an AMI

An AMI needs to be created and registered before it can be used to launch new instances.
An AMI can be copied within the same region or to other regions based on its availability. 
When the user doesn’t require the AMI anymore, they can deregister it.  A specific AMI can be searched for that would match the user’s criteria for the specific instance. AMIs provided by community and AMIs provided by AWS can be searched for.
Once an AMI has been used to launch an instance, the user can connect to it. Once the user is connected to the instance, it can be used like any other server.
Creating an AMI
An instance can be launched with the help of an already present AMI, customize the instance, and save this updated configuration as a custom AMI. When instances are launched from this new custom AMI, the updated changes are reflected in it. The root storage device of an instance determines which process needs to be followed to create an AMI. The root volume of an instance can be an Amazon EBS volume or an instance store volume.  

Creating an instance Store-backed Linux AMI:
The below image shows how an AMI can be created with the help of an instance store-backed instance:

Creating an instance Store-backed Linux AMI

Launch an instance from AMI.
Connect to the instance and customize it if required.
When the instance is ready to be launched, bundle it, which will take some time. It consists of an image manifest and files which contain template for the root volume.
Once the process is complete, upload the bundle to Amazon S3 bucket and register the AMI with the instance.
When an instance is launched with the AMI, the root volume is created for that instance with the help of the bundle which was uploaded to Amazon S3.
The storage space used by the bundle on Amazon S3 is chargeable.
If instance store volumes are added to the user instance along with the root device volume, the block device mapping for the new AMI will hold information for both these volumes.
The block device mappings will automatically contain information for the volumes.
Creating a Linux AMI from an instance
An AMI can be created with the help of AWS Management Console as well as AWS CLI.

The below image shows how a Linux AMI can be created from an instance:

Creating a Linux AMI from an instance

An appropriate EBS backed AMI has to be selected as a starting point to create the new AMI and configure it as required before launching it.

Click on ‘Launch’ to launch an instance which has been selected. The default values have to be accepted via the wizard.

When the instance is running, it needs to be connected and the below mentioned operations can be performed to customize it:

Copying data.
Installing software and applications.
Reducing start time since temporary files will be deleted, and hard drive will be defragmented.
Additional Amazon EBS volumes can be attached.
This is an optional step- Create snapshots of all the volumes which are attached to the instance.

In the navigation pane, click on the ‘Instances’, select the instance, click on ‘Action’, ‘Image’, ‘Create Image’.

In the ‘Create Image’ dialog box, specify the ‘Image name’, ‘Image description’, ‘No reboot’ as ‘No’ and the ‘Instance Volume’. Now click on ‘Create Image’.

The AMI takes a while to get created. To view its status, click on ‘AMIs’. The status is initially ‘pending’ and then goes to ‘available’ once it is successfully created.


--------------------------------------------------------------------------------------------------------
	Instance-type
--------------------------------------------------------------------------------------------------------
	already covered
--------------------------------------------------------------------------------------------------------
	Key pair
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	Network settings
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Firewall (security group)
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Ingress and Egress
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	Configure Storage
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Volume type
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html
--------------------------------------------------------------------------------------------------------
•	IOPS
--------------------------------------------------------------------------------------------------------
Provisioned IOPS volumes
	backed by solid-state drives (SSDs)
	highest performance Elastic Block Store (EBS) storage volumes 
	low latency.
	IOPS (input/output operations per second) 
		standard unit of measurement 
		for the maximum number of reads and writes 
			to non-contiguous storage locations. 
			IOPS is pronounced EYE-OPS.

io2 volumes
	available since 2020
	maximum of 64,000 IOPS 
	1,000 MB/s of throughput per volume
	4x more IOPS than general-purpose volumes. 
	io2 volumes 
		designed to provide 100X higher durability of 99.999% 
	when attached to EBS-optimized EC2 instances, 
		io2 volumes are designed to achieve single-digit millisecond latencies 
		deliver the provisioned performance 99.9% of the time. 
		This makes io2 volumes ideal for 
			performance intensive, 
			business-critical applications 
				like 
					SAP HANA, 
					Oracle, 
					Microsoft SQL Server
					IBM DB2 
		These volumes also support Multi-Attach
			allows customers to attach an io2 or io1 volume 
				to up to sixteen Nitro-based EC2 instances 
					within the same Availability Zone. 
			
--------------------------------------------------------------------------------------------------------
•	Snapshots
--------------------------------------------------------------------------------------------------------
https://www.youtube.com/watch?v=Gl9to6MmgOM
--------------------------------------------------------------------------------------------------------
•	Encrypted
--------------------------------------------------------------------------------------------------------
	encryption of snapshots
	https://www.youtube.com/watch?v=ROL2v7Crqtg
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Throughput
--------------------------------------------------------------------------------------------------------
https://aws.amazon.com/premiumsupport/knowledge-center/ebs-maximum-iops-throughput/
--------------------------------------------------------------------------------------------------------
	Advanced details
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Spot Instances
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Domain join directory
--------------------------------------------------------------------------------------------------------
	https://aws.amazon.com/blogs/security/how-to-domain-join-amazon-ec2-instances-aws-managed-microsoft-ad-directory-multiple-accounts-vpcs/
	
	You can share 
		single AWS Directory Service 
			for Microsoft Active Directory 
				(also known as an AWS Managed Microsoft AD) 
					with multiple AWS accounts within an AWS Region. 
		why?
			easy and more cost-effective for you to manage directory-aware workloads 
				from a single directory across accounts and Amazon Virtual Private Clouds (Amazon VPC). 
			Don't need to manually domain join 
				EC2 instances or create one directory per account and VPC
			you can use your directory from any AWS account and from any VPC within an AWS Region.


follow below url to practically do it. https://aws.amazon.com/blogs/security/how-to-domain-join-amazon-ec2-instances-aws-managed-microsoft-ad-directory-multiple-accounts-vpcs/
--------------------------------------------------------------------------------------------------------
•	IAM instance profile
--------------------------------------------------------------------------------------------------------

https://medium.com/devops-dudes/the-difference-between-an-aws-role-and-an-instance-profile-ae81abd700d
This is where you attach the role

From AWS Management Console
	create an IAM Role for EC2 using the 
		it creates 
			EC2 instance profile 
		and	
			IAM role.
From AWS CLI, SDKs, or CloudFormation
	need to explicitly define both:

however you can create a profile and attach role to it
	now use it in automation.


--------------------------------------------------------------------------------------------------------
•	Instance auto-recovery
--------------------------------------------------------------------------------------------------------
https://www.youtube.com/watch?v=XdU9KoQ3qAc

--------------------------------------------------------------------------------------------------------
•	Detailed CloudWatch monitoring
--------------------------------------------------------------------------------------------------------
	
	
	https://catalog.workshops.aws/general-immersionday/en-US/basic-modules/40-monitoring
	
	https://www.youtube.com/watch?v=SHjq9TfH7NA
	take automatic snapshot from ec2 instance
	
	Target: EC2 create snapshot [api call]
	
permission given
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "CloudWatchEventsBuiltInTargetExecutionAccess",
            "Effect": "Allow",
            "Action": [
                "ec2:CreateSnapshot"
            ],
            "Resource": [
                "*"
            ]
        }
    ]
}	
	
--------------------------------------------------------------------------------------------------------
•	Elastic GPU
--------------------------------------------------------------------------------------------------------
https://aws.amazon.com/ec2/
https://aws.amazon.com/ec2/instance-types/

try: https://www.youtube.com/watch?v=B75B-JBmyDw

--------------------------------------------------------------------------------------------------------
•	Credit specification
--------------------------------------------------------------------------------------------------------
https://www.youtube.com/watch?v=C3-XwFiU5sQ
	70% is the baseline
		if usage goes beyond the baseline, then your credit starts reducing. 
		let's say credit max. is 500
			it keeps reducing .
		in between if usage reduces then credit starts going up to the max (say 500)
		if credit becomes 0, then aws continues to support you with better cpu but there will be a charge
--------------------------------------------------------------------------------------------------------
•	Placement group
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html
	For e.g. 
	Multiple vm's 
		in the same Rack server
			Vs
			different Rack server.
			
		Performance Vs risk
		Similarly multiple vm's in same region vs different diff. region.

	Placement group helps you to decide between them.
		


	launch a new EC2 instance
		default EC2 service 
			attempts to place the instance 
				instances are spread out 
				across underlying hardware to 
			why? minimize correlated failures. 
		
		use placement groups 
			to influence the placement of 
			
	3 placement strategies supported:
		Cluster – 
			instances close together 
				inside an Availability Zone. 
			low-latency network performance 
				tightly-coupled node-to-node communication 
					high-performance computing (HPC) applications.
					high risk
		Partition – 
			spreads your instances across logical partitions 
				groups of instances 
					in one partition 
						do not share the underlying hardware 
						with groups of instances in different partitions. 
				good for large distributed and replicated workloads
					e.g. Hadoop, Cassandra, and Kafka.
		Spread – 
			strictly places a small group of instances across 
				distinct underlying hardware 
					to reduce correlated failures.


		Spread Vs Partition
			https://stackoverflow.com/questions/56447086/aws-ec2-placement-groups-partition-vs-spread


Refer image in https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html

Rack server
	computer dedicated to be used as a server 
	designed to be installed in a framework called a rack. 
	Each rack has its own network and power source.

In Cluster Placement group, 
	all instances are placed within a rack. If the rack fails (hardware failure), all instances fails at the same time. Hence, this is not suitable for High Availability or mission critical applications. But ideal for High Performance applications, as all the instances are in very close proximity to each other.

Parition Placement Group
	Each partition within a PG has its own set of racks. Each rack has its own network and power source
	Good for deploying large distributed and replicated workload. There are at most 7 parititions in each AZ, but the number of instances in each partition is limited only by the account limits.
	Offer visibility into the partitions
	Partitions can be in different AZs in the same region
	A newer feature (compared to the Spread PG) introduced only in December 2018 (see Annoucement of the feature)
Spread Placement Group
	Each instance is placed in its own distinct rack. Each rack has at most one instance
	Good for deploying applications that have a smaller number of instances. You can have at most 7 instances per AZ in the group
	The group can span multiple AZs in the same region.

Placement group strategies
	You can create a placement group using one of the following placement strategies:

	Cluster placement groups
	------------------------
	cluster placement group 
		logical grouping of instances 
			within a single Availability Zone. 
		cluster placement group 
			can span peered virtual private networks (VPCs) in the same Region. 
		Instances in the same cluster placement group 
			enjoy a higher per-flow throughput limit for TCP/IP traffic 
			placed in the same high-bisection bandwidth segment of the network.

Refer image in https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html
	Cluster placement groups 
		recommended for applications 
			that require 
				low network latency, 
				high network throughput, 
				or both. 
		
			or if majority of the network traffic is between the instances in the group. 
		choose an instance type 
			that supports enhanced networking. 
			
	recommend 
		launch your instances in the following way:
			Use a single launch request to place in the placement group.
			Use the same instance type for all instances in the placement group.

	Try to add more instances 
	or 
		try to launch more than one instance type 
			chances of getting an insufficient capacity error.

	stop an instance in a placement group and then start it again, it still runs in the placement group. However, the start fails if there isn't enough capacity for the instance.

	If you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group, and try the launch again. Starting the instances may migrate them to hardware that has capacity for all of the requested instances.

Partition placement groups
	Partition placement groups help reduce the likelihood of correlated hardware failures for your application. When using partition placement groups, Amazon EC2 divides each group into logical segments called partitions. Amazon EC2 ensures that each partition within a placement group has its own set of racks. Each rack has its own network and power source. No two partitions within a placement group share the same racks, allowing you to isolate the impact of hardware failure within your application.

	The following image is a simple visual representation of a partition placement group in a single Availability Zone. It shows instances that are placed into a partition placement group with three partitions—Partition 1, Partition 2, and Partition 3. Each partition comprises multiple instances. The instances in a partition do not share racks with the instances in the other partitions, allowing you to contain the impact of a single hardware failure to only the associated partition.


                    A partition placement group with three partitions.
                
	Partition placement groups can be used to deploy large distributed and replicated workloads, such as HDFS, HBase, and Cassandra, across distinct racks. When you launch instances into a partition placement group, Amazon EC2 tries to distribute the instances evenly across the number of partitions that you specify. You can also launch instances into a specific partition to have more control over where the instances are placed.

	A partition placement group can have partitions in multiple Availability Zones in the same Region. A partition placement group can have a maximum of seven partitions per Availability Zone. The number of instances that can be launched into a partition placement group is limited only by the limits of your account.

	In addition, partition placement groups offer visibility into the partitions — you can see which instances are in which partitions. You can share this information with topology-aware applications, such as HDFS, HBase, and Cassandra. These applications use this information to make intelligent data replication decisions for increasing data availability and durability.

	If you start or launch an instance in a partition placement group and there is insufficient unique hardware to fulfill the request, the request fails. Amazon EC2 makes more distinct hardware available over time, so you can try your request again later.

Spread placement groups
	A spread placement group is a group of instances that are each placed on distinct hardware.

	Spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other. Launching instances in a spread level placement group reduces the risk of simultaneous failures that might occur when instances share the same equipment. Spread level placement groups provide access to distinct hardware, and are therefore suitable for mixing instance types or launching instances over time.

	If you start or launch an instance in a spread placement group and there is insufficient unique hardware to fulfill the request, the request fails. Amazon EC2 makes more distinct hardware available over time, so you can try your request again later. Placement groups can spread instances across racks or hosts. You can use host level spread placement groups only with AWS Outposts.

Rack spread level placement groups
	The following image shows seven instances in a single Availability Zone that are placed into a spread placement group. The seven instances are placed on seven different racks, each rack has its own network and power source.


	A rack spread placement group can span multiple Availability Zones in the same Region. For rack spread level placement groups, you can have a maximum of seven running instances per Availability Zone per group.

Host level spread placement groups
	Host spread level placement groups are only available with AWS Outposts. For host spread level placement groups, there are no restrictions for running instances per Outposts. For more information, see Placement groups on AWS Outposts.


General rules and limitations
	Before you use placement groups, be aware of the following rules:

	You can create a maximum of 500 placement groups per account in each Region.

	The name that you specify for a placement group must be unique within your AWS account for the Region.

	You can't merge placement groups.

	An instance can be launched in one placement group at a time; it cannot span multiple placement groups.

	On-Demand Capacity Reservation and zonal Reserved Instances provide a capacity reservation for EC2 instances in a specific Availability Zone. The capacity reservation can be used by instances in a placement group. When using a cluster placement group with capacity reservation, it is recommended that you reserve capacity within the cluster placement group. For more information, see Capacity Reservations in cluster placement groups.

	Zonal Reserved Instances provide a capacity reservation for instances in a specific Availability Zone. The capacity reservation can be used by instances in a placement group. However, it is not possible to explicitly reserve capacity in a placement group using a zonal Reserved Instance.

	You cannot launch Dedicated Hosts in placement groups.

	Cluster placement group rules and limitations
	The following rules apply to cluster placement groups:

	The following instance types are supported:

	Current generation instances, except for burstable performance instances (for example, T2) and Mac1 instances.

	The following previous generation instances: A1, C3, cc2.8xlarge, cr1.8xlarge, G2, hs1.8xlarge, I2, and R3.

	A cluster placement group can't span multiple Availability Zones.

	The maximum network throughput speed of traffic between two instances in a cluster placement group is limited by the slower of the two instances. For applications with high-throughput requirements, choose an instance type with network connectivity that meets your requirements.

	For instances that are enabled for enhanced networking, the following rules apply:

	Instances within a cluster placement group can use up to 10 Gbps for single-flow traffic. Instances that are not within a cluster placement group can use up to 5 Gbps for single-flow traffic.

	Traffic to and from Amazon S3 buckets within the same Region over the public IP address space or through a VPC endpoint can use all available instance aggregate bandwidth.

	You can launch multiple instance types into a cluster placement group. However, this reduces the likelihood that the required capacity will be available for your launch to succeed. We recommend using the same instance type for all instances in a cluster placement group.

	Network traffic to the internet and over an AWS Direct Connect connection to on-premises resources is limited to 5 Gbps.

Partition placement group rules and limitations
	The following rules apply to partition placement groups:

	A partition placement group supports a maximum of seven partitions per Availability Zone. The number of instances that you can launch in a partition placement group is limited only by your account limits.

	When instances are launched into a partition placement group, Amazon EC2 tries to evenly distribute the instances across all partitions. Amazon EC2 doesn’t guarantee an even distribution of instances across all partitions.

	A partition placement group with Dedicated Instances can have a maximum of two partitions.

	You can't use Capacity Reservations to reserve capacity in a partition placement group.

	Capacity Reservations do not reserve capacity in a partition placement group.

Spread placement group rules and limitations
	The following rules apply to spread placement groups:

	A rack spread placement group supports a maximum of seven running instances per Availability Zone. For example, in a Region with three Availability Zones, you can run a total of 21 instances in the group, with seven instances in each Availability Zone. If you try to start an eighth instance in the same Availability Zone and in the same spread placement group, the instance will not launch. If you need more than seven instances in an Availability Zone, we recommend that you use multiple spread placement groups. Using multiple spread placement groups does not provide guarantees about the spread of instances between groups, but it does help ensure the spread for each group, thus limiting the impact from certain classes of failures.

	Spread placement groups are not supported for Dedicated Instances.

	Host level spread placement groups are only supported for placement groups on AWS Outposts. There are no restrictions for the number of running instances with host level spread placement groups.

	You can't use Capacity Reservations to reserve capacity in a spread placement group.

	Capacity Reservations do not reserve capacity in a spread placement group.

--------------------------------------------------------------------------------------------------------
•	Capacity reservation
--------------------------------------------------------------------------------------------------------

On-Demand Capacity Reservations 
	enable you to reserve compute capacity 
		for your Amazon EC2 instances 
		in a specific Availability Zone for any duration. 
	
	cheaper than on-demand if  you will continue to run.

--------------------------------------------------------------------------------------------------------
•	EBS-optimized instance
--------------------------------------------------------------------------------------------------------
https://n2ws.com/blog/how-to-guides/enabling-ebs-optimized-instances-on-running-ec2-instances
--------------------------------------------------------------------------------------------------------
•	Tenancy
--------------------------------------------------------------------------------------------------------
Tenancy 
	how EC2 instances are distributed 
		across physical hardware and affects pricing. 
	
	Shared (default) — 
		Multiple AWS accounts may share the same physical hardware.
	Dedicated Instance (dedicated) — 
		Your instance runs on single-tenant hardware.
	Dedicated Host (host) — 
		Your instance runs on a physical server with EC2 instance capacity fully dedicated to your use, an isolated server with configurations that you can control.

https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-dedicated-instances.html
--------------------------------------------------------------------------------------------------------
•	RAM disk ID
--------------------------------------------------------------------------------------------------------
A RAM disk 
	contains the necessary drivers to make the chosen kernel work 
	Didn't specify?
		value from source template will be used. 
	template value is not specified 
		default API value will be used.
		
--------------------------------------------------------------------------------------------------------
•	Kernel ID
--------------------------------------------------------------------------------------------------------

operating system kernels 
	If no value is specified 
		value of the source template will still be used. 
		If the template value is not specified 
			then the default API value will be used.
--------------------------------------------------------------------------------------------------------
•	Tags
--------------------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------------------
o	The Lifecycle of Instances
--------------------------------------------------------------------------------------------------------
Refer to image in 
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html
--------------------------------------------------------------------------------------------------------
o	Storage Options for EC2 Instances 
--------------------------------------------------------------------------------------------------------

https://www.youtube.com/watch?v=7p7p4_4RYxY

https://www.geeksforgeeks.org/what-is-aws-ec2-instance-storage/

	Ephemeral store
	Temporary store
	
Features of EC2 instance storage:
	Temporary storage: 
		EC2 instance storage 
			provides temporary storage for EC2 instances.
	Cost:  
		cost included in the cost of the EC2 instance. 
		
	Data Transfer Rate: 
		storage volumes 
			reside on the same host as the EC2 server
			I/O speed offered by these storage volumes in extremely high. 
			
	Security: 
		Security on instance store volumes 
			same as the security on the EC2 associated with them. 
			The roles, users, and policies which have access to an EC2 instance will have access to the associated Instance Storage Volumes.
	Not backed up as AMI: 
		If the user takes an AMI snapshot of an existing EC2 instance, and launches a new instance from that AMI, the instance storage data is not replicated onto the new EC2 instance machine.
	Note: This storage is not recommended for critical or valuable data.

Limitations of EC2 instance storage:
	a temporary storage service
		not suitable for storing important/critical data.
	If EC2 instance associated with this storage is stopped or terminated
		all data on the storage is lost with no possible means for data recovery.
	If the host device that provided the EC2 instance fails/crashes due to internal errors, 
		all data on EC2 instance storage is lost.
	Not all instance types in EC2 service support Instance Storage Volumes.
	Instance Store Volumes can only be specified for an instance during its launch. 
		Users cannot add new storage volumes as a later update to the same EC2.

--------------------------------------------------------------------------------------------------------
o	LAB
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	Creating EC2 Linux and EC2 Windows Instance 
--------------------------------------------------------------------------------------------------------

aws ec2 run-instances  --image-id ami-0cc87e5027adcdca8 --count 1  --instance-type t2.micro  --key-name vilas 

--------------------------------------------------------------------------------------------------------
	Connecting to EC2 instance using a PEM file through ssh
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	From putty
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	From console
-
-------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	Connecting to windows instance using RDP
--------------------------------------------------------------------------------------------------------
done
--------------------------------------------------------------------------------------------------------
	Understanding Public IP and Private IP 
--------------------------------------------------------------------------------------------------------
done
--------------------------------------------------------------------------------------------------------
•	Stop and start ec2 instance
--------------------------------------------------------------------------------------------------------
done
--------------------------------------------------------------------------------------------------------
	Create an Elastic IP and attach/detach it
--------------------------------------------------------------------------------------------------------
	https://www.youtube.com/watch?v=h-yOoHbH_Dw
--------------------------------------------------------------------------------------------------------
•	Stop and start ec2 instance
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	EC2 hibernate
--------------------------------------------------------------------------------------------------------
https://www.youtube.com/watch?v=nCEGcgmoTgQ
--------------------------------------------------------------------------------------------------------
o	Pricing of EC2 instances
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	Other cost involved
--------------------------------------------------------------------------------------------------------
	Region 
	Instance type 
	On-Deman vs spot vs dedicated
	Linux vs windows vs private os
	
	
	You also pay for 
		storage
		data transfer 
		fixed IP
		public address
		load balancing
		
		don't pay for the instance if the instance is stopped
		
		https://aws.amazon.com/ec2/pricing/on-demand/
--------------------------------------------------------------------------------------------------------
o	AWS ec2 best practices
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-best-practices.html
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------------------

Covered in "Deploying a Spring boot application on tomcat"
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Deploying a Spring boot application on tomcat

install latest maven (not yum install maven)
Refer Graalvm installation
yum install java if required


rating.tar.gz in D:/code1
tar xvfz rating.tar.gz
cd rating 
mvn clean package

install java 
readlink -f $(which java)
export JAVA_HOME

cd target
java -jar rating.jar
listens on port 8093
open port 8093

http://3.20.232.39:8093/ratings/employeerating/1

--------------------------------------------------------------------------------------------------------



Day 3

o	AWS VPC, VPC Gateway End points, CloudFront, Transit Gateway
--------------------------------------------------------------------------------------------------------

Hands on 
	https://catalog.workshops.aws/general-immersionday/en-US/basic-modules/20-vpc
	
	https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html


D:\PraiseTheLord\HSBGInfotech\AWS\ReferenceImages.doc

Exercise 1
		https://4sysops.com/archives/aws-vpc-overview-setup-subnets/
Exercise 2		
		https://4sysops.com/archives/aws-vpc-internet-gateway-route-tables-nacls/
Exercise 3
		https://4sysops.com/archives/aws-vp-setup-ec2-instance-test-connectivity/
--------------------------------------------------------------------------------------------------------
o	Networking 	
--------------------------------------------------------------------------------------------------------
o	AWS Networking overview
--------------------------------------------------------------------------------------------------------
AWS Networking
	Network 
		way of communication between devices. 
		AWS Networking 
			allows creating a 
				fast, 
				reliable
				secure network. 
		
--------------------------------------------------------------------------------------------------------
o	Networking challenges
--------------------------------------------------------------------------------------------------------
Networking 
	present a number of challenges
		in traditional on-premises environments and 
		in the cloud. 
Here are a few common networking challenges:

	Network Complexity: 
		networks grow and evolve
		can become increasingly complex and difficult to manage. 
		can be especially challenging 
			in hybrid environments 
			that span multiple 
				locations, 
				data centers
				cloud providers.

	Scalability: 
		Networks 
			need to scale 
				to accommodate growth in 
					traffic, 
					users, and 
					resources. 
		can be difficult
		traditional networking technologies like 
			firewalls, 
			load balancers
			routers 
				may become bottlenecks as traffic increases.

	Security: 
		cyber attacks and data breaches increasing 
		network security 
			top priority for organizations. 
		securing a network can be challenging
			many potential vulnerabilities 
			attack vectors that need to be addressed.

	Application Performance: 
		Network latency
		bandwidth constraints
		other performance issues 
			can impact performance 
		in cloud environments
			applications are distributed 
				across multiple regions or 
				data centers.

	Integration with Third-party Services: 
		organizations rely on third-party services 
			for critical business functions
		integrating these services 
			with their own networks can be challenging. 
		configuring VPNs
		establishing secure connections
		ensuring compatibility with different 
			protocols and APIs.

	Compliance and Regulatory Requirements: 
		This can add an extra layer of complexity to network management and increase the risk of non-compliance.

	Network Visibility and Monitoring: 
		AWS provides tools for 
			monitoring network traffic and performance
			needs right tools 
		set up monitoring tools like 
			CloudWatch, 
			configuring alerts, and 
			analyzing network traffic logs.

	Cost Management: 
		AWS networking 
			can be expensive
		optimize network configuration to minimize costs is challenging. 
		Use AWS Cost Explorer 
			to analyze network usage
			setting up billing alerts
			optimizing network usage.


--------------------------------------------------------------------------------------------------------
o	CIDR 
--------------------------------------------------------------------------------------------------------
CIDR 
	Classless Inter-Domain Routing
	method of assigning IP addresses to devices 
		on a network. 
	network administrator 
		can allocate IP addresses 
			in a 
				flexible and 
				efficient way 
				than old classful networking.

In CIDR
	IP addresses 
		combination of 
			network prefix and a 
			host identifier. 
	The network prefix 
		portion of the IP address 
			that identifies the network
		contiguous block of bits in the IP address
		specified using CIDR notation.	
	host identifier 
				indicates the specific device on that network. 
		

CIDR notation 
	shorthand way of representing an IP address 
	its associated with network prefix. 
	It consists of 
		IP address followed by a slash (/) subnet mask. 
	For example
		CIDR notation 192.168.0.0/24 
			indicates 
				network prefix: 24 bits long
				host identifier: 8 bits long. 
	256 possible host addresses on the network (2^8)
	network address range 
		192.168.0.0 to 192.168.0.255.

CIDR 
	allows network administrators 
		allocate IP addresses in a more efficient way
	using variable-length subnet masks to allocate IP addresses to different subnets within a larger network. 

--------------------------------------------------------------------------------------------------------
•	Security in networking
--------------------------------------------------------------------------------------------------------
Networks 
	target of 
		cyber attacks and 
		other security threats
Security in networking 

	Some important security considerations in networking:

	Access Control: 
		controlling access to network resources. 
			authentication and 
			authorization mechanisms 
				only authorized users 
					can access sensitive 
						data and 
						resources.

	Encryption: 
		key security technology 
			protect data 
				at rest and motion.
		ensure 
			even intercepted 
				cannot be understood or used.

	Firewalls: 
		Firewalls 
			critical component of network security
			prevent unauthorized access to network resources 
			filter incoming and outgoing traffic 
				based on predefined rules.

	Intrusion Detection and Prevention: 
		Intrusion detection and prevention 
			monitor network traffic and 
			detect potential security threats. 
		identify and block 
			unauthorized access attempts
			malware
			other types of attacks.

	Network Segmentation: 
		process of dividing a network 
			into smaller and manageable subnets. 
		limit impact of security breaches 
			contain them within 
				a smaller portion of the network.

	Monitoring and Logging: 
		Monitoring network traffic and logging security events 
			help organizations to 
				detect and respond to 
					security incidents quickly. 
		
	Vulnerability Management: 
		identifying and prioritizing 
			potential security vulnerabilities in network 
				infrastructure, 
				applications, and 
				devices
		implement measures to address them. 
			patching software and firmware
			scanning for vulnerabilities
			implementing security best practices.

	Physical Security: 
		Taken care by amazon

	Incident Response: 
		process of responding to security incidents
			such as 
				cyber attacks or 
				data breaches. 
		detect and contain the incident
		investigat the cause 
			extent of the incident
			taking steps to remediate the damage 
			prevent future incidents.

	Compliance: 
		Network security must be 
			designed to comply with these regulations
			include 
				data protection requirements, 
				access control measures
				incident response protocols.

	Human Factors: 
		Network security 
			impacted by human factors
			user behavior and awareness. 
		Organizations need 
			implement security awareness training programs 
			educate users about security best practices
				password management and 
				phishing prevention.


Malware Protection
Data Protection
Identity and Access Management
Network Device Security
Remote Access Security
Wireless Network Security
Cloud Network Security
Disaster Recovery and Business Continuity
Network Security Auditing
Network Penetration Testing
Social Engineering Prevention
Third-Party Risk Management
Application Security
Patch Management
Data Backup and Recovery
Disaster recovery
Network Configuration Management

--------------------------------------------------------------------------------------------------------
•	VPC Overview
--------------------------------------------------------------------------------------------------------
		
		

Amazon VPC (Virtual Private Cloud) 
	service that allows customers to 
		create a logically isolated virtual network 
			within the Amazon Web Services (AWS) cloud. 
	enables customers to launch AWS resources
		e.g. 
			EC2 instances
			RDS instances
			ELB load balancers
				within a virtual network 
					that they define. 
	provides greater control over 
		network security and 
		access to resources in the cloud.

key features of Amazon VPC include:
	Network Isolation: 
		Amazon VPC 
			provides complete network isolation 
				from other customers 
			Customers can 
				create a 
					private virtual network 
				or
					public virtual network
	Subnets: 
		can create subnets 
			within VPC
		partition network resources 
		control access. 
		Subnets can span multiple Availability Zones for high availability.

	Security Groups: 
		Amazon VPC 
			provides security groups
			virtual firewalls for customer resources 
			control inbound and outbound traffic.

	VPN Connections: 
		Amazon VPC 
			supports secure VPN connections 
				between the virtual network and 
				customer data centers or other networks.

	Network Address Translation (NAT): 
		Amazon VPC 
			supports 
				NAT instances and 
				NAT gateways
			allow resources within 
				to access the internet 
					while keeping inbound traffic 
						from the internet out of the network.

	Elastic IP Addresses: 
		Amazon VPC 
			allows customers to allocate and associate 
				elastic IP addresses 
		static
		publicly-routable 
			IP addresses to resources.

	Amazon VPC enables customers to create a secure and scalable virtual network in the AWS cloud. With VPC, customers can configure their own IP address ranges, subnets, and routing tables, and they can control inbound and outbound traffic to and from resources within the virtual network.

--------------------------------------------------------------------------------------------------------
•	AWS Regions and Replication of data between the Regions
--------------------------------------------------------------------------------------------------------

Replication of data 
	between AWS regions is important for several reasons like
		disaster recovery
		data locality
		compliance requirements. 
	
	Most imp. e.g. of ways to replicate data between AWS regions:

		AWS S3 Cross-Region Replication: 
			S3: highly durable and scalable 
					object storage service. 
			AWS S3 Cross-Region Replication 
				automatically replicate S3 objects 
					between different AWS regions. 
				use case:
					disaster recovery 
					reduce latency.

		AWS Database Migration Service (DMS): 
			AWS RDS Multi-AZ Replication: 
				Amazon RDS (Relational Database Service) 
					managed database service 
					supports relational database engines like 
						MySQL, 
						PostgreSQL, 
						Oracle, and 
						SQL Server. 
				Multi-AZ Replication provides 
					high availability and 
					failover capabilities 
						in a single AWS region. 
				To replicate data between AWS regions
					use AWS Database Migration Service (DMS) 
						to migrate data from one region to another.

		AWS CloudFront Geo-Targeting: 
			Amazon CloudFront 
				content delivery network 
				caches and delivers content from edge locations. 
			AWS CloudFront Geo-Targeting 
				serve content from the edge 
				configure CloudFront to serve 
					content from different regions.

		AWS Route 53 DNS Failover: 
			Amazon Route 53 
				DNS (Domain Name System) 
				routes user requests 
					to AWS resources like 
						EC2 instances, 
						load balancers
						S3 buckets. 
			AWS Route 53 DNS Failover 
				provides automated failover between AWS regions. 
				If one region becomes unavailable
					automatically route traffic to a healthy region.
			
		Amazon Elastic File System (EFS): 
			A fully managed file storage service 
				supports file replication between AWS regions.
				
			steps to replicate data between AZs using EFS:
				Can be done usign console, cli or sdk
			
				Create an Amazon EFS file system: 
				Enable EFS file system automatic backups: 
				Create a mount target in each AZ: 
				Mount the EFS file system on EC2 instances: 
					use NFS protocol and standard mount commands.
				Configure cross-AZ replication: 
					configure cross-AZ replication 
						for the EFS file system 
							to replicate data between 
								primary AZ 
							and 
								secondary AZs. 
						Use the AWS Management Console or CLI.

				Once cross-AZ replication is configured, any changes made to the data in the primary AZ are automatically replicated to the secondary AZs, ensuring high availability and fault tolerance for the EFS file system.

			In summary, Amazon EFS provides a simple and scalable way to replicate data between AZs, which can help improve the availability and durability of your applications and data. By following the steps outlined above, you can easily configure cross-AZ replication for your EFS file system.	
				
		
		Amazon EBS Snapshots: 
			Amazon Elastic Block Store (EBS) 
				block-level storage service 
				persistent storage for EC2 instances. 
				EBS snapshots 
					can replicate data between AWS regions.
			
			Steps to replicate data between AZs using EBS:

				Create an EBS volume: 
					through Console, CLI, or SDK.
				Attach the EBS volume to an EC2 instance: 
					in the primary AZ to store data on the volume.
				Create an EBS snapshot: 
					through Console, CLI, or SDK.
				Copy the snapshot to the destination AZ: 
					through Console, CLI, or SDK.
				Create an EBS volume from the snapshot: 
					in the destination AZ
					through Console, CLI, or SDK.
				Attach the EBS volume to an EC2 instance: 
					in the destination AZ to access the replicated data.


		AWS Transit Gateway: 
			network transit hub 
				interconnect VPCs and VPNs. 
			AWS Transit Gateway 
				interconnect VPCs across different AWS regions.

		AWS Global Accelerator: 
			improves availability and performance of applications by using AWS's global network. 
			supports routing traffic to different AWS regions.

		AWS Lambda Cross-Region Replication: 
			A feature of AWS Lambda that allows you to replicate functions across different AWS regions.

		Amazon MQ Replication: 
			A managed message broker service that supports replicating messages between different AWS 	regions.

		AWS Storage Gateway: 
			hybrid cloud storage service 
				securely integrate 
					on-premises IT environments 
						with cloud storage. 

		AWS Direct Connect: 
			dedicated network connection 
				between your on-premises data center and AWS. 
				AWS Direct Connect can be used to replicate data between different AWS regions.

--------------------------------------------------------------------------------------------------------
•	Availability Zones and High Availability
--------------------------------------------------------------------------------------------------------

Availability Zones 
	physical data center locations 
	geographically separated within an AWS region. 
	independent	
		own power
		cooling
		networking
		connectivity 
			to the internet. 
	use multiple AZs for
		high availability 
		fault tolerance 
	High availability 
		ability to remain operational and accessible 
			even during failures or disruptions. 
	
	Most imp. AWS features for HA:
		1. Multi-AZ Deployments: 
			Amazon RDS, 
			Amazon Elasticache
			Amazon Aurora 
				can be deployed in Multi-AZ configurations. 
			In this configuration
				automatically replicate 
					to a standby replica 
						in a different AZ. 
			primary instance fails
				standby instance is automatically promoted to take over the workload
		2. Load Balancing: 
			AWS Elastic Load Balancer (ELB) 
				distribute traffic 
					across multiple instances or containers. 
				can automatically distribute traffic 
					across multiple AZs 
				achieve high availability and 
				fault tolerance.
		3. Auto Scaling: 
			scale the capacity of your applications 
				based on demand. 
			automatically add or remove instances 
				across multiple AZs 
			high availability and 
			meet performance requirements.
		4. AWS Route 53: 
			AWS Route 53 
				highly available and scalable 
					DNS service 
			route traffic between different AWS services. 
			
--------------------------------------------------------------------------------------------------------
•	Subnets
--------------------------------------------------------------------------------------------------------


Subnet: 
	range of IP addresses 
		meant to be used by VPC. 
	AWS resources 
		can be launched in specific subnets. 
	Public subnets 
		connected to the Internet
	private subnets 
		not connected to the internet. 
	To protect AWS resources 
		provide layered security. 
	Use 
		security groups 
		network access control lists (NACL).


Features of Amazon VPC 
	IPV4 static addresses 
		can be assigned to user’s 
		in a range.  
	An IPV6 CIDR block 
		can be optionally associated with the user’s VPC. 
	An IPV6 address 
		can be associated with the user’s instance.  
	Multiple IP addresses can be assigned to the user’s instance.  
	One/more 
		Network interfaces can be defined
			can be attached to the user’s instance. 
	Security group membership of the user’s instance 
		can be changed when it is in the running state. 
	inbound traffic/outgoing traffic  
		can be controlled 
			ingress filtering/egress filtering
	The user’s instances can be made to run on single-tenant hardware.  
	An additional layer of access control 
		can be added to the instances 
		using NACLs (network access control lists).  
	Accessing Amazon VPC 
	Amazon VPC can be 
		created, 
		accessed, and 
		managed from: 
		
	can support of Windows, MacOS and Linux.  

	AWS Management Console: 
	AWS Command Line Interface (AWS CLI): 
	AWS SDK: 
	Query API: 
		HTTPS requests. 
		
VPC Pricing
	No additional charges 
	A site-on-site VPN connection and a NAT gateway are chargeable.

Default VPC: 
	If the user account supports EC2-VPC platform, 
		it comes with a default VPC
			provisioned by EC2-VPC 
				ready-to-use. 
		selected by default: default VPC itself. 
Non default VPC: 
	user can create their own VPC 
	configure them according to their own requirements. 
	
c
--------------------------------------------------------------------------------------------------------
•	Route tables
--------------------------------------------------------------------------------------------------------
Route table: 
	
	control traffic flow 
		between different subnets 
			within a Virtual Private Cloud (VPC). 
	An AWS route table 
		contains a list of rules/routes
		determine how traffic is directed 
			between subnets 
				within the same VPC 
			or to destinations outside the VPC. 
	Each route in the table 
		specifies a 
			destination CIDR block(IP address range) 
		and 
			a targcet
				can be 
					internet gateway
					NAT gateway
					virtual private gateway

--------------------------------------------------------------------------------------------------------
•	Internet gateways
--------------------------------------------------------------------------------------------------------
Internet gateway: 
	horizontally scaled
	redundant
	highly-available component of the VPC 
	allows communication between 
		instances of the VPC and the internet.  
	acts as a gateway for internet traffic 
		to and from the VPC
		provide path for 
			incoming 
		and 
			outgoing 
				traffic.

	The Internet Gateway 
		enables access to public internet resources like  
			websites, 
			APIs
			other services. 
	
	Steps
	-----
		Create Internet Gateway 
		attached to a VPC
		Create a default route in the VPC route table 
			direct all non-local traffic 
				to the Internet Gateway. 
		Resources in VPC 
			can communicate 
				with internet 
					using public IP addresses.

--------------------------------------------------------------------------------------------------------
•	API Gateway
--------------------------------------------------------------------------------------------------------
Amazon API Gateway 
	fully managed service 
	like normal api gateway
	developers can 
		create, 
		publish, 
		monitor
		manage APIs 
			(Application Programming Interfaces) at any scale.
	allows developers to expose backend services as 
		RESTful APIs or 
		WebSocket APIs 

	central point of control 
		for API traffic
		provide features like 
			authentication, 
			authorization, 
			throttling, 
			caching, 
			and monitoring.
	supports a wide range of backend integrations like 
		AWS Lambda
		HTTP/HTTPS endpoints
		AWS services
		third-party APIs.

	The API Gateway 
		provides multiple deployment options like 
			regional APIs
				accessible only within the same AWS region 
				default type in API Gateway
				provide 
					high performance 
					low latency
					not be optimal for global users or applications
			edge-optimized APIs
				provide low latency and high throughput 
					to users located in different geographic locations. 
				use Amazon CloudFront content delivery network (CDN) 
					cache and distribute API responses 
						to edge locations 
							closer to the user. 
				recommended for APIs 
					serve a global user base and require low latency.
			private APIs.
				accessible only within the customer's Amazon VPC 
				not exposed to the public internet. 
				designed API that should be 
					private and secure
					like 
						internal APIs or 
						APIs exposed to trusted partners. 
				Private APIs 
					accessed through a VPC endpoint
					provides a secure and direct connection between 
						VPC and the API Gateway.
--------------------------------------------------------------------------------------------------------
•	Transit gateways 
--------------------------------------------------------------------------------------------------------
AWS direct connect supports 3 gateways
	AWS Transit Gateway 
	Virtual Private Gateways
	
AWS Transit Gateway 
	fully managed network transit hub 
	simplifies management of multiple VPCs 
		(Virtual Private Clouds) and 
		remote(on-premises) networks
	makes it easier for customers to 
		build 
			scalable and 
			secure cloud networks.

	simplify network connectivity 
	reduce the need for 
		complex routing and peering configurations.

	Connecting multiple VPCs and remote networks 
		through a single Transit Gateway
			reduce the number of connections 
			simplify network management
				continue features like 
					route tables
					security policies
					monitoring capabilities.

	Transit Gateway 
		can connect VPCs across different accounts
		integration with other AWS services like 
			Direct Connect and 
			VPN
				
--------------------------------------------------------------------------------------------------------
•	Virtual Private Gateways
--------------------------------------------------------------------------------------------------------
Amazon Virtual Private Gateway 
	VPN (Virtual Private Network) connection component 
		that 
	secure communication between 
		Amazon VPC (Virtual Private Cloud) 
		on-premises networks 
			over an encrypted VPN connection.

	IPsec VPN connection 
		from their on-premises network to their VPC. 
	secure and private connection 
		between the customer's data center and their AWS VPC.

	The Virtual Private Gateway 
		supports two types of VPN connections 
			static routing  
				manually configures the routing table 
			dynamic routing. 
				customer uses BGP (Border Gateway Protocol) 
					dynamically exchange routing information between 
						on-premises network 
						VPC.

	Virtual Private Gateway supports 
		high availability 
		redundancy
		create 
			active-active and 
			active-passive VPN connections 
				across multiple Availability Zones.
--------------------------------------------------------------------------------------------------------
•	Virtual interfaces
--------------------------------------------------------------------------------------------------------
AWS Virtual Interface 
	direct private network connection (bypassing the public internet) between 
		AWS Direct Connect location 
	and 
		customer's data center
		co-location facility
		other network service provider. 
	
	high-bandwidth connection to AWS services
		bypassing the public internet 
		secure and 
		reliable connection.

	established by creating a logical connection 
		between a customer's router 
	and 
		AWS Direct Connect router 
	
	supports different 
		port speeds
			ranging from 50Mbps to 10Gbps

	support different types of connections
		private virtual interfaces 
			private, dedicated connection to a customer's VPC or other AWS services
		public virtual interfaces. 
			provides access to AWS public services like 
				Amazon S3
				Amazon DynamoDB
				Amazon EC2.


3 types 
	Public virtual interface 
	Private virtual interface
	Transit virtual interface
	

--------------------------------------------------------------------------------------------------------
o	Transit virtual interface
--------------------------------------------------------------------------------------------------------
Transit Virtual Interface 
		direct, private network connection 
			between 
				AWS Direct Connect location 
			and 
				Amazon Web Services Transit Gateway. 
	
	dedicated and high-bandwidth connection 
		for customers to connect 
			on-premises networks 
		to 
			multiple Amazon VPCs (Virtual Private Clouds) 
				over the AWS global network.

	extend 
		data center networks 
			to the cloud 
		create a unified network architecture 
			with multiple VPCs and 
			on-premises networks. 
	secure and reliable 

	centralized hub 
		for network traffic and routing
		simplify network 
			on-prem to cloud. 
		Use 
			route tables
			security policies
			monitoring capabilities
				managed through the AWS Transit Gateway.

--------------------------------------------------------------------------------------------------------
o	Public virtual interface (vilas)
--------------------------------------------------------------------------------------------------------
AWS Public Virtual Interface 
	direct, private network connection between 
		AWS Direct Connect location and a 
		public AWS service like 
			Amazon S3
			Amazon EC2
			Amazon DynamoDB. 
	dedicated and high-bandwidth connection 
		for customers to access public AWS services over the AWS global network.

Public Virtual Interface
	customers need to establish a logical connection 
		between their router and 
		AWS Direct Connect router 
			at a Direct Connect location. 
		Once the connection is established
			use the Public Virtual Interface 
				to access public AWS services 
					using AWS Direct Connect gateway
						acts as a proxy for accessing public services.
Recommended for customers 
	need to access public AWS services 
		frequently and require low latency and high throughput. 
	can be configured with different port speeds, ranging from 50Mbps to 10Gbps, depending on the customer's requirements.

Overall, AWS Public Virtual Interfaces provide a secure and dedicated connection for customers to access public AWS services, enabling them to take advantage of the scalability, flexibility, and cost-effectiveness of the cloud while maintaining control over their data and network traffic.
--------------------------------------------------------------------------------------------------------
o	Private virtual interfaces (vilas)
--------------------------------------------------------------------------------------------------------
An AWS Private Virtual Interface 
	direct, private network connection between 
		AWS Direct Connect location 
		and 
		customer's VPC (Virtual Private Cloud). 
	
	high-bandwidth connection 
		for 
			VPC resources over the AWS global network.


To create a Private Virtual Interface, customers need to establish a logical connection between their router and an AWS Direct Connect router at a Direct Connect location. 

connection is established?
	use the Private Virtual Interface 
		to access their VPC resources over 
			AWS Direct Connect connection.

recommend 
	access their VPC resources frequently 
		low latency and high throughput. 
	configured with different port speeds
		50Mbps to 10Gbps, depending on the customer's requirements.

AWS Private Virtual Interfaces provide a secure and dedicated connection for customers to access their VPC resources, enabling them to take advantage of the scalability, flexibility, and cost-effectiveness of the cloud while maintaining control over their data and network traffic.

continue from here

--------------------------------------------------------------------------------------------------------
•	VPC Peering 
--------------------------------------------------------------------------------------------------------
	Amazon Virtual Private Cloud (Amazon VPC) peering 
		networking feature 
		connect two VPCs together 
			privately over the Amazon network. 
				using private IP addresses
			no public internet access or VPN connections.
				all traffic stays within the Amazon network
		secure 
		reliable 

	permissions
		modify the VPC's route table
	VPCs 
		should not have overlapping IP address ranges. 
		establish peering connection 
			configure the VPCs to route traffic between them 
			control access to resources across the two VPCs.

--------------------------------------------------------------------------------------------------------
•	VPC Endpoints 
--------------------------------------------------------------------------------------------------------
Amazon Virtual Private Cloud (Amazon VPC) endpoints 
	networking feature 
	privately connect your VPC to supported AWS services 
		without 
			public internet gateway 
		or 
			NAT device. 
	secure and reliable way to access AWS 
		in your VPC
		don't expose 
			resources to the public internet.

When you create a VPC endpoint
	create 
		interface endpoint 
			access AWS services through endpoint service
				Amazon S3 or 
				Amazon DynamoDB.
	or 
		gateway endpoint
			access AWS services through gateway endpoints
				Amazon Elastic File System (EFS) 
				Amazon Simple Notification Service (SNS).

To create a VPC endpoint, you need to create an endpoint service or a gateway endpoint and then configure your VPC to use the endpoint. Once the endpoint is created, you can use it to access the supported AWS service using private IP addresses within your VPC.

VPC endpoints can be used for a variety of use cases, such as:
	Securing access to AWS services from within your VPC
	Improving performance by reducing latency and increasing throughput
	Enabling communication between VPCs and AWS services without requiring a public internet gateway or NAT device
	Reducing data transfer costs by keeping traffic within the Amazon network
	Overall, VPC endpoints are a powerful networking feature that enables you to securely and reliably access AWS services from within your VPC without requiring a public internet gateway or NAT device, providing a more secure and efficient way to access and use AWS services.








	--------------------------------------------------
	https://www.knowledgehut.com/tutorials/aws/vpc-endpoint-and-flowlogs

	VPC endpoint: It enables the user to connect the VPC privately to the AWS resources (which support it) and to VPC endpoint services, which is powered by PrivateLink that enables this connection to occur without requiring an internet gateway or a NAT device or a VPN connection or an AWS Direct Connection. The VPC instances don’t need a public IP address to communicate with the resources present in the service. The traffic present between the VPC and the other services wouldn’t leave the Amazon network.  


	What are VPS Endpoint and Flowlogs in AWS?
	VPC Endpoint
	A VPC endpoint is used to allow users to privately connect the VPC to the AWS resources. It also helps connect VPC endpoint services that are powered by PrivateLink to AWS services without the need of an internet gateway, NAT gateway, VPN connection or an AWS Direct Connect connection.  

	The VPC instance don’t need a public IP address so as to communicate with the resources present in the service. The traffic between VPC and the other services stays within the Amazon network only. Endpoints can be understood as virtual devices that are horizontally scaled, redundant, and highly-available VPC components. These VPC components help in the communication between instances in the VPC and the services, without causing any availability risks or bandwidth constraints with regards to the network traffic.  

	There are two types of VPC endpoints: 

	Interface endpoints 
	Gateway endpoints  
	Note: The VPC endpoints can be created based on the requirement and the service which supports this.   

	Interface endpoints
	An interface VPC endpoint is used to connect to services which use AWS PrivateLink. These services include Amazon services, services hosted by other AWS customers, partners of the user’s own VPCs, and AWS Marketplace partner services. The owner of the service refers to the service provide, and the user is the person who creates these interface endpoints, who are known as ‘service consumers’.  

	Gateway endpoints 
	A gateway endpoint is a gateway that is specified by the user as a target to the route in the route table so that it follows the traffic it is assigned to the Amazon service that it supports. It supports Amazon S3 and DynamoDB services.  

	Controlling usage of VPC Endpoints
	IAM users don’t have the permission to work with endpoints by default. The user has to create an IAM policy that is used to grant users the permission to create, change, describe and delete endpoints. The user can’t create an IAM policy to grant permission to a specific endpoint or prefix list.  

	When an endpoint is created, the user can attach the endpoint policy to the endpoint which has control access to the service it connects to. Endpoint policies are written in JSON format.  

	If the user is using an endpoint to connect to Amazon S3, S3s bucket policies also need to be used to control access to these buckets from specific endpoints or VPCs.  

	Usage of VPC endpoint policy 
	VPC endpoint is an IMA policy which is attached to the endpoint when an endpoint is created or modified. If such a policy is not attached when an endpoint is created, Amazon, by default, attaches a policy that allows complete access to the service. No endpoint policy overrides or replaces the IAM user policies or other policies which are specific to the service. 

	Key Aspects 
	More than one policy can’t be attached to an endpoint, but a policy can be modified at any point in time. Once a policy is modified, it takes a few minutes for the changes to show up.  
	The part of the policy which is related to the specific service will work. An endpoint policy can’t be used to allow resources present in the VPC to perform other actions.  
	The endpoint policy must contain a principal element.  
	The size of the endpoint policy shouldn’t exceed 20480 characters, including whitespaces.  
	VPC Flowlogs
	VPC flowlogs is a feature provided by Amazon that helps the user capture information regarding IP traffic that goes to and comes from network interfaces in the VPC. Flow log data which is captured can be published to Amazon CloudWatch Log and Amazon S3. Once the user creates a flow log, they can retrieve and view the data in the destination of their choice.  

	Flow logs can be used with multiple tasks, and some of them have been listed below: 

	Traffic monitoring which reaches the user instance. 
	The over restrictive security group rules can be diagnosed and worked on.  
	Determining the direction from where traffic come to and goes from the network interfaces.  
	Flow log data that is outside the network traffic is collected, and hence, it doesn’t affect the network’s throughput or latency. Flow logs can be created or deleted without worrying about its effect on the impact on network performance.  

	When flow log data is placed inside CloudWatch logs, it is chargeable.  

	A flow log can be created for a VPC, a subnet or a network interface. When a flow log is created for a subnet or a VPC, every network interface which is present within that subnet or VPC is considered for the process of monitoring.  

	The monitored network’s flow log data is recorded as ‘flow log record’, which logs events that consist of fields which describe the flow of traffic.  

	When a flow log has to be created, the below mentioned attributes are specified: 

	The resource for which the flow log needs to be created.  
	The type of traffic which it will be used to capture (Accepted traffic, rejected traffic or all kinds of traffic). 
	The destination wherein this flow log data is to be published.  
	Once a flow log has been created, it takes a few minutes for data to be collected and then publish to the chosen destinations. Flow logs can’t capture real-time log streams for the user’s network interfaces.  

	If the user launches more than one instance in the subnet once the flog log has been created for subnet/VPC, a new log stream (for CloudWatch Logs) or a log file object (for Amazon S3) gets created for every new network interface. This operation happens as soon as the network traffic gets recorded for that specific network interface.  

	Flow log for network interfaces can be created by other Amazon services which have been listed below: 

	Elastic Load Balancing 
	Amazon RDS 
	Amazon ElastiCache 
	Amazon Redshift 
	NAT gateways 
	Transit gateways 
	Amazon WorkSpaces 
	Irrespective of the network interface type, the Amazon EC2 console or Amazon EC2 API has to be used to create a flow log for the network interface.  

	When a flow log is created, the default format can be used for its flow log record or a customized format can be used (Amazon S3 only).  

	When the user no longer requires the flow log, it can be deleted. When a flow log is deleted, it disables the flow log service for that specific resource, and no new flow log gets created or published to CloudWatch or Amazon S3.  

	When a flow log is deleted, it doesn’t delete any existing flow log record or log stream or log file objects for a network interface.  

	If an existing log stream needs to be deleted, the CloudWatch Log console needs to be used. If an existing log file object needs to be deleted, the Amazon S3 console needs to be used.  

	Flow Log Records
	A flow log record is used to represent a network flow for the VPC. Every record is used to capture a network internet protocol traffic by default. This traffic is present within a ‘capture window’. Capture window is a time period of about 10 minutes during which the data flow is captured. ‘Aggregation period’ refers to the total amount of time it takes to capture, process and publish the flow data. The aggregation period can take up to 15 minutes.  

	The record includes values of different components present in the IP flow by default, and this includes the source, destination and the protocol.  
--------------------------------------------------------------------------------------------------------
•	Dynamic Host Configuration Protocol (DHCP) Option Sets (DHCP) option sets
--------------------------------------------------------------------------------------------------------
	Dynamic Host Configuration Protocol (DHCP) option sets are a feature in Amazon Web Services (AWS) that enable you to configure DHCP options for your Amazon Virtual Private Cloud (VPC). DHCP option sets allow you to specify additional options that are not included in the default DHCP options, such as domain name servers, time servers, and NTP servers.

When you create a VPC, AWS automatically creates a default DHCP option set that includes basic DHCP options, such as the default domain name server and the default domain name. However, if you need to configure additional DHCP options, you can create a custom DHCP option set and associate it with your VPC. You can then specify the additional DHCP options that you want to use, such as the IP address of your own domain name server or NTP server.

DHCP option sets enable you to centralize the configuration of DHCP options for your VPC, making it easier to manage and maintain your network configuration. You can create multiple DHCP option sets and associate them with different VPCs or subnets, depending on your requirements.

DHCP option sets can be used for a variety of use cases, such as:

Configuring custom DHCP options for your VPC or subnet, such as domain name servers or NTP servers
Using your own domain name servers or NTP servers instead of the default AWS servers
Centralizing the management of DHCP options for your VPCs and subnets
Enabling dynamic IP address assignment for your instances in your VPC
Overall, DHCP option sets are a useful feature in AWS that enable you to customize the DHCP options for your VPC, providing greater control and flexibility over your network configuration.

--------------------------------------------------------------------------------------------------------
•	DNS firewall
--------------------------------------------------------------------------------------------------------
DNS firewall is a security service that helps protect your Domain Name System (DNS) infrastructure from various types of threats, including malware, phishing attacks, and data exfiltration. It works by filtering and blocking DNS traffic based on predefined rules and policies.

DNS is a critical part of the internet infrastructure that translates domain names into IP addresses. However, it is also a popular target for cyber attacks, as it can be used to redirect users to malicious websites or to exfiltrate sensitive information from your network.

DNS firewall helps protect your DNS infrastructure by blocking malicious DNS requests and filtering out unwanted traffic. It can be used to enforce policies that restrict access to certain websites or domains, prevent phishing attacks, and detect and block malware that uses DNS as a communication channel.

DNS firewall can be deployed as a standalone service or integrated into your existing security infrastructure, such as firewalls, intrusion detection systems, and security information and event management (SIEM) tools. It can also be used in conjunction with other DNS security services, such as DNSSEC and DNS filtering, to provide a comprehensive DNS security solution.

Overall, DNS firewall is a powerful security service that helps protect your DNS infrastructure from various types of threats, providing an additional layer of defense against cyber attacks and ensuring the reliability and availability of your DNS services.
--------------------------------------------------------------------------------------------------------
•	Network firewall
--------------------------------------------------------------------------------------------------------
AWS Network Firewall is a managed service that provides network security and traffic inspection for your Amazon Virtual Private Cloud (VPC). It enables you to monitor and control inbound and outbound traffic to and from your VPC, allowing you to create granular security policies based on IP addresses, ports, and protocols.

Network Firewall uses rules and policies to filter and inspect network traffic, allowing you to block traffic that doesn't meet your security requirements. It can inspect traffic at the packet and session levels, providing visibility and control over network traffic that traditional firewall solutions cannot match.

Network Firewall also supports deep packet inspection (DPI) and intrusion prevention system (IPS) features, which allow you to detect and block threats in real-time. DPI enables Network Firewall to identify and block traffic based on application-level protocol information, while IPS allows it to detect and block known threats such as viruses, malware, and phishing attempts.

Network Firewall is highly scalable and can handle millions of connections per second, making it suitable for use in high-traffic environments. It integrates with other AWS services, such as Amazon CloudWatch, AWS Config, and AWS Firewall Manager, providing centralized management and visibility of your network security policies and configurations.

Overall, AWS Network Firewall is a powerful and flexible network security solution that enables you to secure your VPC and protect your workloads from various types of threats, including malware, viruses, and phishing attempts. It provides granular control over network traffic and can be easily integrated with other AWS services, making it a valuable addition to your cloud security toolkit.





--------------------------------------------------------------------------------------------------------
•	Secrets Manager
--------------------------------------------------------------------------------------------------------

AWS Secrets Manager is a managed service that enables you to store and retrieve secrets, such as database credentials, API keys, and other sensitive information, securely and easily. Secrets Manager allows you to centralize your secrets management, reducing the risk of accidental exposure and improving the security of your applications and services.

With Secrets Manager, you can create, store, and manage secrets as well as control access to them using fine-grained permissions. You can rotate secrets automatically or manually, ensuring that your secrets are up-to-date and reducing the risk of unauthorized access. Secrets Manager also integrates with AWS services and third-party applications, allowing you to securely provide secrets to your applications and services without exposing them directly.

Some key features of AWS Secrets Manager include:

Secure storage and retrieval of secrets, using encryption both in transit and at rest
Automated or manual rotation of secrets, ensuring that they are regularly updated and minimizing the risk of compromise
Fine-grained access control and permission management, allowing you to control who has access to your secrets
Integration with AWS services and third-party applications, enabling you to securely provide secrets to your applications and services without exposing them directly
Auditing and monitoring of secret usage, providing visibility into who is accessing your secrets and when
Overall, AWS Secrets Manager is a powerful and flexible solution for managing secrets and sensitive information in the cloud. By centralizing your secrets management and providing automated rotation, access control, and integration with other AWS services, Secrets Manager can help you improve the security of your applications and services while reducing the risk of accidental exposure.

--------------------------------------------------------------------------------------------------------
•	Direct connect
--------------------------------------------------------------------------------------------------------

AWS Direct Connect is a dedicated network connection service that provides a secure and reliable way to transfer data between your on-premises infrastructure and AWS cloud services. It enables you to establish a private, high-bandwidth, low-latency connection between your data center, office, or colocation environment and AWS, bypassing the public internet.

With Direct Connect, you can establish multiple virtual interfaces over a single physical connection, allowing you to use the same connection to access different AWS services or VPCs. You can also use Direct Connect to establish a dedicated connection to a specific AWS region or use AWS Transit Gateway to connect multiple VPCs across different regions.

Direct Connect offers several benefits, including:

Enhanced security and privacy: Direct Connect provides a private connection to your AWS resources, ensuring that your traffic does not traverse the public internet and reducing the risk of interception or eavesdropping.
Improved performance and reliability: Direct Connect offers lower latency and higher bandwidth than public internet connections, providing a consistent and reliable connection to your AWS resources.
Cost-effective: Direct Connect can help reduce data transfer costs by providing a predictable pricing model and eliminating the need for costly VPN or MPLS connections.
Flexible and scalable: Direct Connect supports a range of connection speeds and can be easily scaled up or down to meet your changing needs.
Direct Connect can be used for a variety of use cases, such as data replication, backup and recovery, big data processing, and running hybrid cloud applications. It can be integrated with other AWS services, such as Amazon VPC, Amazon S3, and Amazon DynamoDB, enabling you to access and manage your AWS resources securely and efficiently.

Overall, AWS Direct Connect is a powerful and flexible service that can help you establish a private, secure, and reliable connection between your on-premises infrastructure and AWS cloud services, enabling you to leverage the benefits of the cloud while maintaining the security and control of your on-premises environment

--------------------------------------------------------------------------------------------------------
•	AWS Private Link Vs Direct connect
--------------------------------------------------------------------------------------------------------

AWS PrivateLink and AWS Direct Connect are two different services provided by AWS for connecting to resources in the AWS Cloud.

AWS PrivateLink is a service that enables you to access AWS services, such as Amazon S3, Amazon EC2, and others, over a private connection from your Amazon VPC or on-premises network, without using public IP addresses, NAT devices, or VPN connections. It allows you to access AWS services as though they were hosted within your own VPC, providing a more secure and scalable way to access these services.

On the other hand, AWS Direct Connect is a dedicated network connection service that provides a private, high-bandwidth, low-latency connection between your on-premises infrastructure and AWS cloud services. It allows you to establish a dedicated, private connection to AWS, bypassing the public internet, and provides higher bandwidth and lower latency compared to VPN or internet connections.

The main difference between AWS PrivateLink and AWS Direct Connect is the type of connection they provide. PrivateLink provides a private connection to specific AWS services within your VPC, while Direct Connect provides a dedicated private connection between your on-premises infrastructure and AWS cloud services.

Here are some additional differences between the two services:

AWS PrivateLink is typically used to access specific AWS services from within your VPC, while Direct Connect is used to access the entire AWS cloud.
PrivateLink is primarily used for accessing AWS services over a private connection within your VPC, while Direct Connect is used for more general connectivity needs.
PrivateLink is often used for services that have heavy data transfer requirements, while Direct Connect is used for services that require low-latency connections, such as real-time applications or data replication.
Overall, the choice between AWS PrivateLink and AWS Direct Connect depends on your specific use case and requirements. If you need to access specific AWS services within your VPC over a private connection, PrivateLink may be a better fit. If you require a dedicated, private connection to the entire AWS cloud with lower latency and higher bandwidth, Direct Connect may be the better choice.

--------------------------------------------------------------------------------------------------------
•	Bastion Hosts
--------------------------------------------------------------------------------------------------------

A bastion host is a specially configured instance in a public subnet of an Amazon Virtual Private Cloud (VPC) that allows secure remote access to instances in private subnets. The bastion host acts as a gateway, providing a single point of entry for remote access to the private instances, while isolating them from the public internet.

The main purpose of a bastion host is to provide a secure and controlled way to access instances in private subnets. Typically, the private instances are not directly accessible from the internet, and can only be accessed through the bastion host. This helps to improve the security of the private instances by reducing their exposure to external threats.

Here are some key features of a bastion host:

Secure remote access: The bastion host provides a secure and controlled way to access instances in private subnets, using SSH or other remote access protocols.

Isolated network access: The private instances are isolated from the public internet, and can only be accessed through the bastion host. This reduces their exposure to external threats.

Monitoring and logging: The bastion host can be monitored and logged to detect and prevent unauthorized access.

Scalability: Multiple bastion hosts can be deployed to support high availability and scalability.

Customization: The bastion host can be customized to meet specific security and access control requirements.

In summary, a bastion host is an essential component of a secure and scalable VPC architecture. By providing a secure and controlled way to access instances in private subnets, the bastion host helps to improve the security and isolation of the private instances, while providing a scalable and flexible solution for remote access.




-----------------------
https://www.knowledgehut.com/tutorials/aws/aws-bastion-host
AWS Bastion Host - How to create it?
Security is a prime concern for almost any company, which use the services to store their own data. Even though Amazon provides excellent security with its services, it is strongly suggested by Amazon to use SSH access to further secure the services and their instances. This is when bastion host comes into the picture.

A bastion host can be thought of as a special purpose machine, which has been configured to work against attacks. The machine contains a single application only, which it hosts. It has access to the public network, and it also known as a ‘Jump Box’. It is a powerful server, which provides high-level network security, since it is the only host that is granted permission to access the public network.

This machine can be used by system administrators to connect to other instances of service, which happens in the infrastructure backend. This usage place with the help of many authentication mechanisms making sure that the system is safe.

These hosts are accessed with the help of SSH or RDP protocols. After a connectivity (remotely) is established with the bastion host, it allows using SSH or RDP to log in to other instances (thereby behaving like a ‘jump server’), that are present within the private network/subnet.

Once the connection is properly configured with the help of security groups and network ACLs (NACL), bastion host behaves like a bridge between the private instances of the service and the internet, thereby protecting the instances from attacks outside.

When is a bastion host needed?
If a user is confused whether they need a bastion host or not, ponder over the question- Do I need remote connection to my private instance of a service, through the public internet? If the answer to this question is ‘Yes’, then a bastion host is required, otherwise it is not needed.

The below snip shows how a bastion host can be used to connect to a private instance of the AWS infrastructure:

AWS Bastion Host

Designing a bastion host for AWS infrastructure
A bastion host designed to work with a specific infrastructure should work with that unit only, and nothing else. The reason behind limiting the usage of bastion host to a specific instance/requirement is to avoid formation of unnecessary security loopholes.

Steps to create a bastion host for a specific AWS infrastructure
Following are the steps to create a bastion host:

Sign into your AWS account.
Create an EC2 instance or launch an EC2 instance which was previously defined.
Harden the OS, which basically refers to increasing the security which has been provided by the OS.
Specify appropriate security groups or create a security group for the bastion host.
This will open up the port 22 which is usually used with SSH.
Select a source, which is done to ensure that relevant people (who have access to add their IPs) have access to the Bastion host.
The security groups of the current instances have to be changed to make sure that inbound SSH (if any) can be accessed through the Bastion Host’s IP address only.
The local ~/.ssh/config file has to be edited to reflect the bastion host name, username, and a ‘Yes’ value for the ForwardAgent field. This is used to set up the SSH forwarding via the local machine to the bastion host so that the file used to access the EC2 instance is made available only when the user tries to connect to one of the servers.
Username refers to the person who has the rights to login to the server. Hostname refers to the IP address of the bastion host.
This makes sure that the user can SSH into the Bastion server by just typing ‘ssh bastion’ from the command line interface.
Bastion Host needs to be accessed with the help of SSH. into an existing server instance, and this way, a much tighter security would be built into the servers, by making these servers accessible only through a Bastion host.
Security groups play a vital role in maintaining the security and making sure the Bastion host doesn’t fail. A security group is created so that it can be used to allow the connection of a bastion host to a private instance (the instance is owned by the user).

This security group should only have the ability to accept inbound requests from SSH or RDP (Remote Desktop Gateway), which would come from the bastion host which is across the user’s Availability Zone. The rules of this security group should be applied to all the private instances of the user, which need tighter security, and which need to be able to connect to the internet via a Bastion Host.

The same security group needs to be applied to the bastion host.  The incoming requests should only accept connections from an SSH or RDP connection, which comes from specific IP addresses only (like those from administrators, or those who have been granted permission by an admin, to have the ability to connect to the instance). On the same lines, the outgoing connection should be able to access private instances via SSH or RDP only.

Make sure to avoid access to IP addresses like 0.0.0.0/0. SSH and RDP connections can be authenticated with the help of private and public keys. Logging in to the private instances via the bastion host will require the bastion host to have the private keys. But storing private keys on a remote instance isn’t considered a safe security method. Because of this reason, AWS recommends implementing the RDP (to connect to instances on Windows) or the SSH agent forwarding technique (to connect to instances on Linux). These solutions remove the need to store private keys on the bastion host.

It is highly recommended to deploy a bastion host within a public Availability Zone which is currently being used. It should be made sure that the services are resilient to failures and are highly available most of the time.

In case the hosting in a specific availability service goes down or fails, the connection to the private instance in other availability zones will also fail.
--------------------------------------------------------------------------------------------------------
•	NAT Instances
--------------------------------------------------------------------------------------------------------

An AWS Bastion Host is a special purpose instance that acts as a secure gateway to connect to your other instances in a private subnet within a Virtual Private Cloud (VPC). It's often used as an additional layer of security to secure SSH and RDP access to your EC2 instances, which are located in private subnets and do not have public IP addresses.

The Bastion Host is typically placed in a public subnet of your VPC and is used to securely connect to your EC2 instances within private subnets. It acts as a single entry point to your private subnet and eliminates the need to expose SSH or RDP ports on your EC2 instances directly to the Internet. By accessing the Bastion Host first and then using it to connect to other instances within the private subnet, you can reduce the attack surface and better protect your resources from potential security threats.

In addition to providing a secure gateway to access your private subnet, a Bastion Host can also be used for other tasks, such as system administration, monitoring, and troubleshooting. It can be configured with the appropriate security controls, such as Multi-Factor Authentication (MFA) and identity and access management (IAM) policies, to ensure that only authorized users can access it.

AWS offers several ways to set up a Bastion Host, including using EC2 instances or deploying a managed Bastion Host using AWS Systems Manager. By using a Bastion Host, you can enhance the security of your infrastructure and provide a secure gateway for remote access to your private instances in a VPC.

--------------------------------------------------------------------------------------------------------
•	Nat gateway
--------------------------------------------------------------------------------------------------------

An AWS NAT (Network Address Translation) Gateway is a fully managed service that allows instances within a private subnet in a Virtual Private Cloud (VPC) to access the Internet. NAT Gateway provides high availability, automatic scaling, and improved performance compared to traditional NAT instances.

When an instance in a private subnet needs to access the Internet, it sends a request to the NAT Gateway, which then forwards the request to the Internet. The NAT Gateway then sends the response back to the original instance in the private subnet. This allows the private subnet to access resources on the Internet, such as software updates or patches, without being exposed to the public Internet.

NAT Gateway is a fully managed service, which means that AWS takes care of scaling and managing the NAT Gateway infrastructure. This allows users to focus on their core business functions rather than managing and scaling NAT instances. NAT Gateway can scale automatically up to 45 Gbps of network throughput, making it suitable for high-traffic workloads.

AWS NAT Gateway can be created and configured using the AWS Management Console, AWS CLI, or AWS SDKs. When creating a NAT Gateway, users must select the VPC and the subnet in which the NAT Gateway will be created. It is important to note that NAT Gateway is a regional service and can only be used within the same region as the VPC.

NAT Gateway is a popular choice for organizations that need to provide their private instances with Internet access, but still want to maintain a secure environment. It can be used in conjunction with AWS security features, such as security groups and network ACLs, to provide additional layers of security.

--------------------------------------------------------------------------------------------------------
•	NAT Instances Vs NAT Gateways
--------------------------------------------------------------------------------------------------------

AWS NAT (Network Address Translation) instances and NAT Gateways both provide a way for instances in a private subnet to access the Internet while still maintaining a secure architecture. However, there are some key differences between the two:

Managed vs. self-managed: NAT Gateway is a fully managed service, meaning that AWS takes care of scaling and managing the NAT Gateway infrastructure, while NAT instances need to be self-managed.

Automatic scaling: NAT Gateway can automatically scale up to 45 Gbps of network throughput, while NAT instances need to be manually scaled by adding or removing instances based on traffic.

Performance: NAT Gateway provides better performance compared to NAT instances, which can be limited by the performance of the underlying EC2 instance.

High availability: NAT Gateway provides higher availability compared to NAT instances, as it is designed to automatically failover to a standby NAT Gateway in the event of a failure.

Billing: NAT Gateway is billed based on the amount of data processed, while NAT instances are billed based on the EC2 instance type and the amount of data processed.

In general, NAT Gateway is recommended over NAT instances for most use cases due to its improved performance, scalability, and high availability. However, NAT instances may be a good option for organizations that require more control over the NAT infrastructure, or for those with specialized networking requirements that cannot be met by NAT Gateway.

--------------------------------------------------------------------------------------------------------
•	NACL and security groups
--------------------------------------------------------------------------------------------------------

Network Access Control Lists (NACLs) are stateless, subnet-level firewalls that can be used to control traffic at the network level. NACLs can be associated with subnets and can contain rules that allow or deny traffic based on the source or destination IP address, protocol, and port number. NACLs are evaluated in order, starting with the lowest rule number, and the first rule that matches the traffic determines whether the traffic is allowed or denied.

Security groups, on the other hand, are stateful, instance-level firewalls that can be used to control traffic at the instance level. Security groups can be associated with instances and can contain rules that allow or deny traffic based on the source or destination IP address, protocol, and port number. Security groups are evaluated independently of each other, and traffic that is allowed in is automatically allowed out, regardless of the outbound rules.

In general, NACLs are used to provide network-level security, while security groups are used to provide instance-level security. NACLs can be used to block or allow entire subnets or ranges of IP addresses, while security groups are used to control access to specific instances based on their security groups. Additionally, NACLs are evaluated before security groups, meaning that if a rule in a NACL denies traffic, the traffic will not even reach the security group.

It is important to note that both NACLs and security groups are essential components of network security in AWS, and that they should be used in conjunction with each other to provide a comprehensive security solution for VPCs.


-----------------------
https://www.knowledgehut.com/tutorials/aws/aws-nacl
AWS Network Access Control List - What are its Components?
NACL refers to Network Access Control List, which helps provide a layer of security to the Amazon Web Services stack.  

NACL helps in providing a firewall thereby helping secure the VPCs and subnets. It helps provide a security layer which controls and efficiently manages the traffic that moves around in the subnets. It is an optional layer for VPC, which adds another security layer to the Amazon service. 

VPC refers to Virtual private Cloud, which can be visualized as a container that stores subnets. Subnets can be considered as a container, which helps store data.  

Components of NACL
Following are the components of Network Access Control List (NACL): 

Rule number: Every rule is assigned a unique number. The rule’s priority is also based on the number it is assigned. When it matches to a specific request or traffic, this rule is applied to the request, irrespective of whether another high-numbered rule contradicts it or not.  
Rules are created with specific increments, like the difference between 2 rules is either 1,10, 100 and all the rules created have this same difference.  

Type: This tells about the type of traffic, like SSH, HTTP, HTTPS.  
Protocol: Protocol is a set of rules, that is applied to every request, ex: http, https, ICMP, SSH. 
Portrange: The listening port, which takes in the request from the user, such as HTTP is associated with port 80.  
Inboundrules: Also known as source. These rules talk about the source from where the request or traffic is coming from, and about the destination port/ the port through which the response is sent.  
Outboundrules: Also known as destination. These rules talk about where the response should be sent and about the destination port.  
Allow/Deny: Whether the specific traffic has to be allowed or denied.  
There are two types of NaCl: 

Customized NACL: It can also be understood as a user-defined NACL, and its inherent characteristic is to deny any incoming and outgoing traffic until a rule is added to handle the traffic.  
Default NACL: This is the opposite of customized NACL, which allows all the traffic to flow in and out of the network. It also comes with a specific rule which is associated with a rule number, and it can’t be modified or deleted. When the request doesn’t match with its associated rule, the access to it is denied. When a rule is added or removed, changes are automatically applied to the subnets which are associated with it.  
Let us consider the below use case: 

When a website needs to be accessed, the request from the user has to hit the right port, and the website has to access the database and by extracting the appropriate data, it has to give back a response to the user.  

The VPC comes in-built with a default NACL, which applies to ipv4 traffic. A custom NACL can be created, which can be associated with a subnet. This customized NACL’s default behaviour is to deny incoming and outgoing ipv4 traffic. It has to be specified rules, so as to behave in a certain way when it receives a request.  

Multiple subnets can be bound with a single NACL, but one subnet can be bound with a single NACL only, at a time.  




Security groups vs Network ACLs - What is the
https://www.knowledgehut.com/tutorials/aws/nacl-vs-security-groups
--------------------------------------------------------------------------------------------------------
•	VPN, virtual private gateway and customer gateway
--------------------------------------------------------------------------------------------------------

VPN (Virtual Private Network) is a technology that provides secure and private network communication over a public network such as the Internet. AWS provides a VPN solution through its Virtual Private Gateway and Customer Gateway services.

A Virtual Private Gateway is an AWS service that enables customers to establish a VPN connection between their on-premises data center and their Amazon VPC. The Virtual Private Gateway acts as an endpoint for the VPN connection on the AWS side of the network, while the Customer Gateway acts as the endpoint on the customer's side of the network. The Virtual Private Gateway provides a highly available and scalable gateway for customers to connect their on-premises network to their VPC.

A Customer Gateway is a physical device or software application that is installed and configured on the customer's network. The Customer Gateway serves as the endpoint for the VPN connection on the customer's side of the network, and is responsible for encrypting and decrypting the traffic that flows between the on-premises network and the VPC.

Together, the Virtual Private Gateway and Customer Gateway form a VPN connection that provides a secure and private network communication channel between the on-premises network and the VPC. The VPN connection allows customers to extend their existing IT infrastructure into the AWS cloud, and provides a way to securely access resources within the VPC from their on-premises network.

It is important to note that VPN connections are billed based on the amount of data transferred over the connection, and that both the Virtual Private Gateway and Customer Gateway may have associated costs depending on the configuration and usage.



--------------------------------------------------------------------------------------------------------
•	Direct connect and direct connect gateway
--------------------------------------------------------------------------------------------------------

AWS Direct Connect is a service that enables customers to establish a dedicated network connection between their on-premises data center and AWS. The connection is established through a private, high-speed network connection provided by a Direct Connect partner, which provides a more reliable and consistent connection compared to a traditional internet connection.

A Direct Connect Gateway is a service that enables customers to connect multiple VPCs in different AWS Regions or AWS accounts to a single Direct Connect connection. The Direct Connect Gateway acts as a hub, allowing traffic from any of the connected VPCs to be routed over the Direct Connect connection.

The Direct Connect Gateway provides several benefits, including improved network performance, reduced network costs, and increased network security. By using a Direct Connect connection and Direct Connect Gateway, customers can establish a private and secure network connection between their on-premises data center and multiple VPCs in different AWS Regions or AWS accounts.

It is important to note that Direct Connect and Direct Connect Gateway are separate services and have different pricing models. Direct Connect is billed based on the speed of the connection, while Direct Connect Gateway is billed based on the number of virtual interfaces connected to the gateway. Additionally, there may be additional charges for data transfer over the Direct Connect connection and for the use of Direct Connect partner services.


--------------------------------------------------------------------------------------------------------
•	Overview of VPC traffic monitoring
--------------------------------------------------------------------------------------------------------

VPC (Virtual Private Cloud) traffic monitoring is a critical component of network security in AWS. Monitoring network traffic helps to detect and mitigate security threats, troubleshoot network issues, and optimize network performance.

AWS provides several tools and services for VPC traffic monitoring, including:

VPC Flow Logs: This is a feature that captures information about the IP traffic going to and from network interfaces in a VPC. VPC Flow Logs can be used to monitor traffic behavior, detect anomalies, and investigate security incidents.

Amazon CloudWatch Logs: This is a monitoring service that can be used to collect, analyze, and store logs from VPC Flow Logs. CloudWatch Logs can be used to gain insights into network traffic, monitor network performance, and detect security incidents.

AWS Network Firewall: This is a managed service that provides network security for VPCs. Network Firewall can be used to monitor network traffic, block traffic that violates security rules, and protect against DDoS attacks.

AWS Config: This is a service that provides a detailed inventory of AWS resources and their configuration settings. Config can be used to monitor changes to VPC resources, detect noncompliant configurations, and troubleshoot issues.

Third-party tools: There are also third-party tools available that can be used for VPC traffic monitoring, such as security information and event management (SIEM) solutions.

In summary, VPC traffic monitoring is essential for ensuring network security, detecting and mitigating threats, and optimizing network performance in AWS. AWS provides several tools and services for VPC traffic monitoring, including VPC Flow Logs, CloudWatch Logs, Network Firewall, AWS Config, and third-party tools.

--------------------------------------------------------------------------------------------------------
•	Networking Costs in AWS
--------------------------------------------------------------------------------------------------------
Networking costs in AWS can vary depending on a number of factors, including the amount of data transfer, the type of traffic, the region, and the services used. Here are some of the most common networking costs in AWS:

Data Transfer: Data transfer costs apply to traffic that flows between AWS services, between AWS and the internet, or between different AWS Regions. Data transfer pricing varies based on the amount of data transferred and the distance between the source and destination.

Inter-Region Data Transfer: This refers to data transfer between different AWS Regions. Inter-Region data transfer pricing is higher than Intra-Region data transfer pricing.

Elastic IP Addresses: Elastic IP addresses (EIPs) are public IP addresses that can be assigned to instances in a VPC. There is no charge for an EIP as long as it is being used by a running instance. However, if an EIP is not associated with a running instance, there may be additional charges.

Load Balancer: AWS provides several load balancing services, including Classic Load Balancer, Application Load Balancer, and Network Load Balancer. Load balancer pricing varies based on the type of load balancer, the number of load balancer hours, and the amount of data processed.

VPN and Direct Connect: VPN and Direct Connect are two services that can be used to establish a secure network connection between on-premises infrastructure and AWS. Both services have associated costs based on the amount of data transferred.

Transit Gateway: AWS Transit Gateway is a service that can be used to connect multiple VPCs and on-premises networks. Transit Gateway pricing varies based on the number of attachments and the amount of data transferred.

It's important to note that AWS provides a free usage tier for many of its services, including some networking services, which can be used for experimentation, prototyping, or testing. Additionally, AWS provides a cost explorer tool that can be used to estimate and analyze costs associated with using AWS services.
--------------------------------------------------------------------------------------------------------
•	CloudFront
--------------------------------------------------------------------------------------------------------

AWS CloudFront is a content delivery network (CDN) that speeds up the delivery of static and dynamic web content, such as HTML, CSS, JavaScript, images, and videos. It distributes content from servers located around the world, delivering it to users from the server closest to them. This helps to reduce latency and improve the user experience.

CloudFront integrates with other AWS services, such as Amazon S3, Amazon EC2, and Elastic Load Balancing, as well as third-party services, such as content management systems and video platforms. It also offers a range of features and capabilities, including:

DDoS protection: CloudFront provides protection against distributed denial-of-service (DDoS) attacks using AWS Shield.

Custom SSL certificates: CloudFront allows you to use custom SSL/TLS certificates to encrypt data in transit between CloudFront and your users.

Access control: CloudFront provides a range of access control options, including signed URLs and cookies, to restrict access to your content.

Real-time logs: CloudFront provides real-time logs that can be used to monitor usage and analyze traffic patterns.

Analytics: CloudFront integrates with Amazon CloudWatch and other analytics tools to provide detailed metrics and insights.

Streaming: CloudFront supports the streaming of audio and video content using popular protocols such as HLS and RTMP.

CloudFront charges for data transfer out of the service, the number of requests made, and data transfer between AWS regions. Pricing can vary based on the location of the CloudFront edge server used to deliver content, the volume of data transferred, and the frequency of requests. However, CloudFront also offers a free tier of service, which includes 50 GB of data transfer per month and 2,000,000 HTTP and HTTPS requests per month for a year.


--------------------------------------------------------------------------------------------------------
•	AWS Global accelerator Vs CloudFront
--------------------------------------------------------------------------------------------------------

AWS Global Accelerator and CloudFront are both AWS services that can improve the performance and availability of your applications. However, they serve different use cases.

AWS Global Accelerator is a network-layer service that uses AWS's global network to optimize the performance and availability of your applications. It directs traffic to the optimal AWS endpoint based on your network policies and traffic characteristics. AWS Global Accelerator can help reduce latency, increase throughput, and improve the availability of your applications.

On the other hand, CloudFront is a content delivery network (CDN) that speeds up the delivery of your static and dynamic web content, such as HTML, CSS, JavaScript, images, and videos. It distributes content from servers located around the world, delivering it to users from the server closest to them. This helps to reduce latency and improve the user experience.

In summary, AWS Global Accelerator is best suited for applications that require high availability and low latency for non-HTTP applications such as gaming or IoT. CloudFront, on the other hand, is best suited for applications that require faster delivery of web content to users, such as static and dynamic content, and streaming media.

--------------------------------------------------------------------------------------------------------
•	Elastic Network Interfaces (ENIs)
--------------------------------------------------------------------------------------------------------

Elastic Network Interfaces (ENIs) are virtual network interfaces that can be attached to Amazon Elastic Compute Cloud (EC2) instances in a Virtual Private Cloud (VPC). An ENI acts as a virtual network card, allowing an EC2 instance to communicate with other resources in the VPC, such as other EC2 instances, RDS databases, and Elastic Load Balancers.

ENIs can be created and attached to EC2 instances on demand, and can be moved between instances within the same VPC. This allows for flexible and scalable network architectures, such as attaching multiple ENIs to an EC2 instance to create a high-availability architecture.

ENIs also support advanced networking features, such as multiple IPv4 and IPv6 addresses, Elastic IP addresses, and network traffic mirroring. They can also be used to create and manage network interfaces for other AWS services, such as AWS Lambda and AWS Elastic Beanstalk.

ENIs are charged by the hour for each interface in use, as well as for data transfer in and out of the VPC. The cost of ENIs can vary depending on the number of interfaces used and the amount of data transfer. However, the first ENI that is attached to an EC2 instance is free of charge.

--------------------------------------------------------------------------------------------------------
•	VPC Security
--------------------------------------------------------------------------------------------------------

VPC security refers to the various mechanisms and features that are used to secure a Virtual Private Cloud (VPC) in Amazon Web Services (AWS). VPC security is a critical component of any AWS deployment, as it helps to ensure the confidentiality, integrity, and availability of your applications and data.

Some key elements of VPC security include:

Security groups: Security groups act as virtual firewalls that control the inbound and outbound traffic to your EC2 instances. You can configure security groups to allow or deny traffic based on source/destination IP addresses, ports, and protocols.

Network ACLs: Network ACLs provide an additional layer of security by allowing you to control traffic at the subnet level. Network ACLs can be used to allow or deny traffic based on IP addresses, ports, and protocols.

VPC flow logs: VPC flow logs allow you to capture information about the IP traffic going to and from network interfaces in your VPC. You can use flow logs to monitor and troubleshoot network traffic, and to detect and respond to security threats.

Virtual private gateway: A virtual private gateway enables you to establish a secure connection between your VPC and your on-premises data center or other remote network.

AWS WAF: AWS WAF is a web application firewall that can be used to protect your web applications from common web exploits and attacks.

AWS Shield: AWS Shield is a managed DDoS protection service that provides additional protection against distributed denial of service (DDoS) attacks.

By implementing these and other security measures, you can help to protect your VPC and its resources from unauthorized access, data breaches, and other security threats.



--------------------------------------------------------------------------------------------------------
	Lab: 
	Create and manage
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	VPC
--------------------------------------------------------------------------------------------------------
https://4sysops.com/archives/aws-vpc-internet-gateway-route-tables-nacls/
https://4sysops.com/archives/aws-vpc-overview-setup-subnets/
https://4sysops.com/archives/aws-vp-setup-ec2-instance-test-connectivity/
--------------------------------------------------------------------------------------------------------
•	Subnets
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Public and private subnets
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Internet Gateway
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Route Tables
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Bastion Hosts
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	NAT Instances
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Nat Gateways
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	NACL
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Security Group
--------------------------------------------------------------------------------------------------------



--------------------------------------------------------------------------------------------------------
•	VPC peering
--------------------------------------------------------------------------------------------------------
You can request a 
	VPC peering connection 
		with another VPC in your account
	or 
		with a VPC in a different AWS account. 
	access it using private ip.	
		
		https://docs.aws.amazon.com/vpc/latest/peering/create-vpc-peering-connection.html
--------------------------------------------------------------------------------------------------------
•	VPC Endpoint
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/vpc-interface-endpoints.html

Activity
	Create an instance in a private subnet 
	For now create it in a public vpc
			curl google.com
			aws configure
			aws s3 ls
			
			(if above is not working, then give roles to the instance)
			
		but remove egress after connecting
			go to nacl (network acl update rule to my ip)
			(connect using quick access in aws console not working 
				but gitbash connection works)
		check that s3 access is failing
		create a vpc endpoing and attach it.
		now it should be able to access s3 
			but ping google.com should fail.
			
	There are few more steps follow
		
		Network ACL rules
			both inbound and outbound rules needs to be added.
		https://aws.amazon.com/premiumsupport/knowledge-center/connect-s3-vpc-endpoint/
	


A Virtual Private Cloud (VPC) endpoint 
	VPC resource 
	create private connection between 
		VPC and 
		another AWS service 
			without 
				internet
				a VPN connection
				or AWS Direct Connect. 
	e.g. 
		connect to 
			Amazon S3, 
			Amazon DynamoDB
			Amazon ECR 
				using a private connection 

Three types of VPC endpoints: 

	Interface endpoints: 
		To create an interface endpoint 
			select a VPC and a subnet
				specifying the service that you want to access. 
				accessed using public/private DNS name of service
	Gateway endpoints: 
		VPC resources 
			route traffic to 
				Amazon S3 buckets 
				Amazon DynamoDB tables. 
	Gateway Load Balancer endpoints

Interface endpoints and Gateway Load Balancer endpoints 
	powered by AWS PrivateLink
	uses ENIs (Elastic Network Interfaces) 
		as an entry point 
Gateway endpoints
	powered probably by vpc peering (documentation is silent)
Gateway endpoints and Gateway Load Balancer endpoints 
	serve as a target for a route 
		in your route table for traffic destined for the service.	
		
	
	To create a VPC endpoint
		In the VPC dashboard 
			go to "Endpoints"
				rest is self explanatory
				

Advantages: 
	● Access Amazon S3 or Amazon DynamoDB 
		from VPC 
		without exposing data to the internet 
	● Offloading data transfer 
		from on-premises 
		to AWS
			use AWS Direct Connect and a VPC endpoint 
	● Reduce the use of VPN connection  
	● Improve security  
--------------------------------------------------------------------------------------------------------
•	VPN, Virtual private gateway and Customer Gateway
--------------------------------------------------------------------------------------------------------
Try
https://aws.amazon.com/blogs/networking-and-content-delivery/simulating-site-to-site-vpn-customer-gateways-strongswan/
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------

Hands on 
https://catalog.workshops.aws/general-immersionday/en-US/advanced-modules/network
--------------------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------------------
	o	Security
--------------------------------------------------------------------------------------------------------
 
Introduction to defense at deapth
	authentication and authorization
	data protection
		data at rest
		data in motion
	monitoring and enforcement
		attack identification
			valid user makes vulnerabile req.
			invalid user makes valid req.
			invalid user makes invalid req.
		blocking attack
		mitigating attacks

AWS Security at a high level includes
	Identity and Access Management
		Amazon Cognito
			integrate with 3rd party for authentication.
				e.g. google
				
				User pools
				identity pools
		AWS SSO
			centrally manage aws sso
				manage user permission across various accounts
				uses SAML
		AWS Directory Services
			provides
				Amazon Cloud Directory
				Microsoft Active Directory
				other similar services
		AWS Resource access manager
			securely share AWS resources with 
				aws accounts
				aws organizations
			
		
	Detective Controls
		Infrastructure protection
			AWS Shield
				managed DDoS protection service 
			AWS Firewall Manager
				security management service
				centrally configure and manage firewall rules across accounts and applications in organization
			AWS Web Application Firewall
				protects web apps and api's
					against common exploits affecting
						availability
						security
						resource consumption

	Data Protections
		Amazon Macie
			fully managed data security and data privacy service
			uses ML/AI pattern matching 
				discover and protect sensitive data
				
		AWS Key Management Service
			create and manage 
				cryptographic keys 
			control their use across applications
		AWS CloudHSM
			cloud based Hardware security module (HSM)
			enables to generate and use 
				our own encryption keys 
		AWS Certificate manager 
			provision and manage public and private (SSL/TLS) certificates 
	Compliance

--------------------------------------------------------------------------------------------------------
		Data protection
--------------------------------------------------------------------------------------------------------
	Data 
		is growing 
			zettabytes size - 1ZB = 1 Trillion GB
			current (181 ZB)
		has value
		needs 
			protection
			governance

	Data protection
		strategies one puts into place to 
			safeguard critical and imp. data
				from 
					loss
					compromise and 
					corruption
				recover data to operational state
					to ensure business continuity


	In-build data protection available
	Yet customers write scripts to manage security
		siloes solutions
		compliance
--------------------------------------------------------------------------------------------------------
			AWS PrivateLink
--------------------------------------------------------------------------------------------------------



https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-privatelink.html
https://docs.aws.amazon.com/vpc/latest/privatelink/what-is-privatelink.html

Various ways to connect from outside to service inside
	VPC 
		Public access: NAT Gateway + Internet gateway
			enable public dns
		Private access: (secure connection)
			1. Create private subnet
				Create NAT Gateway and route through that 
			2. Create site to site VPN	
				Access VPC instances
			3. Use direct connect 



AWS PrivateLink 
	highly available, 
	scalable technology 
	privately connect your VPC to services 
		as if they were in your VPC. 
	do not need 
		internet gateway, 
		NAT device, 
		public IP address, 
		AWS Direct Connect connection 
		or 
			AWS Site-to-Site VPN connection 
		to allow communication with the service from your private subnets. 
		
	Refer dia in https://docs.aws.amazon.com/vpc/latest/privatelink/what-is-privatelink.html	
	VPC on the left 
		has several EC2 instances 
			in a private subnet and 
			three interface VPC endpoints. 
		The top-most VPC endpoint 
			connects to AWS services. 
		The middle VPC endpoint 
			connects to a service hosted by another AWS account 
			(a VPC endpoint service). 
		The bottom VPC endpoint 
			connects to an AWS Marketplace partner service.
			
	Private link 
		establish connection between 
			vpc's and
				services on aws
				services on marketplace
				services on-prem 
					without exposing data to internet

	Why AWS Private link
	--------------------
	1. Security
		connect vpc's to services in aws in 
			secured
			scalable manner
		use private ip and security groups
			service functions like they were hosted in your private network.
	2. Simplify network management
		connect services across different accounts and amazon vpc's
		No need of 
			internet gateway
			vpc peering
			manage vpc classless internet domain routing (CIDR)
	3. Acclerate cloud migration
		easily migrate traditional on-premises applications to SaaS solutions in cloud using private link
		
		
	continue from https://www.youtube.com/watch?v=0bHXWIM4_0o 11 min.	
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
			AWS managed policies
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html
already done.
--------------------------------------------------------------------------------------------------------
			Compliance validation
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/compliance-validation.html


AWS Compliance Validation 
	process of verifying 
		organization's use of AWS 
			is in compliance with 
				applicable laws
				regulations
				industry standards. 
	Need to comply with laws like
		data protection laws
		security standards
		industry-specific regulations
			etc.

Compliance validation 
	identify compliance requirements 
	implement controls and best practices 
		to meet those requirements
	monitoring and reporting compliance over time. 

AWS services and features that can assist 
	AWS Artifact
	AWS Config
	AWS CloudTrail
	AWS Identity and Access Management (IAM)
	AWS Security Hub
	AWS Key Management Service (KMS).

e.g. of compliance standards and regulations 
	AWS compliance validation 
		Health Insurance Portability and Accountability Act (HIPAA)
		Payment Card Industry Data Security Standard (PCI DSS)
		International Organization for Standardization (ISO) standards
		Service Organization Control (SOC) framework
		Family Educational Rights and Privacy Act (FERPA), among others.


Enabling compliance validation in EC2 
	configure 
		monitor, 
		audit, 
		validate 
			compliance with various regulations and standards. 
	steps involved:
		Choose a Compliance Standard: 
			Determine the compliance standard(s) that you need to comply 
				HIPAA, 
				PCI DSS, or 
				ISO 27001. 
			requirements differ 
			

		Enable AWS Config: 
			provides a detailed inventory of 
				AWS resources 
				their configuration history
			can monitor resource 
				changes and 
				compliance 
					with desired configurations. 
			Enable AWS Config 
			create rules 
				define compliance requirements 
				
		Implement Configuration Management: 
			Use 
				AWS Systems Manager 
					to manage the configuration of EC2 instances
					like 
						install and configure software
						setting up security settings
						applying patches and updates. 
				Use AWS Config to monitor compliance 
					with your desired configurations 
					identify non-compliant resources.

		Enable Logging and Monitoring: 
			Enable AWS CloudTrail 
				record 
					all API calls 
					activity within your AWS account
			use AWS CloudWatch 
				to monitor log data 
				trigger alerts for specific events or conditions. 
			Use AWS Config 
				to monitor compliance 
					with logging and monitoring requirements 
					identify non-compliant resources.

		Implement Security Best Practices: 
			Implement security best practices 
				like 
					enabling multi-factor authentication
					use secure 
						passwords and 
						key pairs
					encrypt data at rest and in transit
				controlling network access. 
			Use AWS Config and other security services to monitor compliance 
				with security requirements and 
				identify non-compliant resources.

		Implement Disaster Recovery and Backup: 
			Implement 
				disaster recovery 
				backup using AWS Backup 
					to automate backups and recovery
				use AWS Disaster Recovery testing tools 
					to test disaster recovery scenarios. 
			Use AWS Config to monitor compliance with disaster recovery and backup requirements and identify non-compliant resources.


To be a managed instance, instances must meet the following prerequisites:
https://aws.amazon.com/premiumsupport/knowledge-center/systems-manager-ec2-instance-not-appear/
	Have the AWS Systems Manager Agent (SSM Agent) installed and running.
	Have connectivity with Systems Manager endpoints using the SSM Agent.
	Have the correct AWS Identity and Access Management (IAM) role attached.
	Have connectivity to the instance metadata service.
install ncat
sudo rpm -vhU https://nmap.org/dist/ncat-7.93-1.x86_64.rpm

amazon ssm
	https://www.youtube.com/watch?v=X_fznJtSyV8

lab:
	https://www.youtube.com/watch?v=Gx0R3sJJYhU
		min: 11:06
	install ssm agent 
		on amazon linux	
			https://docs.aws.amazon.com/systems-manager/latest/userguide/agent-install-al2.html
	Role setting
		min: 16:31	
		https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-instance-permissions.html




AWS Config (Config in search)
	Conformance pack
		(for standard Best practices)
		new 
			existing template
				
	Rules
		for simple rules.
		we can 
			use existing amazon rule 
		or 
			create custom rules
		"Rules"
			- "Add rule"
				Add AWS managed rule
					search for ec2
						select approved-amis-by-id
							mention one of the ami
			- Create two instances
			
	
Exercise
https://devopslearning.medium.com/100-days-of-aws-day-15-introduction-to-aws-config-9866fe36bec2

	https://www.youtube.com/watch?v=oBuLtfGHETY
		min. 24:25
or	https://www.youtube.com/watch?v=qHdFoYSrUvk
	
--------------------------------------------------------------------------------------------------------
			Resilience
--------------------------------------------------------------------------------------------------------
https://wa.aws.amazon.com/wellarchitected/2020-07-02T19-33-23/wat.concept.resiliency.en.html


AWS Resilience 
	ability of an organization's 
		applications to continue operating 
			during disruptive events like 
				hardware or software failures
				natural disasters
				cyber attacks
				human errors. 
	Need to 
		design
		implement
		test 
			systems and applications 
				to automatically recover 
					from disruptions
					minimize downtime and data loss
				maintain service availability and performance.

Key concepts and best practices for resilience:
	Design for Failure: 
		Don't assume all 
		design systems for 
			fault-tolerant 
			and resilient to failure. 
		Use 
			multiple Availability Zones (AZs) 
		and 
			regions 
				to distribute 
					workloads 
					data
		implement 
			redundancy and 
			failover mechanisms
		use automated scaling and recovery tools.
	Implement Backup and Recovery Strategies: 
		AWS services like  
			Amazon S3
			Amazon EBS
			AWS Backup can be used 
		use 
			Amazon EC2 Auto Recovery 
				to automatically recover EC2 instances.
	Monitor and Test Resilience: 
		Regularly monitor 
			health and performance of AWS systems and applications
		Proactively identify and address potential issues before they become critical. 
		Use AWS services like 
			Amazon CloudWatch
			AWS Config
			AWS Health 
				to monitor and alert 
					on system events and performance metrics
			AWS Disaster Recovery testing tools 
				test disaster recovery scenarios.
	Implement Security Best Practices: 
		secure against potential 
			threats and vulnerabilities
		implement security best practices like 
			MFA
			encryption
			access controls
			(IAM)AWS Identity and Access Management 
			(KMS)AWS Key Management Service 
			AWS Security Hub
	Other services used in Resilience: 
		Elastic Load Balancing (ELB)
		AWS Auto Scaling
		AWS Route 53
		AWS Lambda
		AWS Simple Notification Service (SNS). 
		
TBD: example can be covered along with s3 
	s3 redundancy
--------------------------------------------------------------------------------------------------------
			Infrastructure security
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/whitepapers/latest/introduction-aws-security/infrastructure-security.html


AWS Infrastructure security 
	measures and practices implemented 
		to protect their 
			systems and 
			data 
				on AWS cloud. 
	AWS provides 
		secure and compliant infrastructure 
			with various 
				security features and controls
	Strill our responsibility to secure our 
		applications and 
		data.

key aspects of AWS infrastructure security:

	Access Management: 
		IAM	
			manage 
				users
				groups
				roles
				granular permissions and policies. 
		(MFA) 
			extra layer of security 
		implement least privilege 

	Network Security: 
		Virtual Private Cloud (VPC) 
			isolate resources 
			control network traffic
			
			security groups and network access control lists (ACLs) 
				to control traffic flow. 
		AWS Direct Connect or AWS VPN 
			establish secure connectivity 
				between on-premises data centers and AWS
		encryption for 
			data in 
				transit and 
				at rest.

	Compliance and Governance: 
		implement 
			policies, 
			controls, and 
			procedures
		use AWS services like 
			AWS Artifact 
			AWS Config 
				compliance validation and 
				monitoring. 
			AWS CloudTrail 
				record and 
				monitor 
					activity across all AWS resources
			AWS CloudFormation 
				automate infrastructure deployment and configuration.

	Threat Detection and Response: 
		Detect and respond 
			to security threats 
		use AWS services like 
			Amazon GuardDuty 
			AWS Security Hub 
				to monitor and analyze events and alerts
			AWS Lambda and 
			AWS Step Functions 
				automate threat response and 
				incident response processes. 
			AWS Key Management Service (KMS) 
				manage and protect encryption keys
			implement data backup and disaster recovery mechanisms.

	Application Security: 
		use AWS services like 
			AWS WAF and 
			AWS Shield 
				protect against 
					web application attacks and 
					DDoS attacks.
					
TBD: hands on					
--------------------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------
Day 4

•	Load Balancing ELB Vs NLB Vs ALB	
--------------------------------------------------------------------------------------------------------

load balancer 
	single point for clients.
	distributes the incoming requests 
		to multiple targets
			e.g. Amazon EC2 instance. 
	spread out
		availability improves.


Elastic Load balancing supports the below mentioned type of load balancers:
	Application Load Balancer (ALB): 
		operates at Layer 7 (HTTP/HTTPS) 
		routes traffic to targets 
			based on the content of the request. 
			to http/https end points
		ideal for applications 
			web applications 
				microservices
				container based 
			require advanced routing features 
				e.g. 
					URL-based (path based) routing
					host-name-based (one.com/two.com) routing
					Query-String-based routing
						support for WebSocket protocols.
		ALB Target Groups can be
			EC2 instances 
				managed by auto scaling groups
			ECS tasks 
				managed by ECS
			Lambda functions 
				HTTP request translated to JSON events
			IP Addresses
				private IP's
		Can forward to multiple target groups
		Health check at target group level 
	Network Load Balancer (NLB): 
		operates at Layer 4 (TCP/UDP) 
		routes traffic to targets 
			based on IP protocol data. 
		ideal for applications that 
			require extreme performance
			static IP addresses for incoming traffic
			network-level health checks.

	Classic Load Balancer (CLB): 
		DEPRECATED
		operates at Layer 4 and Layer 7 
		legacy load balancer 
			getting phased out 
				to ALB and NLB. 
		supports 
			TCP/UDP and 
			HTTP/HTTPS traffic 
			provides basic load balancing capabilities.

	Gateway Load Balancer (GWLB): 
		New offering from AWS 
		simplify deployment of third-party virtual appliances
			like firewalls and VPNs
		GWLB operates at 
			Layer 3 (IP) and 
			Layer 4 (TCP/UDP) and 
				supports both IPv4 and IPv6 traffic.
		designed to deploy third-party virtual appliances like 
			firewalls
			intrusion detection and 
			prevention systems
			deep packet inspection systems
		distributes traffic across 
			multiple virtual appliances 
				in a single Availability Zone or 
				across multiple Availability Zones within a region. 
		enhance the security and compliance of applications 
			


	Each of these load balancers has its own strengths and weaknesses, and choosing the right one depends on your specific requirements and use case.






Hands on:
	ALB:
		Create 2 ec2 instances.
			install apache on it 
			using scripts
	
		
-------------
#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd 
systemctl enable httpd
echo "<h1>Hello from $(hostname -f)</h1>" > /var/www/html/index.html
-----------------

	Create an ALB 
		in same vpc and sg as instances
		
		define the target group as the ec2 instances
		ensure the security group is open to 0.0.0.0/0

			



-------------------
Old note
---------
A load balancer can be provided with a ‘listener’ that helps in checking for connection requests that come from clients. This listener uses a protocol and a port which is configured by the user, and forward the request from the client to a specific ‘target group’.  

A target group can be understood as a router which helps in routing the requests from one or multiple registered targets (one such example is an Amazon EC2 instance) with the help of a TCP protocol and the port (which is configured by the user). 

The target can be registered with multiple other target groups. Health checks can be configured on a per-target group basis. These health checks are performed on all targets that are associated with a target group (this is specified in a listener rule that is used with the load balancer). 

A network load balancer works in the fourth layer of the OSI model (Open Systems Interconnection). It has the capability to respond to millions of request every second. When a load balancer receives a connection request, it selects one target from the target group to assign a default rule. A TCP connection is tried to connect to the selected target wherein the port through which this connection happens is configured by the user in the listener configuration.  

When an Availability Zone is enabled for the load balancer, the Elastic Load Balancing creates a load balancer node in this Availability Zone. Every load balancer node distributes the incoming traffic to registered targets in their Availability Zone (of target groups) by default.  

An Elastic Load Balancing helps in the creation of a network interface for every Availability Zone that is enabled. Every load balancer node in this Availability Zone uses this interface to obtain a static IP address. When an Internet-facing load balancer is created, an elastic IP address per subnet can be associated optionally to the node.  

When a target group is created, the user specifies the target type that is used to determine whether the targets are registered by instance ID or IP address.

If a target is registered with the help of an instance ID, the source IP address of the client is stored and provisioned to the user applications.

If the target is registered with an IP address, the source IP address is private IP address of the load balancer node.

Targets can be removed or added as and when required from the load balancer. This will not affect the overall request flow to the user application. Elastic Load Balancing can be used to scale the traffic as and when the user application changes with time. Elastic Load Balancing helps in scaling a majority of workloads automatically.

Health checks for these applications can be configured which will be used to monitor the registered target’s health so that the load balancer gets to send requests to the healthy targets only.

Pre-requisites
	Availability Zone used with the EC2 instance needs to be specified.  
	VPC has to be configured with at least one public subnet in every Availability Zone. These public subnets are used in configuring the load balancer.  
	The EC2 instance can be launched with other subnets present in the same Availability Zone.  
	At least one EC2 instance should be launched in every Availability Zone that will allow the TCP access from clients which is present on the listener port. This way, the health check requests would also be allowed from the VPC.  
	Features of Network Load Balancer
	Can be used to handle varying sized workloads. 
	Can respond to millions of requests from applications per second.  
	It supports static IP address for the load balancer.  
	An Elastic IP address can also be assigned to every subnet that is enabled for the load balancer.  
	It supports registering targets based on IP addresses, and this includes targets which are outside the VPC of the load balancer.
	It supports routing requests of multiple applications which are present on a single EC2 instance. Every instance can be registered or an IP address that has the same target group which uses multiple ports can be specified.
	It provides support to containerized applications. ECS can be used to select an unused port when it schedules a task. This task can be registered with a specific target group using the same port. This way, the clusters can be efficiently used.
	Every service’s health can be monitored separately, and these health checks are defined at the target group level. In addition to this, CloudWatch metrics are associated with the target group level.
	When a target group is associated with an Auto Scaling group, it enables the user to scale every service dynamically based on the requirement and requests.
Steps to begin working with a Network Load Balancer
Following are the steps to begin working with a Network Load Balancer: 

Choosing a load balancer type
Open the Amazon EC2 console.  
On the navigation bar, select an appropriate region for the load balancer. Choose the same region for EC2 instances as well.  
In the ‘Navigation pane’ under ‘Load balancing’, click on ‘Load Balancers’.  
Click on ‘Create Load Balancer’. 
For ‘Network Load Balancer’, click on ‘Create’. 
Configuring load balancer and listener 
On ‘Configure Load Balancer’ page, enter a name for the load balancer. The name must be unique within the set of Application Load Balancers.  
Keep the default value for the ‘Scheme’, i.e internet-facing.  
Keep the default value for the ‘Listeners’, which accepts TCP traffic on port 80.  
In Availability Zones, select the same VPC which is used with EC2 instances.  
Click on ‘Next: Configure Routing’.  
Configuring target type 
Target group is used for routing the request. The listener rule routes the requests to the targets that have been registered with thin target group. The load balancer is used to check the health of target in this target group. In the ‘Configure Routing’ page, follow the below steps: 

Keep the default for ‘Target Group‘.  
Enter a ‘Name’ for the new ‘Target Group’.  
The ‘Protocol’ should be TCP, and ‘Port’ should be 80, and ‘Target Type’ is the instance.  
In ‘Health Checks’, let the default protocol be.  
Click on ‘Next: Register Targets’. 
Register target with target group 
In the ‘Register Targets’ page, select one or more ‘Instances’.  
Let the default port be 80 and choose the ‘Add to registered’. 
Once selection of instances in done, click on ‘Next: Review’.  
Create and test the load balancer 
On the ‘Review’ page, select ‘Create’.  
Once the load balancer has been created, and the notification says so, click on ‘Close’.  
In the navigation pane, under ‘Load balancing’, click on ‘Target groups’.  
Select the newly created target group. 
Click on ‘Targets’ and make sure the instances are ready.  
In the navigation pane, under ‘Load balancing’, click on ‘Load balancers’.  
Select the newly created load balancer.  
Click on the ‘Description’, and note down the DNS name of the load balancer.  
Paste this DNS name in the address filed of a web browser (with working internet connection).  
If everything works well, the browser will display the default page of the server.  













NLB Vs ALB Vs ELB
--------------------------------------------------------------------------------------------------------

AWS offers three types of load balancers, adapted for various scenarios: 
	Elastic Load Balancers, 
	Application Load Balancers, 
	Network Load Balancers. 

Common Features

All AWS load balancers 
	1. Distribute incoming requests 
	2. Implement health checks
		detect unhealthy instances. 
	3. Highly available and elastic 
		(in AWS parlance: 
			scale out and in 
				in a few minutes according to workload). 
	4. TLS termination 
	5. Internet-facing or internal. 
	6. Export useful metrics to CloudWatch 
	
	N.B: sudden spike in traffic 
		contact aws customer support 
			AWS support can “pre-warm” 
				else might take too long to adapt 
				
				
	Cost
		ELB in the us-east-1 region
			e.g. cost 
				$0.025 per ELB-hour + 
				$0.008 per GB of traffic.

AWS discourages the use of ELB in favor of its newer load balancers. Admittedly, there are very few scenarios where the use of an ELB would be preferable; typically, these are cases where you simply don’t have a choice. For example, your workload might still run on EC2-Classic, or you need the load balancer to use your own sticky session cookies, in which cases ELB would be the only option available to you.

You wrote the perfect tech blog post.
So why isn't anyone reading it?
Learn more
The Next Step in Load Balancing
In 2016, AWS launched its Elastic Load Balancing version 2, which is made up of two offers: Application Load Balancer (ALB) and Network Load Balancer (NLB). They both use a similar architecture and concepts. 

Most importantly, they both use the concept of “target groups,” which is one additional level of redirection. It can be conceptualized in this way. Listeners receive requests and decide (based on a wide range of rules) to which target group they will forward the requests. A target group then routes the requests to instances, containers, or IP addresses. Target groups manage the targets in terms of deciding how to split up the traffic and by performing health checks on the targets.

Both ALB and NLB can forward traffic to IP addresses, which allows them to have targets outside the AWS Cloud (for example: on-premises servers or instances hosted on another cloud provider).

Let’s now dive into both offers. 

Application Load Balancer
An Application Load Balancer (ALB) only works at layer 7 (HTTP). It has a wide range of routing rules for incoming requests based on host name, path, query string parameter, HTTP method, HTTP headers, source IP, or port number. In contrast, ELB only allows routing based on port number. Also, contrary to ELB, ALB can route requests to many ports on a single target. Plus, ALB can route requests to Lambda functions.

A very useful feature of ALB is that it can be configured to return a fixed response or a redirection. So you don’t need a server to perform such basic tasks because it is all embedded in the ALB itself. Also very importantly, ALB supports HTTP/2 and websockets.

ALB further supports Server Name Indication (SNI), which allows it to serve many domain names. (In contrast, ELB can serve only one domain name). There is a limit, however, to the number of certificates you can attach to an ALB, namely 25 certificates plus the default certificate.

An interesting feature of ALB is that it supports user authentication via a variety of methods, including OIDC, SAML, LDAP, Microsoft AD, and well-known social identity providers such as Facebook and Google. This can help you off-load the user authentication part of your application to the load balancer. 

ALB pricing is a bit more complicated than ELB. For the us-east-1 region, it would cost you $0.0225 per ALB + $0.008 per LCU-hour. The definition of an LCU can be found here. All in all, pricing is roughly equivalent to ELB.

ALBs are typically used for web applications. If you have a microservices architecture, ALB can be used as an internal load balancer in front of EC2 instances or Docker containers that implement a given service. You can also use them in front of an application implementing a REST API, although AWS API Gateway would generally be a better choice here.

Network Load Balancer
A Network Load Balancer (NLB) works at layer 4 only and can handle both TCP and UDP, as well as TCP connections encrypted with TLS. Its main feature is that it has a very high performance. Also, it uses static IP addresses and can be assigned Elastic IPs—not possible with ALB and ELB.

NLB natively preserves the source IP address in TCP/UDP packets; in contrast, ALB and ELB can be configured to add additional HTTP headers with forwarding information, and those have to be parsed properly by your application.

NLB pricing for the us-east-1 region is $0.0225 per NLB-hour + $0.006 per LCU-hour. The definition of an LCU for NLB is quite similar to that for ALB, and more information can be found here. All in all, pricing is roughly equivalent to ELB and ALB.

NLBs would be used for anything that ALBs don’t cover. A typical use case would be a near real-time data streaming service (video, stock quotes, etc.) Another typical case is that you would need to use an NLB if your application uses non-HTTP protocols.

Comparison Table
		
Basic load balancing features			ALB		NLB			ELB
 	 	 
Balance load between targets			Yes		Yes			Yes
Perform health checks on targets		Yes		Yes			Yes
Highly available						Yes		Yes			Yes
Elastic									Yes		Yes			Yes
TLS Termination							Yes		Yes			Yes
Performance								Good	Very high	Good
Send logs and metrics to CloudWatch		Yes		Yes			Yes
Layer 4 (TCP)							No		Yes			Yes
Layer 7 (HTTP)							Yes		No			Yes
Running costs							Low		Low			Low
 	 	 	 
Advanced load balancing features
 	 	 
Advanced routing options				Yes		N/A			No
Can send fixed response without backend	Yes		No			No
Supports user authentication			Yes		No			No
Can serve multiple domains over HTTPS	Yes		Yes			No
Preserve source IP						No		Yes			No
Can be used in EC2-Classic				No		No			Yes
Supports application-defined sticky session cookies	
										No		N/A			Yes
Supports Docker containers				Yes		Yes			Yes (*)
Supports targets outside AWS			Yes		Yes			No
Supports websockets						Yes		N/A			No
Can route to many ports on a given target	
										Yes		Yes			No
(*) Except for EKS with Fargate





•	Load Balancing with Elastic Load Balancing (ELB)	
--------------------------------------------------------------------------------------------------------


https://catalog.workshops.aws/general-immersionday/en-US/advanced-modules/compute
--------------------------------------------------------------------------------------------------------
o	Use cases of DNS
--------------------------------------------------------------------------------------------------------
Domain Name system 
	human friendly names 
		translatted to ip
	hierarichical naming structure
	Domain Registrar: 
		Amazon Route 53
		GoDaddy
	DNS Records
		A, AAAA, CNAME etc
	Zone file: 
		contains DNS records
		helps to resolve to ip
	Name Server:
		resolves DNS queries
	Top Level Domain (TLD)
		.com
		.us
		.in
		.gov
		.org 
		etc
	Second Level Domain(SLD):
		amazon.com
		google.com
		
	http://api.www.example.com.
							  |- . root
							.com: TLD
					.example.com: SLD
				.www.example.com: Sub domain
			api.www.example.com
				FQDN: Fully qualified domain name.
		http: protocol

	
	How DNS Works
	-------------
	https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.fixrunner.com%2Fwhat-is-dns%2F&psig=AOvVaw0ZkmO1QhEt8xk-4jQX6bvQ&ust=1679102491517000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCOD7z67m4f0CFQAAAAAdAAAAABAJ
			
	
--------------------------------------------------------------------------------------------------------




Day 5

•	AWS Storage (Data S3, EBS Storage, Glacier and more)
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
EBS Storage, 
--------------------------------------------------------------------------------------------------------
	Amazon Elastic Block Store (EBS) 
--------------------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------------------
Glacier
--------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------
S3, 

https://www.knowledgehut.com/tutorials/aws/what-is-aws-s3
https://catalog.workshops.aws/general-immersionday/en-US/basic-modules/60-s3
--------------------------------------------------------------------------------------------------------
o	S3 Cross-Region Replication
Amazon S3 has been used in a wide variety of ways. Customers have given feedback regarding Amazon’s service, and this is how the idea of cross-region replication was born. S3 objects copies can be made easily in another AWS region with the help of Cross region replication. This feature has been built over S3’s existing versioning facility. It can be used to copy files which are present in geographically diverse locations.

When the cross-region replication is enabled, an object which is uploaded to a specific S3 bucket gets automatically duplicated into a destination bucket that is located at a different AWS location.

The objects in the source and destination buckets are copied in an asynchronous manner. For this object replication, bucket-level configuration is necessary. The replication configuration has to be added to the source bucket.

The minimum requirements include:

A destination S3 bucket where objects are replicated to.
An AWS IAM role which S3 considers replicating objects on behalf of the user.
Features of cross-region replication
It helps seamlessly copy object from an S3 bucket in a specific location to another location’s S3 bucket.
It can be implemented only when both the buckets have their versioning feature enabled.
When an object is being copied from one bucket to another, data is encrypted with the help of SSL so as to protect it from attacks.
If data which is being copied from source bucket is already present in the destination bucket, data doesn’t get replicated even if the operation is performed. This way, redundant data is not stored.
When should cross region replication be used?
It can be used in the following cases:

Following the compliance requirements: When the compliance requirements need to be met: Amazon S3 allows the user to store data across various regions all over the world in specific Availability Zones by default, but compliance rules specify the exact region where data has to be stored (at much longer distances).  
Reducing the latency: If two users are in two different locations of the world, objects required by them can be accessed much more quickly by storing copies of the objects closer to the respective user’s locations.  
When two owners need to maintain copies of objects: When the same set of objects need to be used to analyse data, and two compute clusters require this data, wherein the clusters are present in different AWS Regions, object copies can be maintained in both the regions to improve the efficiency.  
--------------------------------------------------------------------------------------------------------
https://www.knowledgehut.com/tutorials/aws/aws-storage-gateway

How does AWS storage gateway work?
AWS Storage Gateway helps in connecting on-premises software appliance with cloud-based storage which helps in provisioning seamless integration of data security features between on-premises IT environment and AWS storage infrastructure. This service can be used to store data in the AWS Cloud which helps in scalability and cost-effectiveness as well as providing data security. 

It offers file-based, volume-based, and tape-based storage solutions. They have been listed below: 

File Gateway
A file gateway helps in storing a file interface inside Amazon S3. It combines a service and a virtual software appliance. With the help of this, objects can be stored and retrieved from Amazon S3 using industry-standard file protocols like Network File System (NFS), AND Server Message Block (SMB). It is deployed into the on-premise environment in the form of a virtual machine that runs on VMware ESXi or Microsoft Hyper-V hypervisor. It also provides access to objects in Amazon S3. 

With the help of a file gateway the following actions can be performed:  

Files can be stored and retrieved directly using the NFS version 3 or 4.1 protocol.  
The files can be stored and retrieved directly using the SMB file system version, 2 and 3 protocol.  
Files can be accessed directly in Amazon S3 from AWS cloud application or service.  
The data in Amazon S3 can be managed with the help of life cycle policies, cross-region replication, and versioning. The file Gateway can be visualized as a file system Mount on S3. 
It helps simplify the file storage method in Amazon S3 since it integrates with the applications that are available currently, with the help of industry-standard file system protocols. It also provisions cost-effective storage in comparison to the on-premise data storage facility. It gives low latency data access with the help of transparent local caching.  

It can also be used to manage data transfer to and from AWS, buffering applications with network congestions, optimizing stream data in parallel and managing the consumption of bandwidth.  

AWS services which can be integrated with file gateway include: 

Common access management system with the help of AWS IAM (Identity and Access Management). 
Encrypting data using AWS KMS (Key Management Service). 
AWS CloudTrail to audit data. 
Monitor data with the help of Amazon CloudWatch.   
Billing and cost management.  
Operations which need to be performed with the help of AWS Management Console and AWS CLI (Command Line Interface).  
Using file gateways
When a file gateway is used, the VM image is downloaded, and it is activated for file gateway via the AWS Management Console or through the Storage Gateway API. The file gateway needs to be activated. The file share needs to be created and configured to associate it with the Amazon S3 bucket. This ensures that share can be accessed by clients using NFS or SMB protocol. There is a one-to-one mapping between the files and the objects, wherein the gateway updates the objects in Amazon S3 asynchronously. Objects are encrypted with the help of Amazon S3–server-side encryption keys (SSE-S3) and data transfer takes place via HTTPS. 

The below image shows how file gateway works:

AWS Storage gateway

Volume Gateway
It helps provision cloud backed storage volumes which can be mounted as Internet Small Computer System Interface (iSCSI) devices form the on-premises application servers. It supports the below mentioned volume configurations:

Cached volumes – Data can be stored in Amazon Simple Storage Service (Amazon S3) and a copy of the frequently accessed datasets can be retained locally. Cached volumes help save costs when using primary storage, and also minimizes the need to scale the storage which is on premise. Low latency feature can be retained to access frequently required data.
Stored volumes – When low-latency is required to access the entire dataset, the on-premise gateway needs to be configured to store all the data locally. Then snapshots need to be asynchronously backed up at every point in time to Amazon S3. This configuration helps in provisioning durability and cost effectiveness so that data can be recovered to the user’s local data center or Amazon EC2.
Usage of Volume Gateways
Volume gateways can be used with cached volumes and stored volumes. Stored volume deployment has been shown in the below image: 

Primary data is stored locally, and this data is asynchronously backed up to AWS. It helps in provisioning low-latency access to the entire data as well as providing durable and offsite backups. Volume storages can be maintained on premise in the data center itself.

AWS Storage gateway

Cached volumes deployment has been shown in the below image. Frequently accessed data is stored locally in the storage gateway. Cached volumes help in minimizing the requirement to scale out based on the infrastructure, as well as providing low-latency access to data objects.

AWS Storage gateway

Tape Gateway
It can be used to cost efficiently and durably archive backup data in GLACIER or DEEP_ARCHIVE. It provides a virtual tape infrastructure that helps in seamlessly scaling the business requirements and eliminating the operational overhead of providing, scaling, and maintaining physical tape infrastructure.  

Tape gateway is used when the user requires a cost-effective, durable, long-term, and offsite way to archive data. It consists of a virtual tape library (VTL) interface that can be used with the existing tape-based backup software infrastructure in order to store data on virtual tape cartridges which is created by the user. When archive tapes are used, the overhead of managing these tapes on premise and provisioning shipments of tapes offsite is eliminated.  

AWS Storage gateway can be run on premise or as a VM appliance or as a hardware appliance or in AWS as an Amazon EC2 instance. The gateway is then deployed on an EC2 instance so as to provide the iSCSI storage volumes in AWS. These gateways can be hosted on EC2 instances that would help in disaster recovery, data mirroring, and providing storage for applications which are hosted on Amazon EC2. Before a storage gateway can be deployed, the storage solution and the hosting option must be decided upon.  

Tape gateway deployment has been shown in the image below: 

It provisions durable and cost-effective solution in archiving the data stored on AWS Cloud. With the help of its VTL (Virtual tape library) interface, the tape data can be backed up, which will be available on iSCSI devices.

AWS Storage gateway

Storage Solution 
It can be chosen from file gateway, volume gateway or tape gateway. 

File gateway: File gateway can be used to ingest files to Amazon S3 which can be used with object-based workloads, and to provide cost-effective storage for traditional backup applications. They can also be used to bind on premise file storage with S3. Data can be cost-effectively, and durably stored and retrieved from Amazon S3 with the help of industry-standard file protocols.  
Volume Gateway: Storage volumes can be created in the AWS Cloud. The on premise applications can be used to access the data in these volumes in the form of iSCSI targets. This can be done using cached and stored volumes. 
Cached volumes are used to store volume data in AWS that will have a portion of most recently accessed data in the on-premise cache. This method makes sure that the low-latency access feature is enabled to access the frequently accessed dataset. Seamless access to the entire dataset stored in AWS is provisioned. Storage resources can be scaled without having to provide additional hardware.  
Note: In stored volumes, the entire dataset/ data volume is stored on premise along with storing backups at certain points in time (also known as snapshots, which are useful in data backup and recovery). The data can be accessed with low latency.  

Hosting options in Storage Gateway 
Storage gateway can be run on premise as a VM appliance or as a hardware appliance or in AWS as an Amazon EC2 instance. If the data center where data is stored goes offline, and no host is available, the gateway can be deployed on an EC2 instance. Storage gateway provides AMI (Amazon Machine Image) which holds the gateway VM image.  

When a host is configured to be deployed to a gateway software appliance, sufficient storage needs to be allocated for the gateway VM.  

Note: When a tape gateway is deployed, the client backup software has to be installed.  

Pre-requisites 

When a gateway is deployed on premise, the type of host has to be chosen beforehand- VMware ESXi Hypervisor or Microsoft Hyper-V. This has to be set up based on the user requirements. If the gateway is deployed behind a firewall, it should be ensured that the ports can be accessed to the gateway VM.  


--------------------------------------------------------------------------------------------------------
o	S3 Bucket/Object Versioning and Lifecycle Policies
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Storage class
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Lab: 
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	Create an S3 bucket
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	Uploading files into S3
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	Uploading folders into S3
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	Accessing files
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	Block public access
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	Bucket permissions 
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Give permission to a user to access
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	Enable S3 bucket versioning
--------------------------------------------------------------------------------------------------------

What is the significance of AWS Versioning?
Versioning refers to maintaining multiple variants of an object in the same Amazon S3 bucket. It is used for the preservation, retrieving and restoration of every version of each object that is stored in the S3 bucket. Versioning can be used to recover from unintended user actions and application failure easily. An S3 bucket can be used to store multiple objects that have the same key, but they would have different version IDs.  

Version-enabled buckets help the user to recover objects in case they have been accidently deleted or have been overwritten.  

When an object is deleted, instead of removing it permanently, S3 inserts a delete marker into it, which becomes the current version of that object. The previous versions can always be restored.  
If an object is overwritten to, the object would have new data, which is considered as a new version of the object. Even in this case, the previous versions of the object can be restored.  
If the user has an object expiration lifecycle policy in their non-version enabled bucket, and they wish to maintain the same permanent delete behaviour when they enabled the versioning of the S3 bucket, the user will need to explicitly add a noncurrent expiration policy. The noncurrent expiration lifecycle policy will help in managing the deletes of the noncurrent object versions in the versioning enabled bucket.  

Versioning enabled buckets help in the maintenance of a single current object version and zero or more non-current object versions.  

An Amazon S3 bucket can be present in one of the 3 states which are mentioned below: 

Unversioned state (considered as the default state) 
Version enabled state  
Version suspended state 
Once a bucket has been enabled for versioning, it can never be made to move back to an unversioned state. But the same bucket can be moved to a suspended version state.  

When versioning is enabled on a bucket, it is applied to all objects in that bucket. It is never enabled on just a few objects in the bucket.  
When versioning is enabled on an S3 bucket for the first time, objects present in the bucket from then on will remain versioned. They will also be given a new and unique version ID.  
Objects that are stored in the bucket before version-enabling have a version ID of null. When versioning is enabled, the existing objects in the S3 bucket don’t change. The only change would be the way in which Amazon S3 would handle the objects when further requests come in.  
The owner of the bucket or any other user who has the appropriate permissions to access the bucket will have the ability to suspend the versioning so as to stop accruing the object versions.  
Configuring versioning on an S3 bucket 
S3 bucket can be versioning in the below mentioned methods: 

Using the Amazon S3 console. 
Programmatically versioning the buckets with the help of AWS SDKs. 
Using the console and SDK to call REST API which is provided by Amazon S3 for the purpose of managing versioning.  
Note: REST API calls can be made from the code as well, but this is a little complicated since it would involve writing code to authenticate user requests.

Every bucket created in Amazon S3 comes with a versioning sub-resource which is associated with it. The bucket is unversioned by default, and on the same lines, the sub-resource versioning stores empty versioning configuration as shown below:



To make sure that versioning is enabled, a request has to be sent to Amazon S3 along with a versioning configuration that has a ‘status’, as shown below:



In order to suspend versioning, this same ‘status’ value can be set to ‘Suspended’.  

The below mentioned users have the permissions to configure the state of versioning of the bucket: 

The bucket owner, 
The AWS account user who created this S3 bucket,  
The authorized users who have been given permissions. 
Note: The SOAP API doesn’t support versioning, and support of SOAP over HTTP has also been deprecated, but can be used with HTTPS. Amazon S3’s new feature doesn’t support SOAP services.  

When an object is PUT in the versioning-enabled bucket, the no current version doesn’t get overwritten. Instead, Amazon S3 generates a new version ID for the same object. This way, when a user accidentally overwrites or deletes an object, its previous version can be restored. The below image shows the same:

AWS Versioning
Image credit: aws.amazon.com

When an object is used with DELETE, all the versions of that object will remain in the bucket, and Amazon S3 just puts in a delete marker. This has been shown in the below image:

AWS Versioning
Image credit: aws.amazon.com

The delete marker is stored as the current version of that object. The GET request retrieves the most recently updated version by default. When GET request is sent to retrieve the most recent version of the object (which has a delete marker, i.e has been deleted), it returns a ‘404 Forbidden error’.It has been shown in the below image:

AWS Versioning
Image credit: aws.amazon.com

But it is possible to get the non-current version of the object with the help of the GET request, by specifying the version ID of the object. This has been shown in the image below:

AWS Versioning
Image credit: aws.amazon.com

An object can be permanently deleted from the S3 bucket by specifying its version. This can be done only if the user trying to delete the object is the owner of the bucket. When an object is permanently deleted, no delete marker is inserted into it. It has been shown in the below image:
AWS Versioning
Image credit: aws.amazon.com

Additional security can be provided by configuring the bucket to be MFA (Multi Factor Authentication) enabled. When this is enabled and an object from the bucket is tried to be deleted, the user who created the S3 bucket will have to include two authentication forms in the request to delete the object. This holds true even while changing the version state of the object.
--------------------------------------------------------------------------------------------------------
	Replication
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	Bucket sever side encryption
--------------------------------------------------------------------------------------------------------

Amazon Transfer Acceleration - Key features
It is a service facilitated by Amazed that helps in quick, easy and secure transfer of files between one client and an S3 bucket. The distance between the client and S3 is usually large. Transfer Acceleration leverages Amazon’s CloudFront’s globally distributed edge locations to transfer files quickly. When data arrives to an edge location, it is routed to the Amazon S3 service via an optimized network path. When Transfer acceleration service is used, extra charges are incurred by the user.  

Features of Amazon S3 Transfer Acceleration
Customer/User gets the ability to upload data to a centralized data bucket from anywhere in the world. 
Data transferred can range from anywhere between gigabytes to terabytes, and such data can be transferred on a regular basis across continents.  
When data is uploaded to S3, all of the available bandwidth present over the Internet can’t be utilized, hence Transfer Acceleration comes to play.  
Enabling transfer acceleration on a bucket
For the S3 bucket to work hand in hand with transfer acceleration, the bucket name must adhere to the DNS naming rules and should not contain periods (‘.’). 

Transfer acceleration can be enabled on an S3 bucket in any of the below ways: 

Go to the Amazon S3 Console, and click on the bucket name for which you wish to enable the transfer acceleration. Click on the ‘properties’ and choose ‘Transfer acceleration’. Choose ‘Enabled’ and click on ‘Save’.  
REST API can be used to enable transfer acceleration,  
AWS CLI and AWS SDK can be used to enable transfer acceleration.  
Requirements to use Transfer Acceleration
Transfer acceleration can be supported on virtual type of requests only.
The name of the S3 bucket used for transfer acceleration should comply with DNS rules and should not contain periods (‘.’).
Transfer acceleration enabled on a bucket takes about 20 minutes to reflect the speed during data transfer.
The endpoint name or the dual stack endpoint name has to be used to access the bucket which is enabled for transfer acceleration normally or over IPV6 respectively.  
The transfer acceleration on a bucket can be set by the user who is the owner of the bucket only. The bucket owner has the ability to grant permissions to other users so that they get the ability to grant permission to enable transfer acceleration on a S3 bucket.  
The GET operation can be implemented to return the transfer acceleration state of an S3 bucket. To implement GET, the user should have appropriate permissions and the transfer acceleration for that bucket has to be enabled, otherwise no state is returned.  

The PUT operation can be implemented to set the accelerate configuration for an existing S3 bucket. The permission to perform this operation is enabled by default for the user. This permission can be granted by the owner of the bucket to other users as well.  

The transfer acceleration state of a bucket can be enabled to be one of the two values mentioned below: 

Enabled: It enables data transfer with higher speeds to the bucket since transfer acceleration would have been enabled.
Suspended: It disables the transfer acceleration of data to S3 bucket.  
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------

Day 6

•	AWS's Domain Name System (Route 53)
--------------------------------------------------------------------------------------------------------
https://www.knowledgehut.com/tutorials/aws/amazon-route-53
Amazon Route 53 - How it works?
Amazon Route 53 is a DNS (Domain Name Server), which is high available and can be scaled according to the requirements. DNS is a distributed service which is present all over the world, that helps people use the internet. It uses hierarchical structure, thereby keeping a clear bifurcation between the levels. It is usually used to perform the below three operations: 

Domain registration
When the user’s website requires a name, route 53 can be used to provide a domain name for the website and register it. This is done by first searching for the domain name (make sure it hasn’t already been taken by others), register this domain name with route 53, provide details of the domain owner, and then Amazon (or Gandi, which is Amazon’s registrar associate) sends information regarding the domain to the registrar.  

This registrar sends the domain owner’s information to the domain’s registry. Registry is an organization which sells domain registrations to top-level domains, like .com. The registry further stores information regarding the domain in its own database.   

DNS routing
The route 53 helps connect the browser to the user's website or application. When a user sends a request to a website (domain or subdomain), this request needs to be routed to the right address to get the relevant response. This is taken care of by route 53.  

Configuring route 53 to route requests: 

The domain name is registered and once this is done, route 53 created a publicly hosted zone automatically. This zone has the same name as that of the domain name.  
The user creates ‘records’, which helps route traffic/requests from users to the user’s resources. These records are also known as ‘resource record sets’, which are present in the publicly hosted zone.  
Every record’s name present in the hosted zone should end with the hosted zone’s name, which is done automatically for the user by route 53.  
Every record has the below information, that helps route traffic to the specific domain:
Name: This corresponds to the domain or subdomain name; whose traffic needs to be routed with the help of route 53.
Type: It helps determine the type of resource which has to be used to route the traffic to. For example: Routing a traffic to a web server which has an IPV4 address, requires an A type, whereas routing traffic to an email server requires an MX Type.  
Value: This is similar to Type, which specifies the name of the resource to which the traffic is routed to.  
Routing traffic to a user domain: 

When the user enters a domain name, and clicks on enter, this request is routed to a DNS resolver, which is managed by the user’s ISP (Internet Service Provider).  
This DNS resolver forwards the request by the user for the specific domain name to a DNS root name server.  
It also forwards the user request to TLD name services (a .com domain). 
The name server for the domain responds to this request by providing 4 route 53 name servers, which are associated with that specific domain.  
The DNS resolver stores these four routes 53 name servers and behaves like a cache, so that if the same user or any other user requests for the same domain, it can be easily fetched from the cache, instead of resolving the domain name.  
This cache stores information for only 2 days, post which data is refreshed to store more recent name servers.  
The DNS resolver chooses one of the 4 name servers and this is forwarded to that name server (to which the user originally sent a request).  
This name server looks for the record in the hosted zone of the domain name, and fetches the value associated with it (IP address).  
This address is returned to the DNS resolver, which will have the IP address required by the user.  
It returns this to the web browser, which sends a request for the name associated with that IP address.  
This is the location where the content is stored, that the user needs.  
The resource (usually a web server) returns the web page associated with that IP address to the web browser so as to be shown to the user.  
Health checking
Route 53 is also responsible in sending automated requests to a resource through the internet. This is done by route 53 to make sure that the web server which is serving user requests is available, functioning properly and reachable. There is a facility in route 53 which allows enabling the notifications which inform the user when a specific resource fails or becomes unavailable. This way, the user can route the requests to other available resources.:

It can be created by specifying values which allow different kinds of health checks to be done. Below are a few of the values which could be specified:
IP address or domain name of the endpoint (the web server which has to be monitored by the Route 53). 
The request interval: This tells about the frequency of route 53 sending a request to the endpoint. 
Protocol: The set of rules which the user wants route 53 to use to perform the health check, whether it should be HTTP, HTTPS, or TCP. 
Failure threshold: The number of times that the endpoint fails to respond to the request continuously before it can be deemed unhealthy or unavailable by route 53. 
When an endpoint is unavailable or unhealthy, it is detected by route 53. Notifications can be enabled to notify the same. This is done by setting a CloudWatch alarm (which is done by route 53 automatically). Amazon SNS is used by CloudWatch to notify users about an endpoint’s health.
Note: The status of other health checks can also be monitored.  
A health check interval is provided so that at this interval of time, route 53 sends request to the endpoint to perform a health check.  
When the endpoint responds to the request sent by route 53, it is considered as a healthy endpoint, and no further action is taken.  
If the endpoint doesn’t respond to a request, route 53 starts sending consecutive requests and keeps a count of the number of requests which are continuously sent to the endpoint:
A failure threshold is pre-defined and when the count reaches the failure threshold, route 53 considers such an endpoint to be unhealthy.  
If the endpoint responds before the count reaches the failure threshold, the count is reset to 0 by route 53.  
If the endpoint is deemed unhealthy by route 53, this is notified to CloudWatch by route 53 (if it was configured to provide a notification when the endpoint is unhealthy).
If the configuration to provide a notification wasn’t done, the status of the route 53 can be still be checked in the route 53’s console.  
If the notification is configured for a health check, CloudWatch triggers an alarm to indicate about an endpoint not being healthy, with the help of Amazon SNS, and sends it to the respective user.
Note: In addition to this, the health of a health check can also be monitored, and configured to provide notifications. The status of a CloudWatch alarm can also be monitored, to keep a tab on a wide range of specifications, not just a resource responding/not responding to requests
--------------------------------------------------------------------------------------------------------
o	Amazon Route 53 Overview
--------------------------------------------------------------------------------------------------------



Day 7

•	Security	 (Security Group, )
--------------------------------------------------------------------------------------------------------

https://www.knowledgehut.com/tutorials/aws/cloud-security
AWS places cloud security as its highest priority. The user data stored in the data center and the network architecture associated with it is built in the most security-centric way. One of the major advantages of using AWS cloud is scalability. It allows users to scale their applications while maintaining a secure environment around it. The customers pay only for services and resources that they use, and security is not an upfront expense.  

AWS provides flexibility and agility in all the security control associated with the application. AWS Cloud uses a shared responsibility model. The security of the cloud is managed by AWS, but the security of user data inside the cloud is the responsibility of the user. This means the user has the control to secure their content by implementing control specifications on content, platform, applications, systems and networks.  

AWS also provisions guidance and expertise with the help of online resources, personnel and partners. It provisions advisories for current issues of users, as well as provides opportunities to work with AWS when users come across security issues. A wide variety of tools and features are present in Amazon that help the consumers meet their security goals. It provides security specific tools and features across areas such as network security, configuration management, access control, and data encryption.  

AWS environments are audited continuously and certified from accreditation bodies across multiple geographies and verticals. The user can leverage the automated tools present in the AWS environment which can be used for asset inventory and privileged access reporting.  

Features of AWS security:
Data Security: AWS infrastructure safeguards user data to help protect the user’s privacy. The data is stored in highly secure AWS data centers.  
Compliance requirements are met: AWS manages multiple compliance programs with respect to their infrastructure. This means the user specific segment’s compliance is already complete and up-to-date.  
Cost-effective: Users only pay for the storage, resources, and service that they use. The highest security standard is maintained even though the user doesn’t manage or explicitly pay for such a facility.  
Scalability: Security is scalable, which means it scales out based on the user’s requirements and cloud usage. Irrespective of the size of the application, AWS infrastructure has been designed to keep the user data safe.  
Cloud security consists of three parts: 

Data monitoring 
Gaining visibility 
Access Management 
The ‘Cloud Monitoring’ tool is used to constantly analyse the data that flows into the cloud application. As soon as something irregular happens, or a threshold is reached or a condition is met, the tool notifies the user about these irregularities. This can be understood as ‘Gaining visibility’ into the irregular situation. This can be done with the help of tools that are present in AWS.  

The user can either take action or configure the system in such a way that the cloud application automatically takes action when it encounters irregularities. The Cloud Monitoring tool also has advanced machine learning application which logs in the data flow.  

In AWS cloud, data monitoring is done with the help of AWS CloudWatch.  
Gaining visibility is handled with the help of AWS CloudTrail.  
Access management is taken care of by AWS IAM.   
Now let us look at every service one at a time: 

CloudWatch Monitoring tool
CloudWatch Monitoring is a service offered by Amazon that helps monitor the AWS resources and the user applications which run on AWS in real-time. It can also be used to gather and keep a track on the metrics, where metrics refers to variables which can be measured for the resources and applications that the user uses.  

CloudWatch page automatically displays the metrics with respect to every AWS service which the user is associated with. Users are also able to create customized dashboards that help display metrics about specific applications. They can also be used to display a customized collection of metrics which are chosen by the user.  

Alarms can be created by the user that help in monitoring the metrics and sending notifications regarding the state of the metrics. These alarms can also be used to automatically make certain changes to the AWS resources which are being monitored by the user when a certain condition is met or a threshold is reached.  

An example would be monitoring the usage of CPU and disk reads and writes of the Amazon EC2 instance.  

This data can be used to understand whether additional instances need to be launched to handle the increase in load.  
The same data can also be used to stop instances which are not being used, and thereby helps in saving costs.  
CloudWatch helps provide system-wide visibility into the utilization of resources, how the application performs and the health of the operations which take place in the system.

CloudTrail to gain visibility
It can be used to monitor calls that are made to Amazon CloudWatch API for that specific user’s account. These calls include the ones made by AWS Management Console, AWS CLI and other AWS services. When the CloudTrail logging is enabled, CloudWatch can be used to write the log file to an Amazon S3 bucket which is specified by the user when CloudTrail is configured. It helps enable governance, compliance and operational and risk auditing of the user’s AWS account. Actions performed by a user, a role or an AWS service is record as an event in CloudTrail, and these events also include actions which have been taken in the AWS Management Console, AWS CLI, AWS SDK and AWS APIs.  

When an AWS account is created, CloudTrail is automatically enabled on it. Whenever an activity occurs in this user account, it is recorded as a CloudTrail event. The events can be viewed by visiting the CloudTrail console-> Event history tab.

CloudTrail can be used to view, search, download, archive, analyse and respond to activities that take place across the AWS infrastructure. The person or the operation that took action can be identified, the resources upon which changes were made can be identified, and the event which occurred can also be identified. These details allow in the analysis of the activities in the AWS account.  

To record an ongoing event in the AWS account, a trail can be created. A trail allows the CloudTrail service to provide log files to an Amazon S3 bucket. When a trail is created, it will be applied across all AWS Regions.  

IAM to manage access
AWS IAM (Identity and Access Management) is a web service facilitated by Amazon that helps the user in securely controlling access to the resources of AWS. IAM can be used to control who gets authenticated to sign-in and who gets the authorization (has permissions) to use the resources provisioned by AWS.  

An IAM role can be created with the help of AWS Management Console, AWS CLI, Tools for Windows PowerShell or IAM API.

If AWS Management Console is used to create an IAM role, a wizard guides the user through the entire steps. The steps while using the Console are slightly different in comparison to other methods.  

IAM role is similar to user, since it is an identity which provides permission policies which are used to determine the operations which the identity can do within AWS. A role doesn’t have any credentials, such as password or access keys attached to it. It can be uniquely associated with a user since a role is basically used to be assumed by a user for a specific task or when need.  

An IAM user can assume a specific role to be granted specific permissions to perform specific tasks. A role can be assigned to a federated user who can sign in with the help of an external identity provider instead of using IAM. AWS uses the details passed via the identity provider so as to determine the role that can be mapped to the federated user.  

An application that runs on Amazon Elastic Cloud Compute (EC2) instance and makes requests to AWS: An IAM role can be created that can be attached to the EC2 instance so as to provide temporary security credentials to the applications which run on this instance. When an application uses these credentials in AWS, it gets the ability to perform all operations that would be allowed by the policies which are attached to the role.  

An application that runs on mobile and makes requests to AWS: An identity provider such as Login with Amazon, or Amazon Cognito or Facebook or Google can be used to authenticate users and map them to an IAM role. Applications can be used to provide the role with temporary security credentials which have the required permissions (based on the policies that are attached to the role).  

IAM is a web service which helps in securely controlling access to AWS resources of the users. IAM can be used to control which users use the account holder’s AWS resources (known as authentication) and which resources could be used in which ways (known as authorization).  

AWS IAM identities
IAM identities are created to provide authentication to people and process in the AWS accounts.  

IAM root user
When an AWS account is created for the first time, the user signs in with a single identity which has access to all the AWS services and resources in the account. This identity is known as ‘AWS account root user’. This root user can be accessed by signing in with the email address and password that was used while creation of the account.  

The root user is not recommended to be used for everyday tasks, not even the administrative ones. The root user has to be securely locked away and used only to perform specific account and service management related tasks.  

IAM user
The entity which is created in the AWS account is an IAM user, which represents a person or a service that uses IAM user to interact with AWS. One of the uses of IAM users is to provide people the ability to sign into their AWS Management Console to perform interactive tasks and programmatic requests to other Amazon services with the help of an API or using the CLI.  

A user has a name, a password that is used to sign into the AWS Management Console and about 2 access keys which can be used in conjunction with the API or CLI. When an IAM user is created, it can be granted certain permissions after it has been made as a member if a group which has the appropriate permission policies attached to it, or by directly attaching the policies to the user.  

Permissions can be cloned from an existing IAM user which will automatically make the new user as a member of the same group, and attaches the policies to the user.  

IAM group
It is a collection of IAM users, which is made to specify permissions for a specific collection of users. This makes it easy to manage the permissions for those users. If a permission is assigned to a group, any user of that group automatically has the same permissions.  

AWS IAM role use cases 
IAM allows the user to manage access to the Amazon services and resources in a secure method. With the help of IAM, the user can create and manage AWS users and groups, provide permissions to allow and deny access to AWS resources. IAM feature can be used with AWS account at no additional cost. The charges are incurred only when Amazon services are used by the users.  

Granular access control to AWS resources
IAM allows users to control access to AWS service APIs and to certain resources. It also allows the user to plug in specific conditions such as time of the day when the user can control AWS, based on their originating OP address, whether they use SSL, whether they authenticate with the help of an MFA (Multi-factor authentication) device.   

MFA for highly privileged users 
An AWS environment can be protected with the help of AWS MFA, which is a security feature that is enabled at no extra cost. This feature augments the username and password credentials. MFA expects the user to prove that they have the physical possession of a hardware MFA token or an MFA enabled mobile device. This can be proven by the user when they provide a valid MFA code.  

Management of access control for mobile applications with Web Identity Providers 
The user can enable mobile and browser-based applications to securely access the AWS resource. This can be done by requesting for temporary security credentials that can be used to provide access to certain specific AWS resources for a specific period of time (which is also configured beforehand).  

Integration with the corporate directory of the user 
IAM can be used to grant specific permissions to the employees and applications. This is known as federated access which is provided to access the AWS Management Console and Amazon service APIS with the help of the user’s existing identity system such as Microsoft Active Directory. Any Identity management solution can be used that comes with support for SAML 2.0. 
--------------------------------------------------------------------------------------------------------


Why Monitoring??
	• We know how to deploy applications
	• Safely
	• Automatically
	• Using Infrastructure as Code
	• Leveraging the best AWS components!
	• Our applications are deployed, and our users don’t care how we did it…
	• Our users only care that the application is working!
	• Application latency: will it increase over time?
	• Application outages: customer experience should not be degraded
	• Users contacting the IT department or complaining is not a good outcome
	• Troubleshooting and remediation
	• Internal monitoring:
	• Can we prevent issues before they happen?
	• Performance and Cost
	• Trends (scaling patterns)
	• Learning and Improvement


• AWS CloudWatch:
	• Metrics: Collect and track key metrics
	• Logs: Collect, monitor, analyze and store log files
	• Events: Send notifications when certain events happen in your AWS
	• Alarms: React in real-time to metrics / events
• AWS X-Ray:
	• Troubleshooting application performance and errors
	• Distributed tracing of microservices
• AWS CloudTrail:
	• Internal monitoring of API calls being made
	• Audit changes to AWS Resources by your users



AWS CloudWatch Metrics
	• CloudWatch provides metrics for every services in AWS
	• Metric is a variable to monitor (CPUUtilization, NetworkIn…)
	• Metrics belong to namespaces
	• Dimension is an attribute of a metric (instance id, environment, etc…).
	• Up to 30 dimensions per metric
	• Metrics have timestamps
	• Can create CloudWatch dashboards of metrics



EC2 Detailed monitoring
	• EC2 instance metrics have metrics “every 5 minutes”
	• With detailed monitoring (for a cost), you get data “every 1 minute”
	• Use detailed monitoring if you want to scale faster for your ASG!
	• The AWS Free Tier allows us to have 10 detailed monitoring metrics
	• Note: EC2 Memory usage is by default not pushed (must be pushed from inside the instance as a custom metric)


CloudWatch Custom Metrics
	• Possibility to define and send your own custom metrics to CloudWatch
	• Example: memory (RAM) usage, disk space, number of logged in users …
	• Use API call PutMetricData
	• Ability to use dimensions (attributes) to segment metrics
	• Instance.id
	• Environment.name
	• Metric resolution (StorageResolution API parameter – two possible value):
	• Standard: 1 minute (60 seconds)
	• High Resolution: 1/5/10/30 second(s) – Higher cost
	• Important: Accepts metric data points two weeks in the past and two hours in the
	future (make sure to configure your EC2 instance time correctly)

CloudWatch Logs
	• Log groups: arbitrary name, usually representing an application
	• Log stream: instances within application / log files / containers
	• Can define log expiration policies (never expire, 30 days, etc..)
	• CloudWatch Logs can send logs to:
	• Amazon S3 (exports)
	• Kinesis Data Streams
	• Kinesis Data Firehose
	• AWS Lambda
	• OpenSearch


CloudWatch Logs Metric Filter & Insights
• CloudWatch Logs can use filter expressions
• For example, find a specific IP inside of a log
• Or count occurrences of “ERROR” in your logs
• Metric filters can be used to trigger CloudWatch alarms
• CloudWatch Logs Insights can be used to query logs and add queries to
CloudWatch Dashboards










-	Introduction to CloudWatch
--------------------------------------------------------------------------------------------------------
https://www.knowledgehut.com/tutorials/aws/aws-cloudwatch-monitoring
How does CloudWatch works?
CloudWatch Monitoring is a service offered by Amazon that helps monitor the AWS resources and the user applications which run on AWS in real-time. It can also be used to gather and keep a track on the metrics, where metrics refers to variables which can be measured for the resources and applications that the user uses.  

CloudWatch page automatically displays the metrics with respect to every AWS service which the user is associated with. Users are also able to create customized dashboards that help display metrics about specific applications. They can also be used to display a customized collection of metrics which are chosen by the user.  

Alarms can be created by the user that help in monitoring the metrics and sending notifications regarding the state of the metrics. These alarms can also be used to automatically make certain changes to the AWS resources which are being monitored by the user when a certain condition is met or a threshold is reached.  

An example would be monitoring the usage of CPU and disk reads and writes of the Amazon EC2 instance.

This data can be used to understand whether additional instances need to be launched to handle the increase in load.  
The same data can also be used to stop instances which are not being used, and thereby helps in saving costs.  
CloudWatch helps provide system-wide visibility into the utilization of resources, how the application performs and the health of the operations which take place in the system.

Access to CloudWatch
CloudWatch can be accessed in the below mentioned ways:

Amazon CloudWatch console 
AWS CLI (Command Line Interface) 
CloudWatch API 
AWS SDK 
Services which can be used with Amazon CloudWatch: 

Amazon Simple Notification Service (SNS)
It is used to coordinate and manage the delivery of sending messages to subscribing endpoints or the clients. This service can be integrated with CloudWatch to send messages to the user when an alarm threshold is reached or a condition is met.  

Amazon EC2 auto-scaling
Auto-scaling helps the user to automatically launch or remove an EC2 instance which is based on certain policies that are defined by the user beforehand, or based on health status of the instance, and schedules. CloudWatch can be integrated with Amazon EC2 to scale the EC2 instance based on the requirements.   

CloudTrail
It can be used to monitor calls that are made to Amazon CloudWatch API for that specific user’s account. These calls include the ones made by AWS Management Console, AWS CLI and other AWS services. When the CloudTrail logging is enabled, CloudWatch can be used to write the log file to an Amazon S3 bucket which is specified by the user when CloudTrail is configured.  

AWS IAM (Identity and Access Management)
IAM is a web service which helps in securely controlling access to AWS resources of the users. IAM can be used to control which users use the account holder’s AWS resources (known as authentication) and which resources could be used in which ways (known as authorization).  

Working of CloudWatch
It can be understood as a repository which is used to store and monitor the metrics. An Amazon service like EC2 can be used to store these metrics into a repository, retrieve the insights based on these metrics and take certain actions (if configured to do so). The same can be done on customized metrics as well.  

This data can be used to extract insights, calculate statistics, and present this data visually in the CloudWatch console.  

Below is an image that shows how CloudWatch works:

AWS CloudWatch Monitoring

CloudWatch doesn’t aggregate any data based on ‘regions’. Hence these metrics are separated between these regions. Region refers to a specific geographical area, wherein a data center is facilitated so that data is highly-available, scalable, and stable and can help achieve maximum failure isolation. Failure isolation is maximized so that when a single component fails, other components don’t break down too. 

CloudWatch homepage
The below image shows the CloudWatch overview homepage. The components on this page have been discussed below:

AWS CloudWatch Monitoring

The upper left portion of the image lists the AWS services which are present in that user account. It also shows the state of alarms present within every Amazon service.  

The upper right part also shows two or four alarms in the user account, and this number depends on the number of services which the user uses. These are the alarms which are currently in the ALARM state or ones which have changed their state in the recent times.  

The upper area of the image allows the user to access the health state of the AWS services. This is done by observing the state of the alarm of every service and noting which alarm has recently changed its state.  

This way, alarms can be monitored easily and issues can be diagnosed quickly.  Just below this is the ‘default dashboard’, if it exists. It is a custom dashboard that is generated by the user, and is named as ‘CloudWatch-Default’.  

This is the simplest way to add customized metrics to the overview page about customized services/applications. This default dashboard can also be used to bring forth the additional key metrics from other AWS services which need to be monitored by the user.  

If the user uses less than 6 Amazon services, another dashboard known as cross-service dashboard is shown automatically on the same page.  

If the user is using more than 6 Amazon services, there is a part below the default dashboard which is a link to the automatic cross-service dashboard. This dashboard helps in automatically displaying key metrics from every Amazon service which the user uses. This doesn’t require the user to choose which metrics need to be monitored (thereby avoiding the creation of another customized dashboard) 

This overview helps the user to focus on a specific resource group or an Amazon service. This way, the view of monitoring the services can be narrowed down to a subset of all the resources. Usage of resource groups helps the user to use tags to organize the projects and focus on specific subsets of this architecture (or even distinguishing between development and production environments), hence improving the efficiency and saving user’s time.  


--------------------------------------------------------------------------------------------------------
o	AWS EC2
--------------------------------------------------------------------------------------------------------


What are the different terminologies used in AWS EC2?
EC2 stands for elastic compute cloud, which provides easy to scale computing capability to the instances of the Amazon services. EC2 provides all the hardware resources needed to run the service, thereby eliminating a portion of the cost which has to be investing in purchasing the hardware. The focus would be on developing and deploying the applications which are created, in a much efficient manner quickly.

EC2 can be used to launch multiple servers depending on the data. It also helps manage security aspects as well as network part of the service. It helps efficiently store data and allows scaling up or down the resources as and when the requirement arises.

Terminologies used in AWS EC2
AMI: Also known as Amazon Machine Pictures. It is a template that houses the software configuration like an operating system, an application, or a server.
Instances: An object that is created within a service which can be used to serve other requests.
Region and availability zones: EC2 is hosted in multiple locations all over the world. These locations consist of a variety of regions and availability zones.
Amazon EC2 key-pairs: EC2 uses public keys which help in encrypting and decrypting the login information.
Security groups: They help associate an EC2 instance with a group, thereby providing security to the instance at the protocol level as well as port access level.
VPC: Virtual Private Cloud, that allows to provide an isolated section in the Amazon Cloud, so that an AWS resource can be launched in a virtual network which is defined by the user.
Features of AWS EC2
Once an instance is terminated or stopped, it is deleted thereby saving the storage space. This is known as ‘instance store volumes’.
It provides isolated virtual computing environments, which can be used to launch separate instances of a service.
Comes with pre-configured AMIs, which are templates that is required by the server.
EBS provides persistent storage volumes for the data which is stored by the user, with the help of Elastic Block Store.
Data of instances, resources, EBS volumes can be stored in multiple locations so as to deliver high-availability. These locations are known as ‘regions’ and ‘availability zones’.
The elastic IP address is a static IPV4 address that is provided for dynamic cloud computing.
It comes with instance varieties, which offers a wide range of options to choose from, in case of hardware, memory, and networking ability.
It has a Virtual Private Network that allows networks to virtually connect to the user’s network.
Amazon EC2 storage
The Amazon EC2 storage consists of two kinds of storage:

Amazon EBS
EBS helps in the storage of data with the help of block level storage. Block level storage is just like a hard drive, but the only change is that it is present in the cloud. It is used in conjunction with Amazon EC2 instances.

These block level storages can be mounted as devices on EC2 instances. These storages can be stacked on top of each other (like EBS volumes) and a file system can be created on top. This setup can be used to manage and run a database, server or utilize it for different requirements like a block device/hard drive.

Instance store
This consists of one or more blocks that belong to the same type. Based on the type of the instance, the size of these instance stores and the number of devices that are available can be changed.
--------------------------------------------------------------------------------------------------------
	Monitoring and alerting of AWS EC2 instances
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	AWS RDS
--------------------------------------------------------------------------------------------------------
	https://catalog.workshops.aws/general-immersionday/en-US/basic-modules/50-rds
	https://catalog.workshops.aws/general-immersionday/en-US/advanced-modules/database
	
	
--------------------------------------------------------------------------------------------------------
	Monitoring AWS RDS
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Working with Amazon CloudFront
--------------------------------------------------------------------------------------------------------

Does AWS CloudFront provide better user request?
CloudFront is an Amazon web service offering that helps in improving the speed of distribution of static and dynamic web content, which includes .html, .css, .js, and image files to the users. CloudFront helps in delivering user content via a worldwide network of data centres known as ‘edge locations’. When a request comes in from the user which is being served by CloudFront, the user gets routed to an edge location which provisions lowest latency so as to deliver the content requested with best performance.  

If the content requested is already present in the ‘edge location’ with lowest latency, CloudFront delivers it immediately.  

If the content requested is not present in the edge location, CloudFront makes it a point to retrieve it from the origin which was defined by the user (like an Amazon S3 bucket, a MediaPackage channel or a HTTP server) and deliver it to the requested source.  

CloudFront speeds up the process of distributing the content requested by user by routing every request via the AWS backbone network to the edge location which can serve the content in the optimal way. This is achieved with the help of a CloudFront edge server which provisions quick delivery of content to the viewer. This is done with the help of AWS network since it dynamically reduces the number of networks through which the user’s request needs to pass through, thereby improving the performance. This way low latency and higher data transfer rates are achieved.  

The reliability and availability also improve since the copies of the files (known as objects) would be held (cached) in multiple edge locations all over the world.  

Set up CloudFront to deliver content
A CloudFront distribution can be created to inform CloudFront about the location where the content needs to be delivered from, in addition to details regarding tracking and managing the delivery of the content. CloudFront uses edge servers which are located near to the viewer to deliver content quickly when a request comes in.  

Configuring CloudFront to deliver content to the requested user
Below are the steps that need to be followed to deliver content to user with the help of CloudFront: 

The ‘origin servers’ has to be specified, similar to an Amazon S3 bucket or a HTTP server. From this location, the CloudFront gets the users files, which will further be distributed from the CloudFront to edge locations all over the world.   

An origin server stores the original and defined version of the user objects. If the content is being served over HTTP, the origin server can either be an Amazon S3 bucket or a HTTP server such as a web server. The HTTP server can be run on an Amazon Elastic Compute Cloud (EC2) instance or a server which is managed by the user. These servers are also known as ‘custom origins’.  

If Adobe Media Server RMTP protocol is used to distribute the media files based on the requirement, the origin server will always be an Amazon S3 bucket.  

The files can be uploaded to the origin servers. These files uploaded by users are also known as objects, which can be web pages, images, or media files, or anything that can be served over a HTTP protocol or a supported version of Adobe RMTP, which is the protocol used by Adobe Flash Media Server.  

If the user is using an Amazon S3 bucket as the origin server, the user can convert the objects in the S3 bucket to be publicly readable, so that any person who has the CloudFront URLs for the objects is able to access them. The objects can be kept private and rules can be defined to control who has access to these objects.  

A CloudFront distribution can be created, which explains to the CloudFront about which origin server needs to be communicated to, that has the ability to get the files requested by the user via a website or an application.  

The user also specifies details regarding whether or not they want CloudFront to log in all requests and whether or not they want the distribution to be enabled right after it has been created.  

A domain name is created by CloudFront for the new distribution which can be viewed in the CloudFront console or which is returned as a response to the programmatic request sent by user in the form of an API request. The user can alternatively add another domain name as well.  

The distribution’s configuration is sent by CloudFront to all of its edge locations or points of presence (POPs). This configuration doesn’t include the content of data. This POP is a collection of servers which is present in many geographically diverse data centres where CloudFront caches the copies of the user files. The amount of time for which these cached files stay in the CloudFront edge location can be configured. By default, every file stay in an edge location for 24 hours before it gets expired. The minimum expiration time is 0 seconds, but there is no limit on the maximum expiration time.  

It can used via the AWS Management Console, an API, AWS CLI, AWS Tools for Windows PowerShell or AWS SDKs.  
--------------------------------------------------------------------------------------------------------
	analytics along with telemetric
--------------------------------------------------------------------------------------------------------
https://www.knowledgehut.com/tutorials/aws/aws-analytics
What are AWS Analytics Services?
It is a service offered by Amazon that helps in processing and providing insights to data by analysing it. There are many services in the analytics section which have been discussed below:

Amazon Athena
An interactive query service which helps in easy analysis of data stored in S3 with the help of SQL queries. It is server-less, hence having no infrastructure that needs to be managed. The user only pays for the queries they run.

Amazon EMR
It provides a Hadoop managed framework, which helps in quick and cost-effective method of processing large amounts of data across on-the-g- scalable EC2 instances. EMR notebooks are similar to Jupyternotebooks, and provide an environment to develop and collaborate for dynamic querying and exploratory analysis if data. It is highly-secure and offers reliability to handle a wide range of uses cases, which include, but are not limited to machine learning, scientific simulation, log analysis, web indexing, data transformations (ETL), and bioinformatics.  

Amazon CloudSearch 
It is a service managed by AWS Cloud, and it makes the process of setting-up, managing and scaling a search solution for a website or an application an easy task. It supports 34 languages and comes with features such as highlighting, auto-complete, and geospatial searchability.  

Amazon ElasticSearch service 
It helps ElasticSearch in the process of deploying, securing, operating and scaling data by searching, analysing and visualizing this data in real-time. It comes with easy-to-use APIs which can be used for log analysis, full-text search, scaling requirements, monitoring applications, clickstream analytics, and security. It can also be integrated with open-source tools such as Kibana, Logstash which help in ingesting data and visualization of data respectively.  

Amazon Kinesis 
It makes the process of collecting, processing, and analysing real-time data an easy task. This helps in achieving real-time insights to data so that this data can be analysed and actions can be taken based on the insights quickly. Real-time data including video, audio, application logs, website clickstreams, IoT telemetry data (meant for machine learning) can be ingested and this data can be responded to immediately in contrast to waiting for all the data to arrive before beginning pre-processing on it.  It currently offers services like- Kinesis Data Firehose, Kinesis Data Analytics, Kinesis Data Streams, and Kinesis Video Streams.  

Amazon redshift
It is a quick, and scalable data warehouse which helps in analysing user data in a cost-efficient and simple manner. It delivers 10 times quicker performance in comparison to other data warehousing services, since it uses machine learning, massively parallel query execution and columnar storage on high-performance disks. Petabytes of data can be queried upon, and a new data warehouse can be setup and deployed in a matter of minutes.  

Amazon Quicksight
It is a cloud powered business intelligence service, which is fast, and helps in delivery insights to people in an organization. It allows users to create and publish interactive visual dashboards which can be accessed via mobile devices and browsers. These visual dashboards can be integrated with other applications thereby providing customers with powerful self-serving analysis service.  

AWS data pipeline
It helps process and move data between different AWS resources (compute and storage devices). Data can be regularly accessed from the place it is stored, it can be transformed and processed at scale. The result of this data processing can be transferred to other AWS services such as S3, RDS, DynamoDB, and EMR. It helps in the creation of complex data processing workloads which provide facilities such as high fault tolerance, high availability and repeatability.  

AWS Glue
It is a completely managed ETL service (Extract, Transform, Load) which helps users prepare and load their data for the purpose of analysis. An ETL job can be set up and run with a few mouse clicks from the AWS Management Console itself. Glue can be pointed to the location of data stored, and it discovers the data and its metadata and stores it in Glue Catalog. Once the data is in the catalog, it can be searched, queried, and made available for ETL process.  

Amazon Lake Formation
It is a service that helps in securing data lake. Data Lake can be visualised as a centralized, customized and secured data repository which stores this data in the original form as well as a processed form meant for data analysis. It helps combine various types of analytics that help in gaining deeper insights to data thereby helping make better business decisions. But the process of setting up and managing a data lake requires a lot of manual efforts. But Lake Formation makes this process an easy one by collecting data and cataloguing it. This data is then classified using ML algorithms as well as providing security for sensitive data.  

Amazon Managed streaming for Kafka (MSK)
It is a service that helps in building and running applications that use Apache Kafka. It is fully managed and helps process streaming data. Apache Kafka is open-source and is used to build real-time streaming data pipelines and application. With the help of MSK, Kafka API can be used to populate data lakes, reflect changes in the database and use machine learning to power other applications.  






Telemetrics
--------------------------------------------------------------------------------------------------------


Day 8 

•	EKS Cluster
--------------------------------------------------------------------------------------------------------
Amazon Elastic Kubernetes Service (Amazon EKS) 
	fully managed service 
	can 
		create Kubernetes cluster to 
			deploy
			manage
			scale containerized applications 
	use existing Kubernetes tools 

Key features of Amazon EKS:
	Fully managed: 
		AWS 
			manages 
				Kubernetes control plane 
				underlying infrastructure.
	Scalable: 
		designed to scale 
		easily add or remove nodes to your cluster as needed.
	High availability: 
		automatically deploys 
			Kubernetes control plane 
				across multiple Availability Zones 
					to ensure high availability.
	Secure: 
		designed to be secure by default
			automatic software updates
			private networking
			IAM integration.
	Integrated with AWS: 
		Amazon EKS integrates with other AWS services 
			Elastic Load Balancing
			Amazon Elastic File System
			Amazon CloudWatch.
	Kubernetes version support: 
		latest stable Kubernetes version 
		rolling updates for Kubernetes versions
	Multi-tenant clusters: 
		run multiple applications and teams on a single cluster.
	Role-based access control (RBAC): 
		integrated with AWS IAM.
	Logging and monitoring: 
		integrates with Amazon CloudWatch 
	Load balancing: 
		integrates with Elastic Load Balancing 
			load balancing of your Kubernetes applications.
	Networking: 
		supports various networking options like 
			Amazo n VPC
			Amazon CNI
			Calico
	Auto scaling: 
		integrates with Amazon EC2 Auto Scaling 
		automatically scale your worker nodes based on demand.
	Hybrid cloud support: 
		supports hybrid cloud deployments
		run Kubernetes workloads in both
			on-premises data centers and 
			AWS.
	Integrations: 
		integrates with various AWS services like
			Amazon ECR
			Amazon S3
			Amazon RDS
			Amazon DynamoDB.
	Kubernetes add-ons: 
		Helm
		Istio
		Prometheus
			extend the functionality of your Kubernetes cluster.


create an EKS cluster. 
	create a VPC
	subnets
	security groups
	configuring IAM roles and policies. 
	Once your cluster is created
		deploy your containerized applications using Kubernetes manifests
		Helm charts
		deployment tools.

Amazon EKS supports 
	different types of worker nodes
		Amazon EC2 instances and 
		AWS Fargate
--------------------------------------------------------------------------------------------------------
o	Understanding the orchestration platforms provided by AWS
--------------------------------------------------------------------------------------------------------

AWS provides 
	several container orchestration platforms:
		ECS
		EKS
		Fargate

Amazon Elastic Container Service (ECS): 
	fully managed container orchestration service 
	supports Docker containers. 
	highly scalable and highly available 
		to deploy and managing Docker containers.

Amazon Elastic Kubernetes Service (EKS): 
	fully managed Kubernetes service 
	run Kubernetes on AWS 
		don't need to manage the underlying infrastructure. 
	EKS provides 
		highly available and highly scalable 
			Kubernetes clusters.

AWS Fargate: 
	serverless compute engine 
		run containers without having to manage the underlying infrastructure. 
		integrates with 
			ECS and EKS.
--------------------------------------------------------------------------------------------------------
	Overview of the different options
--------------------------------------------------------------------------------------------------------
Amazon Elastic Container Service (ECS)
	fully-managed container orchestration service 
		by Amazon Web Services (AWS). 
	
	easily run, scale and manage Docker containers 
	we don't manage 	
		underlying infrastructure, 
		focus on 
			developing and 
			deploying 
				containerized applications.

Key features of Amazon ECS include:
	Easy Container Management: 
		simplifies the management of containers 
			through APIs and command line tools 
			manage the complete lifecycle of containers
			including 
				deploying, 
				starting, 
				stopping
				terminating tasks.
	Scalability: 
		built-in 
			auto scaling capabilities 
			automatically scale container instances in response to changes in demand.
	High Availability: 
		Amazon ECS provides built-in load balancing and automatic failover features that ensure high availability and fault tolerance.
	Security: 
		built-in security 
		manage container access and control network traffic.
	Integration with other AWS Services: 
		integrates with other AWS services like 
			Elastic Load Balancing
			Amazon EC2
			AWS Identity and 
			Access Management (IAM), and 
			Amazon CloudWatch.


	Create a cluster 
	define task definitions and services 
		define Docker container to.
	AWS Compute SLA : 99.99% for Amazon ECS.
	Amazon ECS Exec
		cli to execute commands for
			Amazon EC2 instances or 
			AWS Fargate.  
		interactive shell or single command access to a running container.

	https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/#features
Components of ECS
-----------------
	Containers and Images
		Define containers/images
		All docker standards apply
		Docker Volumes 
			can be 
				a local instance store volume, 
				EBS volume, 
				EFS volume. 
			Connect your Docker containers to these volumes 
				using Docker drivers and plugins.
	Task Components
		Task definitions 
			define parameters for your application. 
			a text file
				in JSON format
				describes one or more containers
				a maximum of ten
					form your application.
		Task definitions are split into separate parts:
		
		Task family – the name of the task, and each family can have multiple revisions.
		IAM task role – specifies the permissions that containers in the task should have.
		Network mode – determines how the networking is configured for your containers.
		Container definitions – specify which image to use, how much CPU and memory the container is allocated, and many more options.
		Volumes – allow you to share data between containers and even persist the data on the container instance when the containers are no longer running.
		Task placement constraints – lets you customize how your tasks are placed within the infrastructure.
		Launch types – determines which infrastructure your tasks use.
		Tasks and Scheduling
		A task is the instantiation of a task definition within a cluster. After you have created a task definition for your application, you can specify the number of tasks that will run on your cluster.
		Each task that uses the Fargate launch type has its own isolation boundary and does not share the underlying kernel, CPU resources, memory resources, or elastic network interface with another task.
		The task scheduler is responsible for placing tasks within your cluster. There are several different scheduling options available.
		REPLICA — places and maintains the desired number of tasks across your cluster. By default, the service scheduler spreads tasks across Availability Zones. You can use task placement strategies and constraints to customize task placement decisions.
		DAEMON — deploys exactly one task on each active container instance that meets all of the task placement constraints that you specify in your cluster. When using this strategy, there is no need to specify a desired number of tasks, a task placement strategy, or use Service Auto Scaling policies.
		You can upload a new version of your application task definition, and the ECS scheduler automatically starts new containers using the updated image and stop containers running the previous version.
		Amazon ECS tasks running on both Amazon EC2 and AWS Fargate can mount Amazon Elastic File System (EFS) file systems.
		Clusters
		When you run tasks using ECS, you place them in a cluster, which is a logical grouping of resources.
		Clusters are Region-specific.
		Clusters can contain tasks using both the Fargate and EC2 launch types.
		When using the Fargate launch type with tasks within your cluster, ECS manages your cluster resources.
		When using the EC2 launch type, then your clusters are a group of container instances you manage. These clusters can contain multiple different container instance types, but each container instance may only be part of one cluster at a time.
		Before you can delete a cluster, you must delete the services and deregister the container instances inside that cluster.
		Enabling managed Amazon ECS cluster auto-scaling allows ECS to manage the scale-in and scale-out actions of the Auto Scaling group. On your behalf, Amazon ECS creates an AWS Auto Scaling scaling plan with a target tracking scaling policy based on the target capacity value that you specify.
		Services
		ECS allows you to run and maintain a specified number of instances of a task definition simultaneously in a cluster.
		In addition to maintaining the desired count of tasks in your service, you can optionally run your service behind a load balancer.
		There are two deployment strategies in ECS:
		Rolling Update
		This involves the service scheduler replacing the currently running version of the container with the latest version.
		The number of tasks ECS adds or removes from the service during a rolling update is controlled by the deployment configuration, which consists of the minimum and maximum number of tasks allowed during service deployment.
		Blue/Green Deployment with AWS CodeDeploy
		This deployment type allows you to verify a new deployment of a service before sending production traffic to it.
		The service must be configured to use either an Application Load Balancer or Network Load Balancer.
		Container Agent
		The container agent runs on each infrastructure resource within an ECS cluster.
		It sends information about the resource’s current running tasks and resource utilization to ECS, and starts and stops tasks whenever it receives a request from ECS.
		The container agent is only supported on Amazon EC2 instances.
		Service Load Balancing
		Amazon ECS services support the Application Load Balancer, Network Load Balancer, and Classic Load Balancer ELBs. Application Load Balancers are used to route HTTP/HTTPS (or layer 7) traffic. Network Load Balancers are used to route TCP or UDP (or layer 4) traffic. Classic Load Balancers are used to route TCP traffic.
		You can attach multiple target groups to your Amazon ECS services that are running on either Amazon EC2 or AWS Fargate. This allows you to maintain a single ECS service that can serve traffic from both internal and external load balancers and support multiple path-based routing rules and applications that need to expose more than one port.
		The Classic Load Balancer doesn’t allow you to run multiple copies of a task on the same instance. You must statically map port numbers on a container instance. However, an Application Load Balancer uses dynamic port mapping, so you can run multiple tasks from a single service on the same container instance.
		If a service’s task fails the load balancer health check criteria, the task is stopped and restarted. This process continues until your service reaches the number of desired running tasks.
		Services with tasks that use the awsvpc network mode, such as those with the Fargate launch type, do not support Classic Load Balancers. You must use NLB instead of TCP. 
		AWS Fargate
		You can use Fargate with ECS to run containers without having to manage servers or clusters of EC2 instances.
		You no longer have to provision, configure, or scale clusters of virtual machines to run containers.
		Fargate only supports container images hosted on Elastic Container Registry (ECR) or Docker Hub.
		Task Definitions for Fargate Launch Type
		Fargate task definitions require that the network mode is set to awsvpc. The awsvpc network mode provides each task with its own elastic network interface.
		Fargate task definitions require that you specify CPU and memory at the task level.
		Fargate task definitions only support the awslogs log driver for the log configuration. This configures your Fargate tasks to send log information to Amazon CloudWatch Logs.
		Task storage is ephemeral. After a Fargate task stops, the storage is deleted.
		Amazon ECS tasks running on both Amazon EC2 and AWS Fargate can mount Amazon Elastic File System (EFS) file systems.
		Put multiple containers in the same task definition if:
		Containers share a common lifecycle.
		Containers are required to be run on the same underlying host.
		You want your containers to share resources.
		Your containers share data volumes.
		Otherwise, define your containers in separate task definitions so that you can scale, provision, and de-provision them separately.
		Task Definitions for EC2 Launch Type
		Create task definitions that group the containers that are used for a common purpose, and separate the different components into multiple task definitions.
		After you have your task definitions, you can create services from them to maintain the availability of your desired tasks.
		For EC2 tasks, the following are the types of data volumes that can be used:
		Docker volumes
		Bind mounts
		AWS Exam Readiness Courses
		Private repositories are only supported by the EC2 Launch Type.
		Amazon Elastic Container Service Monitoring
		You can configure your container instances to send log information to CloudWatch Logs. This enables you to view different logs from your container instances in one convenient location.
		With CloudWatch Alarms, watch a single metric over a time period that you specify, and perform one or more actions based on the value of the metric relative to a given threshold over a number of time periods.
		Share log files between accounts, and monitor CloudTrail log files in real time by sending them to CloudWatch Logs.
		Amazon Elastic Container Service Tagging
		ECS resources, including task definitions, clusters, tasks, services, and container instances, are assigned an Amazon Resource Name (ARN) and a unique resource identifier (ID). These resources can be tagged with values that you define, to help you organize and identify them.
		Amazon Elastic Container Service Pricing
		With Fargate, you pay for the amount of vCPU and memory resources that your containerized application requests. vCPU and memory resources are calculated from the time your container images are pulled until the Amazon ECS Task terminates.
		There is no additional charge for the EC2 launch type. You pay for AWS resources (e.g. EC2 instances or EBS volumes) you create to store and run your application.

	
	
	
Amazon Elastic Kubernetes Service (EKS) 
---------------------------------------
	managed Kubernetes service in (AWS). 
	easily 
		deploy
		manage
		scale containerized applications 
			using Kubernetes on AWS infrastructure.

	without having to manage the underlying infrastructure. 
	EKS automates many of the tasks 
		in running a Kubernetes cluster
			patching
			scaling
			upgrading the Kubernetes control plane.

	EKS integrates with other AWS services
		like 
			Elastic Load Balancing
			Amazon RDS
			Amazon S3
				provide a seamless experience 
					for deploying and managing containerized applications. 
		supports 	
			Helm, 
			Istio
			Prometheus.
	
--------------------------------------------------------------------------------------------------------
	EKS Vs ECS
--------------------------------------------------------------------------------------------------------

Amazon Elastic Container Service (ECS) Vs Amazon Elastic Kubernetes Service (EKS) 
	both container orchestration services 
		by AWS
	several differences:
		Kubernetes vs. Docker: 
			EKS is based on Kubernetes
			ECS is based on Docker
			
			EKS 
				better suited for organizations in Kubernetes 
			ECS 
				better suited for organizations on containerization
					or Docker-based container strategy.

		Scalability: 
			Both ECS and EKS can scale 
				to thousands of containers
			EKS more scalable than ECS
				geneally understood so
			ECS requires manual configuration for scaling.

		Complexity: 
			EKS is generally considered to be more complex than ECS 
				complexity of Kubernetes. 
			EKS 
				requires knowledge of Kubernetes 
			ECS
				generally considered 
					more user-friendly 
					easier to use
						with less experience with containers and container orchestration.

		Integrations: 
			ECS integrates well with other AWS services like	
				AWS Fargate
				AWS Identity and 
				Access Management (IAM), 
				Amazon CloudWatch, 
				Amazon Virtual Private Cloud (VPC). 
			EKS also integrates well with these services, 
				well-suited for organizations 
					which use Kubernetes-based tools and services.

		Pricing: 
			pricing for ECS and EKS is similar
				some differences. 
				ECS pricing 
					based on 
						CPU and memory resources 
							used by the containers, 
				EKS pricing 
					based on the number of worker nodes in the cluster.



--------------------------------------------------------------------------------------------------------

	EKS
--------------------------------------------------------------------------------------------------------
What is Kubernetes?


Benefits of Amazon EKS: Why use AWS EKS?
	Difficult steps like 
		Kubernetes master cluster
		configuring service discovery
		Kubernetes primitives
		and networking. 
			setup for us
		volume 
			plugins
			lb plugins
			network plugins 
				etc available for us
	
Amazon EKS
	the Kubernetes control plane
		backend persistence layer 
		API servers
			provisioned and 
			scaled across 
				AWS availability zones, 
	high availability 
	eliminating a single point of failure. 
	Unhealthy control plane nodes 
		detected and 
		replaced
	patching is provided for the control plane. 
		
	Result
		resilient AWS-managed Kubernetes cluster 
		can withstand even the loss of an availability zone.

Organizations can choose to run EKS 
	using AWS Fargate--
		a serverless compute engine for containers. 
	With Fargate
		no need to 
			provision and 
			manage servers
	organizations can specify and pay for resources per application. 
	
EKS is integrated 
	with various AWS monitoring services
	easy for organizations 
		to scale and secure applications 
	
	AWS Identity Access Management (IAM) 
		for authentication 
			to Elastic Load Balancing 
				for load distribution etc




Resources
---------
PodTemplates are a Kubernetes resource used to define a template for creating new pods. A PodTemplateSpec is used to define the specification of a pod template, which includes information such as the container images to use, the command to run when the pod starts, and the networking settings for the pod.

PodTemplates are used as a blueprint for creating new pods, which can be used to deploy applications, batch jobs, or any other workload that runs on Kubernetes. By defining a PodTemplate, you can create multiple identical pods that share the same configuration and can be easily managed and scaled.

When creating a new pod from a PodTemplate, Kubernetes replaces any variables in the template with actual values, such as the name and labels of the pod. This allows you to create a large number of pods with different names and labels based on the same PodTemplate, without having to define each pod separately.


apiVersion: v1
kind: PodTemplate
metadata:
  name: nginx-template
spec:
  metadata:
    labels:
      app: nginx
  spec:
    containers:
    - name: nginx
      image: nginx:latest
      ports:
      - containerPort: 80
        protocol: TCP


Once you have defined a PodTemplate, you can create new pods based on it by using a controller like a Deployment or a StatefulSet. For example, you could create a Deployment that uses the nginx-template PodTemplate to create multiple replicas of the NGINX pod.


PriorityClasses
---------------
define 
	relative importance 
		of different workloads 
		running on a Kubernetes cluster.
	prioritize the scheduling and preemption of pods
	allows you to ensure that high-priority workloads receive the necessary resources and are not starved by lower-priority workloads.

PriorityClasses are defined using a numeric value between 1 and 1000000, where a higher value indicates a higher priority. 



https://dev-k8sref-io.web.app/docs/cluster/apiservice-v1/

APIService
----------
APIServiceSpec. 
	API objects
		define API resources
	specification of the properties of an API service 
		that the Kubernetes API Server should provide. 
	configuration for a particular version of the API
		including 
			API group
			version
			service name and 
			port
			authentication and authorization settings.

	used to define API services 
	external to the Kubernetes cluster
		like third-party APIs or 
		custom APIs created by users. 
	define internal API services 
		used by the 
			Kubernetes components 
				like 
					Kubernetes API Server itself 
				or 
					Kubernetes Scheduler.


	APIService 
		represents a server for a particular GroupVersion. 
		Name must be “version.group”.

	apiVersion: apiregistration.k8s.io/v1
	kind: APIService
	metadata (ObjectMeta)
	spec (APIServiceSpec)
	Spec contains information for locating and communicating with a server



	status (APIServiceStatus)
	Status contains derived information about an API server

	APIServiceSpec 
		contains information for 
			locating and communicating with a server. 
		Only https is supported
			but can disable certificate verification.
		click on apiservice 
			shows list of apiVersion
			further reference https://dev-k8sref-io.web.app/docs/cluster/apiservice-v1/
			
		
Kubernetes Lease
----------------
	an API object 
	ensure that 
		only one instance of a 
			particular resource exists 
				in a given namespace 
					at a time. 
			it has a timeout (like Lease)
			
	created and maintained by a Kubernetes controller 
	responsible for managing the resource it represents.

	The primary purpose 
		enable coordination 
			between multiple instances of a particular resource like 
				ReplicaSet or Deployment. 
		By creating a Lease for a resource
			only one instance of that resource can exist at a time. 
			When a new instance of the resource is created
				it will try to acquire the Lease. 
		If the Lease already exists
			new instance will not be able to acquire it and will terminate.

	have a configurable timeout
	determines how long the Lease will be held 
		by the owning instance. 
	If the timeout expires 
		Lease is NOT 
			renewed or released
		will be automatically deleted.
	Leases
		simple and effective way 
			to coordinate resources 
			avoid conflicts between multiple instances.


RuntimeClass 
------------
	API object 
	define the runtime configuration 
		for a pod or container. 
	which runtime should be used 
		to run a pod or container 
	defines various configuration options 
		for that runtime like 
			runtime version
			security settings
			other parameters.

	a way to abstract 
		underlying runtime implementation 
			from the pod or container specification
		define desired runtime 
			without knowing 
				details of the underlying implementation. 
		deploy and manage workloads easily
			across different Kubernetes clusters 
				with different runtime environments.

	RuntimeClasses are useful 
		different workloads 
			require different runtime environments like 
				multi-tenant environments or 
			when running workloads across different cloud providers or on-premises infrastructure. 
		By using RuntimeClasses, Kubernetes provides a simple and flexible way to define and manage runtime configurations for different workloads.

	To use a RuntimeClass
		specify the runtimeClassName field 
			in the pod or container specification
			cause Kubernetes to use the corresponding RuntimeClass 
				to run the pod or container. 
	If no runtimeClassName is specified, Kubernetes will use the default runtime configuration for the cluster.


FlowSchema 
----------
	API object 
		used to define a set of rules 
			for network traffic within a cluster. 
	how network traffic should be handled 
		based on various attributes 
			like 
				source and 
				destination IP addresses
				protocol used
					etc.

	created and maintained by a Kubernetes controller 
		responsible for enforcing the rules defined in the schema. 
	When network traffic enters the cluster
		controller will 
			use the rules in the FlowSchema 
				to determine how to handle the traffic
				like 
					whether to 
						allow it, 
						deny it
						modify .

	Usecase:
		to control how network traffic is handled within a cluster
		e.g. 
			when running multi-tenant environments 
				modify the request
		or 
			when enforcing security policies. 
			
	used in conjunction with 
		other Kubernetes networking features like 
			NetworkPolicies
				define network policies 
					for 
						pods or 
						namespaces
			ClusterNetworkPolicies
				define network policies 
					for the entire cluster. 
	
	NetworkPolicies vs FlowSchema
	NetworkPolicies 
		control the ingress and egress traffic 
			of a Kubernetes Pod 
				by defining a set of rules that specify the allowed traffic. 
		can be applied to a set of Pods 
			based on their labels
			can also be used to control traffic 
				between different namespaces. 
			used to enforce 
				security policies, 
				limit access to specific services
			or 
				to isolate workloads.

	FlowSchema
		is used to define 
			custom traffic policies 
				for Kubernetes Services. 
		define custom traffic rules 
			based on a set of criteria like 
				source IP address, 
				protocol
				port number. 
		more granular control over the network traffic 
			compared to NetworkPolicies


PriorityLevelConfiguration 
--------------------------
	API object 
		used to define priority levels 
			for different types of workloads 
				running in a cluster. 
	PriorityLevelConfigurations 
		specify which workloads should be given priority 
			during resources constraints 
	A PriorityLevelConfiguration 
		specifies a set of rules 
			how workloads should be prioritized 
				based on various factors 
					like 
						workload's importance
						resource requirements
					impact on other workloads 
		When a new workload is created
		Kubernetes scheduler 
			use the rules defined in 
				PriorityLevelConfiguration 
					to determine its priority relative 
						to other workloads in the cluster.

	PriorityLevelConfigurations 
		useful to 
			ensure 
				certain workloads prioritized over others
			like running critical production workloads 
				alongside less important test and development workloads. 
		
	Along with 
		priority levels for workloads
		PriorityLevelConfigurations 
			define default limits for certain types of resources
			like 			
				CPU and memory
					resources are allocated in a balanced and efficient way across the cluster.



Endpoint 
---------
	API object 
	represents a set of 
		network addresses and 
		ports 
			used to access a service. 
	automatically created and managed by Kubernetes 
		when a Service is created
		used to route traffic to the correct pods 
			that are backing the service.

	associated with a particular Service
		contains a list of 
			IP addresses and ports 
				for pods that are backing the service. 
	make a request to Service
		Kubernetes uses Endpoints 
			determine which pods should receive request
			routes the traffic to those pods.

	Endpoints can be manually 
		created and updated 
			using the Kubernetes API
		mostly automatically managed by Kubernetes 
			based on the configuration of the Service. 
	

EndpointSlice 
-------------
	API object 
		represents a subset of the endpoints 
			associated with a Service. 
	 EndpointSlice 
		more granular control over ip's 
			than endpoints.

	used by Kubernetes 
		to improve the scalability and performance 
			of Services 
				with lot of endpoints. 
		By dividing the endpoints into smaller slices
			Kubernetes can reduce network traffic and processing 
				required to manage the endpoints
			improve the overall performance and efficiency of the cluster.

	Each EndpointSlice 
		contains 
			subset of the endpoints 
				associated with a Service
			along with metadata like 
				labels and annotations 
		can be used to 
			define and manage the endpoints. 
		automatically created and managed by Kubernetes 
			based on the configuration of Service and the state of the cluster.

IngressClass 

	an API object 
	used to define 
		set of rules 
			for how traffic should be routed 
				to Services within a cluster. 
	used in conjunction with Ingress objects
		ingress:
			define how traffic should be routed to a particular Service.

	IngressClass 
		specifies a set of rules 
		how traffic should be routed 
			based on various factors like 
				type of traffic
				hostname or 
				path of the request
			or 
				other characteristics of the traffic. 
		When a new Ingress object is created
			Kubernetes API server 
				will use the rules defined in the IngressClass 
					determine how traffic should be routed to the Service.
	useful to manage complex routing scenarios 
	or 
		provide different levels of routing functionality 
			for different workloads in the cluster. 
	IngressClasses
		define 
			routing rules 
			default settings for SSL certificates
			authentication and authorization policies
			other features that can be used to further customize and secure network traffic within the cluster.


VolumeAttachment
	API object 
	attach a persistent volume 
		to a node 
	When a VolumeAttachment is created
		Kubernetes will bind 
			persistent volume to the node
				available for use by pods running on that node.

	useful to provide persistent storage 
		for stateful workloads 
		running in the cluster. 
	Kubernetes 
		way to manage storage volumes 
		ensure that they are available to the pods that need them.
	
	manually created and updated 
	using the Kubernetes API, but in most cases they are automatically managed by Kubernetes based on the configuration of the pod and the state of the cluster. When a pod is created or updated, Kubernetes will automatically check to see if the required persistent volume is available and, if necessary, create a new VolumeAttachment to bind the volume to the node.

	In addition to managing storage volumes, VolumeAttachments can also be used to define various settings related to the storage, such as the access mode and the size of the volume. This can help ensure that storage resources are allocated in a balanced and efficient way across the cluster, and that the storage is used in the most effective way possible.

CSIDriver 
	is an API object that is used to define a custom storage driver that can be used by pods to access external storage systems. CSIDrivers provide a simple and flexible way to manage storage volumes in Kubernetes, and they can be used to integrate with a wide variety of external storage systems.

	CSIDrivers are designed to work with the Container Storage Interface (CSI), which is a standardized interface for integrating external storage systems with container orchestration platforms such as Kubernetes. When a CSIDriver is created, Kubernetes will automatically register it with the CSI driver registry, making it available for use by pods running in the cluster.

	CSIDrivers can be used to define various settings related to the storage driver, such as the driver name, the URL of the storage system, and other configuration options. They can also be used to manage various aspects of the storage driver, such as authentication and authorization policies, data encryption settings, and other security-related features.

	CSIDrivers are useful in situations where it is important to provide access to external storage systems, such as cloud storage services or other storage providers. By using CSIDrivers, Kubernetes provides a simple and flexible way to manage storage volumes and ensure that they are available to the pods that need them.


CSINode 
	is an API object that is used to represent a node in the cluster that has one or more CSI (Container Storage Interface) drivers installed and configured. A CSINode provides information about the node's capabilities and configuration related to external storage systems, which can be used by Kubernetes to schedule pods and allocate storage resources in a more efficient and effective way.

	When a CSI driver is installed on a node, it registers itself with Kubernetes using a CSIDriver object. The node can then be identified as a CSINode, which provides information about the available drivers and their capabilities. This information can be used by Kubernetes to determine which nodes are capable of providing the required storage resources for a given pod, and to schedule the pod accordingly.

	CSINodes can be used to define various settings related to the CSI drivers installed on the node, such as the driver name, the URL of the storage system, and other configuration options. They can also be used to manage various aspects of the driver's functionality, such as authentication and authorization policies, data encryption settings, and other security-related features.

	CSINodes are useful in situations where it is important to provide access to external storage systems, such as cloud storage services or other storage providers. By using CSINodes, Kubernetes provides a simple and flexible way to manage storage volumes and ensure that they are available to the pods that need them.


CSIStorageCapacity 
	API object that is used to represent the storage capacity of a node that has one or more CSI (Container Storage Interface) drivers installed and configured. CSIStorageCapacities provide information about the available storage resources on the node, which can be used by Kubernetes to allocate storage resources in a more efficient and effective way.

	When a CSI driver is installed on a node, it can report the available storage resources to Kubernetes using a CSIStorageCapacity object. The object contains information about the available capacity, the access mode, and other relevant attributes of the storage resource. Kubernetes can then use this information to determine which nodes are capable of providing the required storage resources for a given pod, and to schedule the pod accordingly.

	CSIStorageCapacities can be used to define various settings related to the available storage resources, such as the access mode, the size of the storage resource, and other configuration options. They can also be used to manage various aspects of the storage resource's functionality, such as encryption settings and other security-related features.

	CSIStorageCapacities are useful in situations where it is important to provide access to external storage systems, such as cloud storage services or other storage providers. By using CSIStorageCapacities, Kubernetes provides a simple and flexible way to manage storage volumes and ensure that they are available to the pods that need them.

LimitRange 
	is a policy object that is used to specify default and maximum limits for compute resources such as CPU and memory, as well as storage resources such as storage size, for containers in a namespace.

	LimitRanges are useful for enforcing resource constraints in a Kubernetes cluster, which helps prevent a single pod from consuming too many resources and impacting the performance and stability of the entire cluster. By setting resource limits on a per-namespace basis, LimitRanges allow for more fine-grained control over resource usage.

	LimitRanges can be used to define various settings related to resource limits, such as the default and maximum values for CPU and memory, as well as the minimum and maximum storage sizes. They can also be used to manage various aspects of the resource usage, such as enforcing limits on a per-container basis and providing warnings when containers exceed their limits.

	LimitRanges can be particularly useful in multi-tenant environments where multiple teams or applications share the same cluster. By using LimitRanges, administrators can ensure that each team or application has access to the resources they need without impacting the performance and stability of other applications running on the same cluster.


ResourceQuota 
	is a policy object that is used to limit the total amount of compute resources such as CPU and memory, as well as storage resources such as storage size, that can be used by objects in a namespace.

	ResourceQuotas are useful for enforcing resource constraints in a Kubernetes cluster, which helps prevent a single namespace from consuming too many resources and impacting the performance and stability of the entire cluster. By setting resource quotas on a per-namespace basis, ResourceQuotas allow for more fine-grained control over resource usage.

	ResourceQuotas can be used to define various settings related to resource limits, such as the maximum amount of CPU and memory that can be used by all containers in a namespace, as well as the maximum storage size. They can also be used to manage various aspects of the resource usage, such as providing warnings when the usage is close to exceeding the quota and specifying a hard limit to prevent exceeding the quota.

	ResourceQuotas can be particularly useful in multi-tenant environments where multiple teams or applications share the same cluster. By using ResourceQuotas, administrators can ensure that each team or application has access to the resources they need without impacting the performance and stability of other applications running on the same cluster.


NetworkPolicy 
	is a policy object that is used to control the traffic flow between pods in a namespace. NetworkPolicies provide a way to enforce network segmentation and isolation, which helps improve the security and stability of the cluster.

	NetworkPolicies allow administrators to define rules that specify which pods are allowed to communicate with each other and which are not. These rules can be based on various criteria such as pod labels, namespaces, IP addresses, and ports. By specifying these rules, administrators can create a fine-grained network policy that controls the flow of traffic within the cluster.

	NetworkPolicies can be used to define various settings related to network traffic, such as ingress and egress rules, ports, and IP addresses. They can also be used to manage various aspects of the network traffic, such as blocking traffic from certain pods or allowing traffic only between specific pods.

	NetworkPolicies can be particularly useful in multi-tenant environments where multiple teams or applications share the same cluster. By using NetworkPolicies, administrators can ensure that each team or application has access to the network resources they need without impacting the security and stability of other applications running on the same cluster.


PodDisruptionBudget (PDB) 
	policy object that is used to limit the number of pods that can be down simultaneously during a disruption, such as a node outage or a planned update. PodDisruptionBudgets provide a way to manage the availability and reliability of a Kubernetes application.

	PodDisruptionBudgets allow administrators to define rules that specify the minimum number of pods that must be available at any given time, as well as the maximum number of pods that can be unavailable during a disruption. These rules can be based on various criteria such as pod labels and namespaces. By specifying these rules, administrators can create a fine-grained PodDisruptionBudget that controls the availability of pods within the cluster.

	PodDisruptionBudgets can be used to define various settings related to pod disruption, such as the maximum number of pods that can be down at any given time and the minimum number of pods that must be available during a disruption. They can also be used to manage various aspects of the pod disruption, such as the order in which pods are taken down and the amount of time allowed for the disruption.

	PodDisruptionBudgets can be particularly useful in high availability environments where downtime can have significant impact on the business. By using PodDisruptionBudgets, administrators can ensure that a minimum number of pods remain available at all times and that the impact of any disruptions is minimized.

MutatingWebhookConfigurations
ValidatingWebhookConfigurations

--------------------------------------------------------------------------------------------------------
•	Create/Delete a cluster from UI
--------------------------------------------------------------------------------------------------------
D:\PraiseTheLord\HSBGInfotech\Others\vilas\docker-k8s\yaml\eks\Notes.txt
--------------------------------------------------------------------------------------------------------
•	Create/Delete a cluster from eksctl
--------------------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------------------
•	Autoscaling
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Setup your interaction with eks cluster
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Create pod
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Create deployment
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Create services
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Create jobs
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Working with Kubernetes Volumes in AWS
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Kubernetes networking in AWS
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Working with Network policies in AWS
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Working with RBAC in AWS
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------

•	Kubernetes Scheduling
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Discussion and Lab
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	Node Affinity/Anti affinity
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
	Taints and Tolerations
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
•	Security in Kubernetes cluster
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Discussion on 4 C’s cloud native security
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------


•	Security in Kubernetes cluster
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Discussion on 4 C’s cloud native security
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------


Day 9
•	AWS CI/CD
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Introduction to CI/CD
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Real life ci/cd implementation
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Create a repository in github
--------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------
o	Create an s3 bucket and modify it’s permissions to all aws ci/cd
--------------------------------------------------------------------------------------------------------
https://catalog.workshops.aws/general-immersionday/en-US/basic-modules/60-s3
--------------------------------------------------------------------------------------------------------
o	Overview of CodeBuild, CodeDeploy
--------------------------------------------------------------------------------------------------------
• AWS CodeCommit – 
	storing our code
	like github
	create a repo.
	push code into codecommit
	
	Why Version control? 
		understand changes to code 
			over time (and possibly roll back)
	• Enabled using - version control system : Git
	• A Git repository 
		can be synchronized on your computer
		uploaded on a central online repository
	• Benefits are:
		• Collaborate with other developers
		• Make sure the code is backed-up somewhere
		• Make sure it’s fully viewable and auditable
	AWS CodeCommit
		Git repositories can be expensive
		• Other options: GitHub, GitLab, Bitbucket, …

		• Private Git repositories
		• No size limit on repositories (scale seamlessly)
		• Fully managed, highly available
		• Code only in AWS Cloud account => increased security and compliance
		• Security (encrypted, access control, …)
		• Integrated with Jenkins, AWS CodeBuild, and other CI tools

		CodeCommit – Security
		• Interact using Git (standard)
		• Authentication
			• SSH Keys – AWS Users can configure SSH keys in their IAM Console
			• HTTPS – with AWS CLI Credential helper or Git Credentials for IAM user
		• Authorization
			• IAM policies to manage users/roles permissions to repositories
		• Encryption
			• automatically encrypted at rest using AWS KMS
			• Encrypted in transit (can only use HTTPS or SSH – both secure)



• AWS CodePipeline – 
	Visual Workflow to orchestrate your CICD
	• Source
		CodeCommit
		ECR
		S3
		Bitbucket
		GitHub 
	• Build 
		CodeBuild
		Jenkins
		CloudBees
		TeamCity
	• Test 
		CodeBuild
		AWS Device Farm
		3rd party tools, etc.
	• Deploy 
		CodeDeploy
		Elastic Beanstalk
		CloudFormation
		ECS
		S3, etc.
	• Consists of stages:
		• Each stage can have sequential actions and/or parallel actions
		• Example: Build èTest è Deploy è Load Testing è …
		• Manual approval can be defined at any stage
• AWS CodeBuild – 
	building and testing our code
	• A fully managed continuous integration (CI) service
	• Continuous scaling (no servers to manage or provision – no build queue)
	• Compile source code, run tests, produce software packages, …
	• Alternative to other build tools (e.g., Jenkins)
	• Charged per minute for compute resources (time it takes to complete the builds)
	• Leverages Docker under the hood for reproducible builds
	• Use prepackaged Docker images or create your own custom Docker image
	• Security:
	• Integration with KMS for encryption of build artifacts
	• IAM for CodeBuild permissions, and VPC for network security
	• AWS CloudTrail for API calls logging
	• Source – 
		CodeCommit
		S3
		Bitbucket
		GitHub
	• Build instructions: 
		buildspec.yml or 
		insert manually in Console
	• Output logs 
		can be stored in 
			Amazon S3 & 
			CloudWatch Logs
	• Use CloudWatch Metrics to 
		monitor build statistics
	• Use CloudWatch Events to 
		detect failed builds and trigger notifications
	• Use CloudWatch Alarms to 
		notify if you need “thresholds” for failures
	Build Projects can be defined within CodePipeline or CodeBuild	
	CodeBuild – Supported Environments
		• Java
		• Ruby
		• Python
		• Go
		• Node.js
		• Android
		• .NET Core
		• PHP
		• Docker – extend any environment you like
	
	
• AWS CodeDeploy – 
	deploying the code to EC2 instances (not Elastic Beanstalk)
• AWS CodeStar – 
	manage software development activities in one place
• AWS CodeArtifact – 
	store, publish, and share software packages
• AWS CodeGuru – 
	automated code reviews using Machine Learning





Amazon CodeDeploy - Features
--------------------------------------
Reference: 
	Multiple sources
	
It is a service offered by Amazon that is used for the purpose of deployment. It helps in the automation of the application deployment of Amazon EC2 instances, on premise instances, server less Lambda functions, and ECS services.  

Content mentioned below, and much more can be deployed with the help of AWS CodeDeploy: 

	Code 
	Server less Lambda functions 
	Web and configuration files 
	Packages  
	Scripts 
	Executables 
	Multimedia files 
CodeDeploy has the ability to deploy content within the application that is present in many locations. Some of them have been mentioned below: 

Application that runs on servers 
Application that be stored in Amazon S3 buckets 
Application that is stored in Github repositories 
Application that is stored in BitBucket repositories 
Before deploying code, there is no need to make any explicit changes to the existing code so as to deploy it using CodeDeploy.   

It can be used to update AWS Lambda function’s versions. 
It can be used to release new features of the application in a very short period of time.  
It helps handle the complexity of updating the user-developed application without associating any risks that could occur when updates are done manually. This is because manual updates could be error-prone.  
CodeDeploy has been designed in such a manner that it can work with multiple systems to manage configurations, perform source control, continuous integration, continuous deployment (CI-CD), and continuous delivery.  

In addition to this, it provisions a way of quickly searching for the user resources (which includes repositories, project builds, deployment applications, pipelines).  

CodeDeploy can deploy applications to the below mentioned compute platforms: 

EC2 or on premise
This platform talks about the physical servers which could be Amazon EC2 cloud instances, on premise servers or both. When an application is created with the help of EC2 or on-premise compute platform, it can contain executable files, configuration files, images and other types of data. When an application is deployed using the EC2 or on-premise compute platform, it helps manage the way in which traffic is routed to that instance, with the help of an in-place or blue-green deployment type.  

AWS Lambda
It is used to deploy applications which have an updated version of the Lambda function. Lambda helps in the management of Lambda function which is present in a server less compute environment. This environment contains high-available resources. The monitoring of these compute resources is taken care of by AWS Lambda itself.  

When an application is created with the help of the Lambda compute platform, it manages the method in which traffic is routed to the updated Lambda function’s versions. It also manages the deployment by choosing one of the methods from a canary, linear or all-at-once configuration.  

Amazon ECS
It is used to deploy Amazon ECS applications which have been containerized as a part of the task set. CodeDeploy deploys such applications using the blue/green deployment, wherein it installs an updated version of the containerized application as the new replacement task set. Once this is done, CodeDeploy reroutes the traffic from the original containerized application to the replaced task set. Once this is completed successfully, the original task gets terminated.  

CodeDeploy comes with two methods of deployment: 

In-place deployment
In this method, the application present on every instance of the deployment group is stopped, and the latest application (which would have been revised) is installed. After this, the new version of this application starts and gets validation. A load balancer can be used so that every instance gets deregistered when it is being deployed and is restored when the deployment is complete. In-place deployments can be used only when the applications use EC2 or on-premise compute platform.  

AWS Lambda and Amazon ECS can’t deploy their applications using an in-place deployment.  

Blue/green deployment
In this method, the deployment’s characteristics depends on which compute platform is used to deploy the application.  

Blue/green deployment on EC2 or on-premise compute platform: The original environment’s instances are replaced by a different environment’s instances. The replacement environment is provisioned. The latest application (which is revised) gets installed as the replacement instance. There is an optional wait time so as to perform application testing and system verification. This newly replaced instance is registered with an Elastic Load Balancing load balancer, due to which the traffic gets rerouted to these instances. The instances present in the original environment are deregistered, and can be terminated as well, or could be run to perform other operations.  
An important point to remember is that blue/green deployments work only with Amazon EC2 instances.  

Blue/green deployment on AWS Lambda compute platform: The traffic that comes to the current server less environment is rerouted to the updated Lambda function version. Lambda functions can be specified to perform validation tests, and specific way can be chosen to handle the shift in traffic. Any deployment which is performed on the AWS Lambda compute platform is considered as a blue/green deployment. This is the reason why a deployment type need not be specified while using Lambda compute platform.  
Blue/green deployment on Amazon ECS compute platform: This is used with containerized applications. The traffic is rerouted from the original version of a containerized application which is present in Amazon ECS to a replacement task set that is present in the same ECS service. This production traffic is rerouted by specifying the protocol and port of the load balancer. When a deployment takes place, a test listener is used to handle traffic of the replacement task set while the validation tests are being executed.  
Features of CodeDeploy
It can be used to deploy applications which require servers (traditional applications), applications which are server less, and applications such as Amazon ECS (container applications). 
CodeDeploy is a fully managed, automated service which automatically deploys the user application in development, test and production environments.  
It is highly scalable with the user infrastructure and can be used to deploy one or thousand instances.   
An application can be made to be highly available with the help of CodeDeploy in case the application uses an EC2 or on premise compute platform. When an in-place deployment happens, CodeDeploy does a rolling update across Amazon EC2 instances. The number of instances which need to be taken offline must be specified during the time of update.  
When a blue/green deployment is used, the most recent updated application is installed on the replacement instances. As soon as this happens, traffic is rerouted to these instances which are chosen by the user at that point in time or right after the new environment has been tested.  
In case of both these deployment types, CodeDeploy service keeps a track on the health of the application based on the rules used to configure the service.  
A deployment could be automatically or manually stopped. Just after this stoppage, it can be rolled back to the previous state in case of any errors.  
The status of every deployment done by the user can be tracked with the help of CodeDeploy console or the AWS CLI. The user will receive a report that lists information on when every application was revised, and when it was deployed, and to which Amazon EC2 instance it was deployed.  
If multiple applications use the EC2 or on premise compute platform, CodeDeploy helps in deploying all of these application concurrently to the same set of instances.  
CodeDeploy is an easy to use, and easy-to-adopt platform that works with any application. The setup code can be conveniently reused.  
CodeDeploy can be integrated with the process of software release or the continuous delivery toolchain to make the process of deployment efficient and easy.  
------------------------------
Code
	Code commit
		Code build
			Code deploy

<--------- Code pipeline ----------->

CICD Workflow

Create an ec2 instance
	assume this is my dev.
	install git
	connect to CodeCommit using ssh
	code 
		download code from somewhere
		upload from my computer
	Need IAM to push into CodeCommit

	CodeBuild 
		download code 
		build the code
		Generates artifacts
			store it in s3 bucket
		To build
			AWS provides many pre-packaged env.
				refer documentation 
				as pre-packaged env. keeps changing
	CodeDeploy
		pick artifact and deploy into target env.
	Target env.
		EC2
		Target group 
			behind LB
			diff types of env.
			identify using tags
		Container based
		Blue/Green deployment
	

Lab 
	1. Create ubuntu instance
		t2.micro
		vpc
		tag: type: dev-server
		port: 22, 8080
		
		This instance 
			Need access to CodeCommit
	2. Create a role: ec2user-cicd
		EC2: should get access	
			AWSCodeCommitFullAccess
			AWSCodeDeployFullAccess
			AWSCodePipeline_FullAccess
			CloudWatchFullAccess
			AmazonS3FullAccess
			
		Attach this Role into the same instance.
			ec2 instance	
				Security
					
		
	3. Connect to ec2 instance
		sudo su
		apt update -y
		apt upgrade -y
		apt install awscli -y
		aws configure 
		
		Let /opt be the root directory
		cd /opt
		sudo chown ubuntu.ubuntu -R /opt
		ls -al
		
	4. Create a CodeCommit repo.
		- give name (need to do not as admin)
		- Create user 
			- all access
			- give pwd
			- permission 
				AdministratorAccess
		
		Signin as this user
			
	5. Create CodeCommit
		Signin as this IAM user
		
		in ssh 
		1. Connect to ec2 instance install git
			sudo apt-add-repository ppa:git-core/ppa
			sudo apt-get update
			sudo apt-get install git
		
		2. Create a new key pair
			Register "SSH Public Key"
			upload ssh public key
		
			be ubuntu user
			ssh-keygen 
			directory /home/ubuntu/.ssh/codecommit_rsa
			ls ~/.ssh/codecommit_rsa*
			
			Users -> "my user"
				Under "Access Key"..
					"SSH keys for AWS Code Commit"
					Upload public key (e.g. APKAT4L7NELABYXRPS7O)
					Get the Key Id from the screen
		3. Edit Local SSH Configuration
			Create a file ~/.ssh/config
			Add lines as documented 
			Host..
			User: <This should key Id copied from above >
			IdentityFile ~/.ssh/codecommit_rsa
					
					
----------------------------------------------------------
my example entry
Host git-codecommit.*.amazonaws.com
User APKAT4L7NELABYXRPS7O
IdentityFile ~/.ssh/codecommit_rsa
----------------------------------------------------------

			chmod 600 config
			
		test connecting to codecommit 
			ssh git-codecommit .us-east-2.amazonaws.com
				type: passcode of user
			
		4. Clone the gitrepo.
			git clone <your repo>
			or get your code to the env.
			
			git add .
			git commit -m "initial commit"
			git push origin master
				give user's password
				
	
	sudo chown ubuntu:ubuntu codecommitrepo
	git clone https://github.com/vilasvarghese/docker-k8s.git
	cd docker-k8s
	cp src ../codecommitrepo/
	cp pom.xml ../codecommitrepo/
	
	cd ../codecommitrepo/
	git add .
	git commit -m "initial"
	git push origin master
	
		
--------------------------------
below is another way to do the above
mkdir /opt/temp
cd /opt/temp
To be executed on your local laptop
scp -i your.pem HelloWorld-CodeBase.tar.gz ubuntu@PUBLIC-IP:/opt/temp
tar -zxf HelloWorld-CodeBase.tar.gz
cd HelloWorld
rm -rf .git
rsync -r /opt/temp/HelloWorld/ /opt/helloworld
cd /opt/helloworld
rm -rf /opt/temp
Git commands

cd /opt/helloworld
git add .
git status
git commit -a -m “First commit”
git push origin master		
--------------------------------
			
		
			Check image in D:\PraiseTheLord\HSBGInfotech\AWS\cicd
				pipeline triggers codebuild
					Creates a build environment
						docker container
						aws provides out-of-box env.
						we can also create our own env.
					push artifact to s3
					use sns for notification
						like sms
					logs would be pushed to aws cloudwatch
			We need to follow:
				
				https://docs.aws.amazon.com/codebuild/latest/userguide/sample-build-notifications.html
					we setup cloudwatch for notifications
						notifications not managed in codebuild
							but in cloudwatch
					to do this: follow above url images
					lab: 
					----------------------
						TBD
					----------------------	


			https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html
			

			
			
CodBuild 
	Notification setup in CloudWatch 
		not in CodeBuild
	Create a build env.
		provide an env. 
	Build code base
	Create artifact.
	push into s3
	
	Works based on buildspec.yaml
	
5. Create an s3 bucket
	store artifacts in s3 
		destination of build
	create bucket
		name: mybucket
		ACLs disabled (recommended)
		Block public access
		Bucket Key: Disable
		
		
		It can grow in size quickly
			so setup a lifecycle - "delete artificat-automatically" on daily basis
			"management"
				Add lifecycle rule
					"Lifecycle rule name": deleted-daily
					"Apply to all objects in the bucket" 
					Lifecycle rule actions: Expire current versions of objects
					Days after object creation: 1
					

6. CodeBuild
		Create Build Project
		ignore build badge 
			publicly access the progress.
		Source: AWS CodeCommit
		Repository: <name we gave>
		Ref. type: Branch
		Branch: select "master"
		Git clone depth: 1
		
		Env: Managed Image 
			O/S: Amazon Linux2
		Runtime: standard
		Privileged: 
			New service role
			image "standard 3:0"
		Artifact: s3 and copy the name.
			
		Role name: <name>

	Use a build file: 
		give a build file
	Bucket related information 
		self explanatory


	lab: 
		The source code 
			fetched from the CodeCommit repository. 
			can also be from 
				GitHub 
				S3 bucket.
		The CodeBuild service expects 
			buildspec.yml 
				contains some commands to run 
					during the build phases like 
						Install, 
						Pre-Build, 
						Build and 
						Post-Build. 
				needs to be in the ROOT of the project folder 
				or can be directly entered.
			Alternativel we can use maven commands.
		The generated artifacts 
			J2EE JAR/WAR/EAR file is stored in the S3 bucket.
		Notifications 
			using AWS Simple Notification Service (SNS) 
			can be configured 
				e.g. to email the build results to the relevant users.

	
		Create Build Project
			Login to AWS 
				launch the CodeBuild service. 
				Click to create a new Build project.
			Enter the details as follows:
				Project name: AWS-HelloWorld
				Source provider: AWS CodeCommit
				Repository: <your codecommit repo>
				Operating system: Ubuntu/"Amazon Linux 2"
				Runtime: Standard
				Runtime version: "standard 3:0"
					List refer  https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-available.html
					Select based on 
					https://docs.aws.amazon.com/codebuild/latest/userguide/available-runtimes.html
				Build specification: Insert build commands
				Build commands: mvn clean package
				
					Save and "Build Now"
					watch the logs
				
				Output files: Target/AWS-HelloWorld-1.0.0.war (This is in a format of Artifact ID-Version taken from the POM file).
				
				Artifacts: Where to put the artifacts from this build project.
				Type: Amazon S3
				Name: HWJavaWebTarget (This will be the folder within the bucket).
				Bucket name: hwcodebuildbucket (The bucket has to be created initially before running the build and must be in lowercase as per conventions).
		
Repeat it with Build in the repo. 
	Refer: D:\PraiseTheLord\HSBGInfotech\AWS\cicd\HelloWorld\

		
	Build spec can reference to variables 
		Refer https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html
		

continue from here 
CodeDeploy
----------
Check the supported ec2 instances for codedeploy in 
	https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html
	
- create a new prod server
	ubuntu 
	[attach the same IAM role: may not be required]
	
		everything default
		open port : 22, 8080
		connect to it
		
		sudo su
		apt update -y
		apt -y install openjdk-8-jdk
		mkdir /var/lib/tomcat8
		cd /var/lib/tomcat8
		wget https://downloads.apache.org/tomcat/tomcat-8/v8.5.87/bin/apache-tomcat-8.5.87.tar.gz -O tomcat.tar.gz
		tar xvfz tomcat.tar.gz
		cd apache-tomcat-8.5.87
		mv * ../
		cd ..
		rm -rf apache-tomcat-8.5.87
		
		
		
		cd bin 
			./catalina run
		<public ip:8080>
	ls /var/lib/tomcat8/webapps
		/ROOT
		
		apt install awscli -y
		aws configure 
		us-east-1
		

		
		Follow 
			https://docs.aws.amazon.com/codedeploy/latest/userguide/getting-started-codedeploy.html
			
		Install codedeploy agent
		https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent-operations-install.html
		check 
			wget https://aws-codedeploy-us-east-1.s3.us-east-1.amazonaws.com/latest/install

		https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent-operations-install-cli.html
		
		
		Create a role 
			Search for "CodeDeploy"
			
		Create code deploy 
			Create application
			Compute plaform: ec2/on-prem
			
			Create deployment group
			Create application
				Revision location: 
					s3://vilasartifacts/vilasbuild1/HelloWorld.tar.gz
			
		

		
			
		Step 1: Setting up
			Provision user 
			Install aws cli
				
		Step 2: Create a service role for CodeDeploy
			
			Create a new role with "CodeDeploy"
			
			Create code deploy application, deployment group and a deployment
			Run the deployment.
			
			http://publicip:8080/HelloWorld-1/
			
			cat /var/log/aws/codedeploy-agent/codedeploy-agent.log
			
		Step 3: Limit the CodeDeploy user's permissions
		Step 4: Create an IAM instance profile for your Amazon EC2 instances
---------------------------------------------
sudo chown ubuntu:ubuntu -R /opt
cd /opt
sudo apt install ruby -y
cd /opt
Execute any one command (demonstrating two possible options)
wget https://aws-codedeploy-us-west-2.s3.amazonaws.com/latest/install (Links to an external site.)Links to an external site.
wget https://aws-codedeploy-us-west-2.s3.us-west-2.amazonaws.com/latest/install (Links to an external site.)Links to an external site.
chmod +x ./install
sudo ./install auto
After installation steps

Stop and start (multiple times) and then you will see the service running

sudo service codedeploy-agent status
sudo service codedeploy-agent stop
sudo service codedeploy-agent start
Agent logs

tail -30 /var/log/aws/codedeploy-agent/codedeploy-agent.log	
---------------------------------------------



CodePipeline
------------
Update buildspec.
	comment tar and uncomment last 2 lines in artifacts.
	setup codepipeline and execute it.
	
	
	
Automatically trigger, approval flow
-------------------------------------
- Go to SNS
	Create topic 
		name: 
		All default
		Create a role (default values)
		EveryOne access
		HTTP
	
	Create subscription 
		Protocol: email
		endpoint: email id
			
		Needs approval
			- go to your mail
				approve
				Refresh: confirm on sns if it is approved
	Go back to the pipeline 
		edit pipeline
			add a stage between build and deploy
				name: human-approval
				Add action
					Action name: human0-approval-action
					Action provider: Manual approval
					SNS topic: which you created just now
					Variable ns :
					Give a message: 
						
		Save

	Make a change and commit, watch the progress.
		do manual approval
		
		start tomcat and confirm.
		
		http://publicip:8080/HelloWorld-1/
		
	Goto cloudwatch 
		Logs -> loggroups
			click on your build 
				find the build name from code build if required.
				
	

AWS CodeCommit
---------------
Version control 
	ability to understand changes 
		happened to code 
			over time 
				(and possibly roll back)
• Enabled using 	
	version control system 
		e.g. Git
• A Git repository 
	can be synchronized on your computer
		uploaded on a central online repository
• Benefits are:
	• Collaborate with other developers
	• Make sure the code is backed-up somewhere
	• Make sure it’s fully viewable and auditable
• Enterprise specific private Git repositories can be expensive
• 	e.g. GitHub, GitLab, Bitbucket, 
• AWS CodeCommit:
	• Private Git repositories
	• No size limit on repositories 
		(scale seamlessly)
	• Fully managed
	• highly available
	• Code in AWS Cloud account 
		security and compliance
			encrypted
			access control
			etc.
	• Can be integrated with 
		Jenkins
		AWS CodeBuild
		other CI tools


CodeCommit – Security
	• Interactions are done using Git (standard)
	• Authentication
		• SSH Keys 
			AWS Users configure SSH keys in their IAM Console
		• HTTPS 
			AWS CLI Credential helper or Git Credentials for IAM user
	• Authorization
		• IAM policies to manage users/roles permissions to repositories
	• Encryption
		• Repositories are automatically encrypted at rest using AWS KMS
		• Encrypted in transit (can only use HTTPS or SSH – both secure)
	• Cross-account Access
		• Do NOT share your SSH keys or your AWS credentials
		• Use an IAM Role in your AWS account and use AWS STS (AssumeRole API)


Particulars									CodeCommit 	vs. 		GitHub
														
Support Code Review (Pull Requests)			Yes						Yes
Integration with AWS CodeBuild				Yes						Yes
Authentication (SSH & HTTPS)				Yes						Yes
Security 									IAM Users & Roles 		GitHub Users					
Hosting 									Managed & hosted by AWS - Hosted by GitHub		 
																	- GitHub Enterprise: self
																		hosted on your servers
UI 											Minimal 				Fully Featured
--------------------------------------------------------------------------------------------------------
o	Setup a CI/CD pipeline deploying to EKS cluster
--------------------------------------------------------------------------------------------------------
https://www.stacksimplify.com/aws-eks/kubernetes-concepts/learn-kubernetes-secrets-on-aws-eks/
https://eksctl.io/usage/

https://medium.com/codex/create-kubernetes-cluster-on-aws-eks-6ced4c488e62

Step-01: Create EKS Cluster using eksctl
(skip) eksctl create cluster --name test-cluster --version 1.22 --region us-east-1 --nodegroup-name k8s-nodes  --node-type t3a.medium --nodes 2

OR without node group 
(following expects this)
eksctl create cluster --name=eksdemo1 --region=us-east-1 --zones=us-east-1a,us-east-1b --without-nodegroup 

eksctl get clusters  --region us-east-1


Step-02: Create & Associate IAM OIDC Provider for our EKS Cluster
To enable and use AWS IAM roles for Kubernetes service accounts on our EKS cluster, we must create & associate OIDC identity provider.
To do so using eksctl we can use the below command.
Use latest eksctl version (as on today the latest version is 0.21.0)

# Template
eksctl utils associate-iam-oidc-provider --region us-east-1 --cluster eksdemo1 --approve

# Replace with region & cluster name
eksctl utils associate-iam-oidc-provider --region us-east-1 --cluster eksdemo1  --approve


Step-03: Create EC2 Keypair
Create a new EC2 Keypair with name as kube-demo
This keypair we will use it when creating the EKS NodeGroup.
This will help us to login to the EKS Worker Nodes using Terminal.


Step-04: Create Node Group with additional Add-Ons in Public Subnets
These add-ons will create the respective IAM policies for us automatically within our Node Group role.

# Create Public Node Group   
eksctl create nodegroup --cluster=eksdemo1                  --region=us-east-1                  --name=eksdemo1-ng-public1         --node-type=t3.medium             --nodes=2                      --nodes-min=2                      --nodes-max=4                        --node-volume-size=20              --ssh-access                       --ssh-public-key=kube-demo          --managed  --asg-access                       --external-dns-access --full-ecr-access                    --appmesh-access       --alb-ingress-access 

Step-05: Verify Cluster & Nodes
Verify NodeGroup subnets to confirm EC2 Instances are in Public Subnet
Verify the node group subnet to ensure it created in public subnets
Go to Services -> EKS -> eksdemo -> eksdemo1-ng1-public
Click on Associated subnet in Details tab
Click on Route Table Tab.
We should see that internet route via Internet Gateway (0.0.0.0/0 -> igw-xxxxxxxx)
Verify Cluster, NodeGroup in EKS Management Console
Go to Services -> Elastic Kubernetes Service -> eksdemo1
List Worker Nodes

# List EKS clusters
eksctl get clusters --region us-east-1

# List NodeGroups in a cluster
eksctl get nodegroup --cluster=<clusterName>
eksctl get nodegroup --cluster=eksdemo1

# List Nodes in current kubernetes cluster
kubectl get nodes -o wide

# Our kubectl context should be automatically changed to new cluster
kubectl config view --minify
Verify Worker Node IAM Role and list of Policies
Go to Services -> EC2 -> Worker Nodes
Click on IAM Role associated to EC2 Worker Nodes
Verify Security Group Associated to Worker Nodes
Go to Services -> EC2 -> Worker Nodes
Click on Security Group associated to EC2 Instance which contains remote in the name.
Verify CloudFormation Stacks
Verify Control Plane Stack & Events
Verify NodeGroup Stack & Events
Login to Worker Node using Keypai kube-demo
Login to worker node

# For MAC or Linux or Windows10
ssh -i kube-demo.pem ec2-user@<Public-IP-of-Worker-Node>



# For Windows 7
Use putty
Step-06: Update Worker Nodes Security Group to allow all traffic
We need to allow All Traffic on worker node security group


Verification
Step-01: Delete Node Group
We can delete a nodegroup separately using below eksctl delete nodegroup



# List EKS Clusters
eksctl get clusters

# Capture Node Group name
eksctl get clusters --region us-east-1
eksctl get nodegroup --cluster=<clusterName>
eksctl get nodegroup --cluster=eksdemo1 --region=us-east-1


aws eks --region us-east-1 update-kubeconfig --name eksdemo1

# Get Worker Node Status
kubectl get nodes

# Get Worker Node Status with wide option
kubectl get nodes -o wide


# Create  a Pod
kubectl run <desired-pod-name> --image <Container-Image> --generator=run-pod/v1
kubectl run mynginx --image nginx --generator=run-pod/v1

# Expose Pod as a Service
kubectl expose pod <Pod-Name>  --type=NodePort --port=80 --name=<Service-Name>
kubectl expose pod mynginx  --type=NodePort --port=80 --name=my-first-service

# Get Service Info
kubectl get service
kubectl get svc

# Get Public IP of Worker Nodes
kubectl get nodes -o wide

Access the Application using Public IP


http://<node1-public-ip>:<Node-Port>
Important Note about: target-port

If target-port is not defined, by default and for convenience, the targetPort is set to the same value as the port field.

# Below command will fail when accessing the application, as service port (81) and container port (80) are different
kubectl expose pod my-first-pod  --type=NodePort --port=81 --name=my-first-service2     

# Expose Pod as a Service with Container Port (--taret-port)
kubectl expose pod my-first-pod  --type=NodePort --port=81 --target-port=80 --name=my-first-service3

# Get Service Info
kubectl get service
kubectl get svc

# Get Public IP of Worker Nodes
kubectl get nodes -o wide
- Access the Application using Public IP

http://<node1-public-ip>:<Node-Port>


Step-05: Interact with a Pod
Verify Pod Logs

# Get Pod Name
kubectl get po

# Dump Pod logs
kubectl logs <pod-name>
kubectl logs my-first-pod

# Stream pod logs with -f option and access application to see logs
kubectl logs <pod-name>
kubectl logs -f my-first-pod
- Important Notes - Refer below link and search for Interacting with running Pods for additional log options - Troubleshooting skills are very important. So please go through all logging options available and master them. - Reference: https://kubernetes.io/docs/reference/kubectl/cheatsheet/
Connect to Container in a POD
Connect to a Container in POD and execute commands


# Connect to Nginx Container in a POD
kubectl exec -it <pod-name> -- /bin/bash
kubectl exec -it my-first-pod -- /bin/bash

# Execute some commands in Nginx container
ls
cd /usr/share/nginx/html
cat index.html
exit
Running individual commands in a Container


kubectl exec -it <pod-name> env

# Sample Commands
kubectl exec -it my-first-pod env
kubectl exec -it my-first-pod ls
kubectl exec -it my-first-pod cat /usr/share/nginx/html/index.html
Step-06: Get YAML Output of Pod & Service
Get YAML Output

# Get pod definition YAML output
kubectl get pod my-first-pod -o yaml   

# Get service definition YAML output
kubectl get service my-first-service -o yaml   
Step-07: Clean-Up

# Get all Objects in default namespace
kubectl get all

# Delete Services
kubectl delete svc my-first-service
kubectl delete svc my-first-service2
kubectl delete svc my-first-service3

# Delete Pod
kubectl delete pod my-first-pod

# Get all Objects in default namespace
kubectl get all


Expose ReplicaSet as a service: https://www.stacksimplify.com/aws-eks/kubernetes-for-absolute-beginners/kubernetes-replicasets-with-kubectl/

Deployments: 
https://www.stacksimplify.com/aws-eks/kubernetes-for-absolute-beginners/create-deployment-scale-pods-expose-as-service/
Services: 
https://www.stacksimplify.com/aws-eks/kubernetes-for-absolute-beginners/create-kubernetes-services-with-kubectl/


Storage class
kubectl apply -f D:\PraiseTheLord\HSBGInfotech\AWS\eks\dynamicprovisioning\sc.yml

# List Storage Classes
kubectl get sc

# List PVC
kubectl get pvc 

# List PV
kubectl get pv

# List pods
kubectl get pods 

# List pods based on  label name
kubectl get pods -l app=mysql


# Connect to MYSQL Database
kubectl run --rm --image=mysql:5.6 --restart=Never mysql-client 
winpty kubectl.exe exec -it mysql-client -- sh

mysql -h mysql -pdbpassword11



# Verify usermgmt schema got created which we provided in ConfigMap
mysql> show schemas;
exit

Deploy frontend
kubectl apply -f D:\PraiseTheLord\HSBGInfotech\AWS\eks\dynamicprovisioning\frontend.yml
kubectl apply -f frontend.yml


Secrets
https://www.stacksimplify.com/aws-eks/kubernetes-concepts/learn-kubernetes-secrets-on-aws-eks/

init containers
https://www.stacksimplify.com/aws-eks/kubernetes-concepts/learn-kubernetes-init-containers-on-aws-eks/

probes
https://www.stacksimplify.com/aws-eks/kubernetes-concepts/learn-kubernetes-liveness-and-readiness-probes/

requests and limits
https://www.stacksimplify.com/aws-eks/kubernetes-concepts/learn-kubernetes-requests-and-limits-on-aws-eks/

Namespaces
	LimitRange
	ResourceQuota
	https://www.stacksimplify.com/aws-eks/kubernetes-concepts/learn-kubernetes-requests-and-limits-on-aws-eks/
	
	

Use RDS Database for Workloads running on AWS EKS Cluster
https://www.stacksimplify.com/aws-eks/kubernetes-storage/aws-eks-storage-with-aws-rds-database/
	sg-041284788fd3b32f9


Load Balancer
	https://www.stacksimplify.com/aws-eks/aws-loadbalancers/aws-eks-create-private-nodegroup/
eksctl create nodegroup --cluster=eksdemo1 --region=us-east-1 --name=eksdemo1-ng-private1 --node-type=t3.medium --nodes-min=2 --nodes-max=4 --node-volume-size=20 --ssh-access --ssh-public-key=vilasvirgian --managed --asg-access --external-dns-access --full-ecr-access --appmesh-access --alb-ingress-access --node-private-networking                       

kubectl get nodes -o wide




Delete eks cluster
https://www.stacksimplify.com/aws-eks/aws-loadbalancers/aws-eks-create-private-nodegroup/
eksctl get nodegroup --cluster=eksdemo1 --region us-east-1


--------------------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------------------

			OUT OF SYLLABUS
--------------------------------------------------------------------------------------------------------

S3
	S3 Essentials
	Creating An S3 Bucket Using The Console
	Create An S3 Website
	CORS Configuration
	S3 Version Control
	Cross Region Replication
	S3 Lifecycle Management & Glacier
	Cloud Front Overview
	Create a CDN
	S3 - Security & Encryption
	Storage Gateway
	Snowball
	S3 Transfer Acceleration

DynamoDB
	Introduction to DynamoDB
	Creating a DynamoDB Table
	DynamoDB Indexes
	Scan vs Query API Calls
	DynamoDB& Provisioned Throughput
	Using Web Identity Providers To Connect To Authenticate To DynamoDB
	Other important aspects of DynamoDB

Simple Queue Service (SQS)
	What is SQS?
Simple Notification Services (SNS)
	Introduction to SNS
	Creating an SNS Topic
Simple Workflow Service (SWF)
	Introduction to SWF
CloudFormation OR Terraform
Elastic Beanstalk
	Using Elastic Beanstalk
Lambda



https://github.com/thyagomota/aws-labs
https://github.com/widdix/learn-security-fundamentals
https://jayendrapatil.com/aws-iam-role/


